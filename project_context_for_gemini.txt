CẤU TRÚC THƯ MỤC DỰ ÁN:
========================
./
    run.py
    aaa.py
    runs/
        run_20250807_124725_767d46fe/
            states/
                final_preprocessing_code.py
                final_executable_code.py
            generation_iter_0/
                temp_exec_b33da0e3.py
                temp_exec_ebf0562e.py
                temp_exec_7e6e3fa7.py
                temp_exec_d119ecd5.py
                temp_exec_a352b107.py
                temp_exec_00963762.py
                temp_exec_008db28b.py
                temp_exec_fdbe3cad.py
                states/
                    final_assembled_code.py
                    preprocessing_code_response.py
                    modeling_code_response.py
        run_20250807_124847_23a93c15/
            states/
            generation_iter_0/
                temp_exec_3ce48765.py
                temp_exec_b538f6ed.py
                temp_exec_4238bc33.py
                temp_exec_3ca9bccb.py
                temp_exec_0b4feaa7.py
                states/
                    preprocessing_code_response.py
        run_20250807_124601_21be393f/
            generation_iter_0/
                temp_exec_90a5e8a2.py
                temp_exec_b83cc236.py
                temp_exec_b6c666d5.py
                temp_exec_d4e6443a.py
                temp_exec_c7ebca65.py
                temp_exec_50711109.py
                states/
        run_20250807_150457_59bb9c18/
            states/
        run_20250807_124818_c955b65d/
            states/
            generation_iter_0/
                temp_exec_70f30017.py
                temp_exec_3888b329.py
                temp_exec_f12d3515.py
                temp_exec_f0ae4c57.py
                temp_exec_68186c18.py
                states/
                    preprocessing_code_response.py
        run_20250807_150523_c390a90d/
            states/
                final_preprocessing_code.py
                final_executable_code.py
            generation_iter_0/
                temp_exec_375858ec.py
                temp_exec_321ffb06.py
                temp_exec_ccc4519f.py
                temp_exec_1b821a36.py
                temp_exec_a40a7ae2.py
                temp_exec_75da7fd3.py
                temp_exec_c8690537.py
                states/
                    final_assembled_code.py
                    preprocessing_code_response.py
                    modeling_code_response.py
        run_20250807_131833_5dcdce5c/
            states/
                final_preprocessing_code.py
            generation_iter_0/
                temp_exec_ce0cced5.py
                temp_exec_b5e8c540.py
                states/
                    final_assembled_code.py
                    preprocessing_code_response.py
                    modeling_code_response.py
        run_20250807_125538_f5bfb46c/
            states/
            generation_iter_0/
                temp_exec_aa451b26.py
                temp_exec_3be51793.py
                temp_exec_e01a36aa.py
                temp_exec_a7b8f6fd.py
                temp_exec_7d0944a4.py
                states/
                    preprocessing_code_response.py
        run_20250807_124631_3fc49b38/
            states/
                final_preprocessing_code.py
                final_executable_code.py
            generation_iter_0/
                temp_exec_c3c595ea.py
                temp_exec_19b2bb6d.py
                temp_exec_b2bfee36.py
                temp_exec_5e96c93a.py
                temp_exec_061eab52.py
                temp_exec_6ee30c33.py
                temp_exec_2aba2d31.py
                temp_exec_e6ce54f4.py
                temp_exec_6b4db79a.py
                temp_exec_af02bd75.py
                temp_exec_5b5ead59.py
                states/
                    final_assembled_code.py
                    preprocessing_code_response.py
                    modeling_code_response.py
    configs/
    src/
        iML/
            main_runner.py
            __init__.py
            llm/
                bedrock_chat.py
                azure_openai_chat.py
                openai_chat.py
                llm_factory.py
                __init__.py
                base_chat.py
                anthropic_chat.py
            core/
                manager.py
                __init__.py
            utils/
                rich_logging.py
                utils.py
                __init__.py
                file_io.py
                constants.py
            agents/
                profiling_agent.py
                modeling_coder_agent.py
                utils.py
                assembler_agent.py
                candidate_generator_agent.py
                feedback_agent.py
                preprocessing_coder_agent.py
                description_analyzer_agent.py
                candidate_selector_agent.py
                executer_agent.py
                __init__.py
                base_agent.py
                guideline_agent.py
            prompts/
                assembler_prompt.py
                base_prompt.py
                description_analyzer_prompt.py
                candidate_selector_prompt.py
                candidate_generator_prompt.py
                modeling_coder_prompt.py
                preprocessing_coder_prompt.py
                __init__.py
                feedback_prompt.py
                guideline_prompt.py
                executer_prompt.py


NỘI DUNG CHI TIẾT CÁC TỆP:
===========================

--- START FILE: run.py ---
import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from iML.main_runner import run_automl_pipeline

def main():
    """
    Main entry point for the application when run from the terminal.
    Parses input arguments and calls the main pipeline.
    """
    parser = argparse.ArgumentParser(description="iML")
    
    parser.add_argument(
        "-i", "--input", 
        required=True, 
        help="Path to the input data folder"
    )
    parser.add_argument(
        "-o", "--output", 
        default=None,
        help="Path to the output directory. If not provided, one will be auto-generated in the 'runs/' directory."
    )
    parser.add_argument(
        "-c", "--config", 
        default="configs/default.yaml", 
        help="Path to the configuration file (default: configs/default.yaml)"
    )
    
    args = parser.parse_args()
    
    # Call the main pipeline function from main_runner
    run_automl_pipeline(
        input_data_folder=args.input,
        output_folder=args.output,
        config=args.config,
    )

if __name__ == "__main__":
    main()
--- END FILE: run.py ---

--- START FILE: aaa.py ---
import os

# Các thư mục hoặc tệp không muốn đưa vào (ví dụ: virtual environment, cache)
EXCLUDE_DIRS = {'__pycache__', '.venv', 'venv', '.git', '.vscode'}
EXCLUDE_FILES = {'export_project.py'} # Không cần đưa chính file này vào

def generate_project_context(root_dir, output_file):
    """
    Duyệt qua dự án, ghi cấu trúc thư mục và nội dung file vào một file duy nhất.
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # Bước 1: In cấu trúc thư mục trước
        f.write("CẤU TRÚC THƯ MỤC DỰ ÁN:\n")
        f.write("========================\n")
        for root, dirs, files in os.walk(root_dir):
            # Loại bỏ các thư mục không cần thiết
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

            level = root.replace(root_dir, '').count(os.sep)
            indent = ' ' * 4 * level
            f.write(f"{indent}{os.path.basename(root)}/\n")
            sub_indent = ' ' * 4 * (level + 1)
            for file_name in files:
                if file_name.endswith('.py') and file_name not in EXCLUDE_FILES:
                    f.write(f"{sub_indent}{file_name}\n")

        f.write("\n\nNỘI DUNG CHI TIẾT CÁC TỆP:\n")
        f.write("===========================\n\n")

        # Bước 2: In nội dung từng tệp
        for root, dirs, files in os.walk(root_dir):
            # Loại bỏ các thư mục không cần thiết
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

            for file_name in files:
                if file_name.endswith('.py') and file_name not in EXCLUDE_FILES:
                    file_path = os.path.join(root, file_name)
                    relative_path = os.path.relpath(file_path, root_dir)

                    f.write(f"--- START FILE: {relative_path} ---\n")
                    try:
                        with open(file_path, 'r', encoding='utf-8') as file_content:
                            f.write(file_content.read())
                    except Exception as e:
                        f.write(f"*** Không thể đọc file: {e} ***")
                    f.write(f"\n--- END FILE: {relative_path} ---\n\n")

if __name__ == "__main__":
    project_directory = '.'  # '.' nghĩa là thư mục hiện tại
    output_filename = 'project_context_for_gemini.txt'
    generate_project_context(project_directory, output_filename)
    print(f"Đã xuất toàn bộ dự án vào file: {output_filename}")
--- END FILE: aaa.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/states/final_preprocessing_code.py ---
import os
import sys
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split

# Try to import iterative stratification
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

def preprocess_data(file_paths):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict or list/tuple with train and test csv paths.
    Returns:
        X_train, X_val, y_train, y_val
    """
    # Validate input
    if isinstance(file_paths, dict):
        train_path = file_paths.get("train") or file_paths.get("train.csv")
        test_path = file_paths.get("test") or file_paths.get("test.csv")
    elif isinstance(file_paths, (list, tuple)):
        if len(file_paths) < 2:
            raise ValueError("Expected at least [train_path, test_path]")
        train_path, test_path = file_paths[:2]
    else:
        raise ValueError("file_paths must be a dict or list/tuple")

    # Check file existence
    for p in (train_path, test_path):
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load datasets
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Drop 'id' and duplicates
    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    # Verify target columns
    targets = ["Pastry", "Z_Scratch", "K_Scatch", "Stains",
               "Dirtiness", "Bumps", "Other_Faults"]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering on both sets
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test_full = test_df.copy()

    # 1) Correlation filter
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [c for c in upper.columns if any(upper[c] > 0.95)]
    X.drop(columns=drop_cols, inplace=True)
    X_test_full.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_features = X.columns[vt.get_support()]
    X = X[keep_features]
    X_test_full = X_test_full[keep_features]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_], axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test_full = X_test_full[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        # Fallback stratify by concatenated labels
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv"
        }
        X_train, X_test, y_train, y_test = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/states/final_preprocessing_code.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/states/final_executable_code.py ---
#!/usr/bin/env python3
# -*- coding: utf-8

import os
import sys
import pandas as pd
import numpy as np

from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# Try to import iterative stratification for train/val split
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

# Try to import iterative stratification for CV
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
    HAS_MSKF = True
except ImportError:
    HAS_MSKF = False


def preprocess_data(train_path, test_path):
    """
    Load and preprocess training and test data.
    Returns:
        X_train, X_val, y_train, y_val, X_test, test_ids
    """
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Capture test IDs before dropping
    if "id_code" in test_df.columns:
        test_ids = test_df["id_code"].values
    elif "id" in test_df.columns:
        test_ids = test_df["id"].values
    else:
        test_ids = np.arange(len(test_df))

    # Drop identifier columns from features
    drop_id_cols = [c for c in ("id_code", "id") if c in train_df.columns]
    if drop_id_cols:
        train_df = train_df.drop(columns=drop_id_cols)
    drop_id_cols_test = [c for c in ("id_code", "id") if c in test_df.columns]
    if drop_id_cols_test:
        test_df = test_df.drop(columns=drop_id_cols_test)

    # Remove duplicates in train
    train_df.drop_duplicates(inplace=True)

    # Define target columns and verify
    targets = [
        "Pastry", "Z_Scratch", "K_Scatch", "Stains",
        "Dirtiness", "Bumps", "Other_Faults"
    ]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split into features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test = test_df.copy()

    # 1) Correlation filter
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [col for col in upper.columns if any(upper[col] > 0.95)]
    if drop_cols:
        X.drop(columns=drop_cols, inplace=True)
        X_test.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_feats = X.columns[vt.get_support()]
    X = X[keep_feats]
    X_test = X_test[keep_feats]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_],
        axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test = X_test[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val, X_test, test_ids


def train_and_predict(X_train, y_train, X_test,
                      n_splits=5, random_state=42):
    """
    Perform cross-validated training and return test predictions
    and the final fitted model.
    """
    fold_scores = []
    if HAS_MSKF:
        splitter = MultilabelStratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train, y_train)
    else:
        splitter = KFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train)

    for fold, (tr_idx, val_idx) in enumerate(splits, start=1):
        X_tr = X_train.iloc[tr_idx]
        y_tr = y_train.iloc[tr_idx]
        X_val = X_train.iloc[val_idx]
        y_val = y_train.iloc[val_idx]

        clf = OneVsRestClassifier(
            LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
        )
        clf.fit(X_tr, y_tr)
        val_pred = clf.predict_proba(X_val)
        score = roc_auc_score(y_val, val_pred, average='macro')
        fold_scores.append(score)
        print(f"Fold {fold} ROC AUC: {score:.4f}")

    mean_score = np.mean(fold_scores)
    print(f"Mean CV ROC AUC: {mean_score:.4f}")

    final_clf = OneVsRestClassifier(
        LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
    )
    final_clf.fit(X_train, y_train)
    test_preds = final_clf.predict_proba(X_test)

    return test_preds, final_clf


if __name__ == "__main__":
    try:
        # Paths
        train_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/train.csv"
        )
        test_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/test.csv"
        )
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/"
            "iSE_AutoML/RREFACTORED/runs/"
            "run_20250807_124725_767d46fe/submission.csv"
        )

        # Preprocess
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(
            train_path, test_path
        )
        print("Preprocessing completed.")

        # Train, CV, and predict
        preds, final_model = train_and_predict(X_train, y_train, X_test)

        # Hold-out validation
        val_pred = final_model.predict_proba(X_val)
        holdout_score = roc_auc_score(y_val, val_pred, average='macro')
        print(f"Hold-out Validation ROC AUC: {holdout_score:.4f}")

        # Build submission DataFrame
        defect_cols = y_train.columns.tolist()
        submission = pd.DataFrame({"id_code": test_ids})
        for idx, col in enumerate(defect_cols):
            submission[col] = preds[:, idx]

        # Ensure output directory exists and save
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/states/final_executable_code.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_b33da0e3.py ---
import os
import sys
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split

# Try to import iterative stratification
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

def preprocess_data(file_paths):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict or list/tuple with train and test csv paths.
    Returns:
        X_train, X_val, y_train, y_val
    """
    # Validate input
    if isinstance(file_paths, dict):
        train_path = file_paths.get("train") or file_paths.get("train.csv")
        test_path = file_paths.get("test") or file_paths.get("test.csv")
    elif isinstance(file_paths, (list, tuple)):
        if len(file_paths) < 2:
            raise ValueError("Expected at least [train_path, test_path]")
        train_path, test_path = file_paths[:2]
    else:
        raise ValueError("file_paths must be a dict or list/tuple")

    # Check file existence
    for p in (train_path, test_path):
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load datasets
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Drop 'id' and duplicates
    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    # Verify target columns
    targets = ["Pastry", "Z_Scratch", "K_Scatch", "Stains",
               "Dirtiness", "Bumps", "Other_Faults"]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering on both sets
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test_full = test_df.copy()

    # 1) Correlation filter
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [c for c in upper.columns if any(upper[c] > 0.95)]
    X.drop(columns=drop_cols, inplace=True)
    X_test_full.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_features = X.columns[vt.get_support()]
    X = X[keep_features]
    X_test_full = X_test_full[keep_features]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_], axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test_full = X_test_full[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        # Fallback stratify by concatenated labels
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv"
        }
        X_train, X_test, y_train, y_test = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_b33da0e3.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_ebf0562e.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import pandas as pd
import numpy as np

from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# Try to import iterative stratification for train/val split
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

# Try to import iterative stratification for CV
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
    HAS_MSKF = True
except ImportError:
    HAS_MSKF = False


def preprocess_data(train_path, test_path):
    """
    Load and preprocess training and test data.
    Returns:
        X_train, X_val, y_train, y_val, X_test
    """
    # Check files exist
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load data
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Drop 'id' if exists, drop duplicates from train
    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    # Define target columns and verify
    targets = [
        "Pastry", "Z_Scratch", "K_Scatch", "Stains",
        "Dirtiness", "Bumps", "Other_Faults"
    ]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split into features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test = test_df.copy()

    # 1) Correlation filter (drop one of any pair with corr > .95)
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [col for col in upper.columns if any(upper[col] > 0.95)]
    if drop_cols:
        X.drop(columns=drop_cols, inplace=True)
        X_test.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_feats = X.columns[vt.get_support()]
    X = X[keep_feats]
    X_test = X_test[keep_feats]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_],
        axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test = X_test[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        # Fallback: stratify by concatenated labels
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val, X_test


def train_and_predict(X_train, y_train, X_test,
                      n_splits=5, random_state=42):
    """
    Perform cross-validated training and return test predictions
    and the final fitted model.
    """
    fold_scores = []
    if HAS_MSKF:
        splitter = MultilabelStratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train, y_train)
    else:
        splitter = KFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train)

    for fold, (tr_idx, val_idx) in enumerate(splits, start=1):
        X_tr = X_train.iloc[tr_idx]
        y_tr = y_train.iloc[tr_idx]
        X_val = X_train.iloc[val_idx]
        y_val = y_train.iloc[val_idx]

        clf = OneVsRestClassifier(
            LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
        )
        clf.fit(X_tr, y_tr)
        val_pred = clf.predict_proba(X_val)
        score = roc_auc_score(y_val, val_pred, average='macro')
        fold_scores.append(score)
        print(f"Fold {fold} ROC AUC: {score:.4f}")

    mean_score = np.mean(fold_scores)
    print(f"Mean CV ROC AUC: {mean_score:.4f}")

    # Train final model on all training data
    final_clf = OneVsRestClassifier(
        LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
    )
    final_clf.fit(X_train, y_train)
    test_preds = final_clf.predict_proba(X_test)

    return test_preds, final_clf


if __name__ == "__main__":
    try:
        # Paths
        train_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/train.csv"
        )
        test_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/test.csv"
        )
        sample_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/sample_submission.csv"
        )
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/"
            "iSE_AutoML/RREFACTORED/runs/"
            "run_20250807_124725_767d46fe/submission.csv"
        )

        # Preprocess
        X_train, X_val, y_train, y_val, X_test = preprocess_data(
            train_path, test_path
        )
        print("Preprocessing completed.")

        # Train, CV, and predict
        preds, final_model = train_and_predict(X_train, y_train, X_test)

        # Validation on hold-out set
        val_pred = final_model.predict_proba(X_val)
        holdout_score = roc_auc_score(y_val, val_pred, average='macro')
        print(f"Hold-out Validation ROC AUC: {holdout_score:.4f}")

        # Build submission
        sample_sub = pd.read_csv(sample_path)
        defect_cols = y_train.columns.tolist()
        submission = sample_sub.copy()
        for idx, col in enumerate(defect_cols):
            submission[col] = preds[:, idx]

        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_ebf0562e.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_7e6e3fa7.py ---
#!/usr/bin/env python3
# -*- coding: utf-8

import os
import sys
import pandas as pd
import numpy as np

from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# Try to import iterative stratification for train/val split
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

# Try to import iterative stratification for CV
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
    HAS_MSKF = True
except ImportError:
    HAS_MSKF = False


def preprocess_data(train_path, test_path):
    """
    Load and preprocess training and test data.
    Returns:
        X_train, X_val, y_train, y_val, X_test, test_ids
    """
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Capture test IDs before dropping
    if "id_code" in test_df.columns:
        test_ids = test_df["id_code"].values
    elif "id" in test_df.columns:
        test_ids = test_df["id"].values
    else:
        test_ids = np.arange(len(test_df))

    # Drop identifier columns from features
    drop_id_cols = [c for c in ("id_code", "id") if c in train_df.columns]
    if drop_id_cols:
        train_df = train_df.drop(columns=drop_id_cols)
    drop_id_cols_test = [c for c in ("id_code", "id") if c in test_df.columns]
    if drop_id_cols_test:
        test_df = test_df.drop(columns=drop_id_cols_test)

    # Remove duplicates in train
    train_df.drop_duplicates(inplace=True)

    # Define target columns and verify
    targets = [
        "Pastry", "Z_Scratch", "K_Scatch", "Stains",
        "Dirtiness", "Bumps", "Other_Faults"
    ]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split into features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test = test_df.copy()

    # 1) Correlation filter
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [col for col in upper.columns if any(upper[col] > 0.95)]
    if drop_cols:
        X.drop(columns=drop_cols, inplace=True)
        X_test.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_feats = X.columns[vt.get_support()]
    X = X[keep_feats]
    X_test = X_test[keep_feats]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_],
        axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test = X_test[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val, X_test, test_ids


def train_and_predict(X_train, y_train, X_test,
                      n_splits=5, random_state=42):
    """
    Perform cross-validated training and return test predictions
    and the final fitted model.
    """
    fold_scores = []
    if HAS_MSKF:
        splitter = MultilabelStratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train, y_train)
    else:
        splitter = KFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train)

    for fold, (tr_idx, val_idx) in enumerate(splits, start=1):
        X_tr = X_train.iloc[tr_idx]
        y_tr = y_train.iloc[tr_idx]
        X_val = X_train.iloc[val_idx]
        y_val = y_train.iloc[val_idx]

        clf = OneVsRestClassifier(
            LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
        )
        clf.fit(X_tr, y_tr)
        val_pred = clf.predict_proba(X_val)
        score = roc_auc_score(y_val, val_pred, average='macro')
        fold_scores.append(score)
        print(f"Fold {fold} ROC AUC: {score:.4f}")

    mean_score = np.mean(fold_scores)
    print(f"Mean CV ROC AUC: {mean_score:.4f}")

    final_clf = OneVsRestClassifier(
        LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
    )
    final_clf.fit(X_train, y_train)
    test_preds = final_clf.predict_proba(X_test)

    return test_preds, final_clf


if __name__ == "__main__":
    try:
        # Paths
        train_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/train.csv"
        )
        test_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/test.csv"
        )
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/"
            "iSE_AutoML/RREFACTORED/runs/"
            "run_20250807_124725_767d46fe/submission.csv"
        )

        # Preprocess
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(
            train_path, test_path
        )
        print("Preprocessing completed.")

        # Train, CV, and predict
        preds, final_model = train_and_predict(X_train, y_train, X_test)

        # Hold-out validation
        val_pred = final_model.predict_proba(X_val)
        holdout_score = roc_auc_score(y_val, val_pred, average='macro')
        print(f"Hold-out Validation ROC AUC: {holdout_score:.4f}")

        # Build submission DataFrame
        defect_cols = y_train.columns.tolist()
        submission = pd.DataFrame({"id_code": test_ids})
        for idx, col in enumerate(defect_cols):
            submission[col] = preds[:, idx]

        # Ensure output directory exists and save
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_7e6e3fa7.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_d119ecd5.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import logging
import joblib

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.feature_selection import VarianceThreshold
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# Try to import iterative stratifiers
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold
    HAS_ITER = True
    HAS_MSKF = True
except ImportError:
    HAS_ITER = False
    HAS_MSKF = False


class CorrelationFilter(BaseEstimator, TransformerMixin):
    """Drop features with a correlation higher than the threshold."""
    def __init__(self, threshold=0.95):
        self.threshold = threshold
        self.drop_cols_ = []

    def fit(self, X, y=None):
        if isinstance(X, pd.DataFrame):
            corr = X.corr().abs()
            upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
            self.drop_cols_ = [col for col in upper.columns if any(upper[col] > self.threshold)]
        else:
            self.drop_cols_ = []
        return self

    def transform(self, X):
        if isinstance(X, pd.DataFrame):
            return X.drop(columns=self.drop_cols_, errors='ignore')
        else:
            return X


def configure_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    logger = logging.getLogger()
    return logger


def main():
    logger = configure_logging()
    try:
        # Paths (must remain unchanged)
        train_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/train.csv"
        )
        test_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/test.csv"
        )
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/"
            "iSE_AutoML/RREFACTORED/runs/"
            "run_20250807_124725_767d46fe/submission.csv"
        )
        model_path = os.path.splitext(output_path)[0] + "_pipeline.pkl"

        logger.info("Loading data...")
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        if train_df.empty or test_df.empty:
            raise ValueError("One of the input files is empty")

        # Standardize ID column and extract
        if "id_code" in test_df.columns:
            test_df.rename(columns={"id_code": "id"}, inplace=True)
        if "id_code" in train_df.columns:
            train_df.rename(columns={"id_code": "id"}, inplace=True)
        test_ids = test_df["id"].values
        # Drop ID
        train_df.drop(columns=["id"], inplace=True, errors="ignore")
        test_df.drop(columns=["id"], inplace=True, errors="ignore")

        # Remove duplicates
        before = len(train_df)
        train_df.drop_duplicates(inplace=True)
        logger.info(f"Dropped {before - len(train_df)} duplicate rows.")

        # Targets
        targets = ["Pastry", "Z_Scratch", "K_Scatch", "Stains",
                   "Dirtiness", "Bumps", "Other_Faults"]
        missing = [t for t in targets if t not in train_df.columns]
        if missing:
            raise KeyError(f"Missing target columns: {missing}")

        # Feature engineering
        logger.info("Generating new features...")
        for df in (train_df, test_df):
            df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
            df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
            df["Perimeter_to_Area"] = (df["X_Perimeter"] + df["Y_Perimeter"]) / df["Pixels_Areas"]
            df["Luminosity_per_Pixel"] = df["Sum_of_Luminosity"] / df["Pixels_Areas"]

        # Log-transform skewed features
        skew_cols = ["Pixels_Areas", "Sum_of_Luminosity"]
        logger.info(f"Applying log1p transform to: {skew_cols}")
        train_df[skew_cols] = np.log1p(train_df[skew_cols])
        test_df[skew_cols] = np.log1p(test_df[skew_cols])

        # Split X/y
        X = train_df.drop(columns=targets)
        y = train_df[targets].astype(int)
        X_test = test_df.copy()

        # Build preprocessing pipeline
        preprocessor = Pipeline([
            ("corr", CorrelationFilter(threshold=0.95)),
            ("var", VarianceThreshold(threshold=0.01))
        ])

        # Build full model pipeline
        lgbm = LGBMClassifier(
            random_state=42,
            n_jobs=-1,
            learning_rate=0.05,
            n_estimators=500,
            verbosity=-1
        )
        model_pipeline = Pipeline([
            ("preproc", preprocessor),
            ("clf", OneVsRestClassifier(lgbm, n_jobs=-1))
        ])

        # Split train/validation
        logger.info("Preparing train/validation split...")
        if HAS_ITER:
            splitter = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
            train_idx, val_idx = next(splitter.split(X, y))
            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]
        else:
            logger.warning("iterative stratification not available, using random split")
            X_tr, X_val, y_tr, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

        # Cross-validated training
        logger.info("Starting cross-validated training...")
        fold_scores = []
        if HAS_MSKF:
            cv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            splits = cv.split(X_tr, y_tr)
        else:
            logger.warning("iterative stratified KFold not available, using KFold")
            splits = KFold(n_splits=5, shuffle=True, random_state=42).split(X_tr)

        for fold, (tr_idx, vl_idx) in enumerate(splits, 1):
            X_train_fold = X_tr.iloc[tr_idx]
            y_train_fold = y_tr.iloc[tr_idx]
            X_valid_fold = X_tr.iloc[vl_idx]
            y_valid_fold = y_tr.iloc[vl_idx]

            model = clone(model_pipeline)
            model.fit(X_train_fold, y_train_fold)
            y_pred = model.predict_proba(X_valid_fold)
            score = roc_auc_score(y_valid_fold, y_pred, average="macro")
            fold_scores.append(score)
            logger.info(f"Fold {fold} ROC AUC: {score:.4f}")

        logger.info(f"Mean CV ROC AUC: {np.mean(fold_scores):.4f}")

        # Final training on full training set (train+val)
        logger.info("Training final model on full training data...")
        model_pipeline.fit(pd.concat([X_tr, X_val]), pd.concat([y_tr, y_val]))

        # Hold-out evaluation
        y_val_pred = model_pipeline.predict_proba(X_val)
        holdout_score = roc_auc_score(y_val, y_val_pred, average="macro")
        logger.info(f"Hold-out ROC AUC: {holdout_score:.4f}")

        # Predict on test set
        logger.info("Predicting on test set...")
        test_preds = model_pipeline.predict_proba(X_test)

        # Build submission
        logger.info("Building submission file...")
        submission = pd.DataFrame({"id": test_ids})
        for idx, col in enumerate(targets):
            submission[col] = test_preds[:, idx]

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        logger.info(f"Submission saved to {output_path}")

        # Save the pipeline
        joblib.dump(model_pipeline, model_path)
        logger.info(f"Model pipeline saved to {model_path}")

    except Exception as e:
        logger.exception("An error occurred")
        sys.exit(1)


if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_d119ecd5.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_a352b107.py ---
#!/usr/bin/env python3
# -*- coding: utf-8
import os
import sys
import logging
import joblib
import pandas as pd
import numpy as np

from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# try iterative stratifiers
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold
    HAS_ITER = True
    HAS_MSKF = True
except ImportError:
    HAS_ITER = False
    HAS_MSKF = False

# configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# hardcoded paths as required
TRAIN_PATH = (
    "/home/shiinehata/Desktop/"
    "iSE/iSE_AutoML/model_eval/datasets/"
    "steel_plate_defect_prediction/train.csv"
)
TEST_PATH = (
    "/home/shiinehata/Desktop/"
    "iSE/iSE_AutoML/model_eval/datasets/"
    "steel_plate_defect_prediction/test.csv"
)
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/"
    "iSE_AutoML/RREFACTORED/runs/"
    "run_20250807_124725_767d46fe/submission.csv"
)
MODEL_PATH = os.path.join(os.path.dirname(OUTPUT_PATH), "model_pipeline.joblib")

TARGETS = [
    "Pastry", "Z_Scratch", "K_Scatch",
    "Stains", "Dirtiness", "Bumps", "Other_Faults"
]

def preprocess_data(train_path: str, test_path: str):
    """Load data, feature-engineer, filter and split."""
    logging.info("Loading data...")
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    if train.empty or test.empty:
        raise ValueError("Empty train or test data")
    # standardize id column name
    if "id" in train.columns:
        train.rename(columns={"id": "id_code"}, inplace=True)
    if "id" in test.columns:
        test.rename(columns={"id": "id_code"}, inplace=True)
    test_ids = test["id_code"].values

    # drop exact duplicates
    train.drop_duplicates(inplace=True)
    # drop id_code from features
    train_feats = train.drop(columns=["id_code"] + TARGETS)
    test_feats = test.drop(columns=["id_code"])

    # feature engineering
    for df in (train_feats, test_feats):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (df["X_Perimeter"] + df["Y_Perimeter"]) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        # log-transform skewed numeric features
        df["Pixels_Areas"] = np.log1p(df["Pixels_Areas"])
        df["Sum_of_Luminosity"] = np.log1p(df["Sum_of_Luminosity"])

    # remove high-correlation features
    logging.info("Removing highly correlated features...")
    corr = train_feats.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]
    train_feats.drop(columns=to_drop, inplace=True)
    test_feats.drop(columns=to_drop, inplace=True, errors="ignore")
    logging.info(f"Dropped {len(to_drop)} correlated cols: {to_drop}")

    # variance threshold
    logging.info("Applying variance threshold...")
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(train_feats)
    keep = train_feats.columns[vt.get_support()]
    train_feats = train_feats[keep]
    test_feats = test_feats[keep]

    # tree-based per-label importance filtering
    logging.info("Computing feature importances per label...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    y = train[TARGETS]
    multi_rf.fit(train_feats, y)
    importances = np.vstack([est.feature_importances_ for est in multi_rf.estimators_])
    # drop features that are below mean importance for all labels
    mean_imp = importances.mean(axis=1)
    threshold = mean_imp.mean()
    keep_mask = (importances >= threshold).any(axis=0)
    selected = train_feats.columns[keep_mask]
    train_feats = train_feats[selected]
    test_feats = test_feats[selected]
    logging.info(f"Selected {len(selected)} features after importance filter")

    # final split
    logging.info("Splitting train/validation...")
    if HAS_ITER:
        splitter = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        tr_idx, val_idx = next(splitter.split(train_feats, y))
        X_tr = train_feats.iloc[tr_idx].reset_index(drop=True)
        X_val = train_feats.iloc[val_idx].reset_index(drop=True)
        y_tr = y.iloc[tr_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        strat = y.astype(int).astype(str).agg("".join, axis=1)
        X_tr, X_val, y_tr, y_val = train_test_split(
            train_feats, y, test_size=0.2,
            random_state=42, stratify=strat
        )
    return X_tr, X_val, y_tr, y_val, test_feats, test_ids

def train_and_predict(X_tr, y_tr, X_val, y_val, X_test):
    """Cross-validate, train final model, and predict."""
    logging.info("Starting cross-validation...")
    cv_scores = []
    if HAS_MSKF:
        kf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        splits = kf.split(X_tr, y_tr)
    else:
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        splits = kf.split(X_tr)
    for fold, (ti, vi) in enumerate(splits, 1):
        X_train_f, X_val_f = X_tr.iloc[ti], X_tr.iloc[vi]
        y_train_f, y_val_f = y_tr.iloc[ti], y_tr.iloc[vi]
        clf = OneVsRestClassifier(
            LGBMClassifier(
                objective='binary', boosting_type='gbdt',
                learning_rate=0.05, num_leaves=31,
                n_estimators=200, random_state=42,
                categorical_feature=['TypeOfSteel_A300',
                                     'TypeOfSteel_A400',
                                     'Outside_Global_Index'],
                n_jobs=-1, verbose=-1
            ),
            n_jobs=-1
        )
        clf.fit(X_train_f, y_train_f)
        preds = clf.predict_proba(X_val_f)
        score = roc_auc_score(y_val_f, preds, average='macro')
        cv_scores.append(score)
        logging.info(f"Fold {fold} ROC AUC: {score:.4f}")
    logging.info(f"Mean CV ROC AUC: {np.mean(cv_scores):.4f}")

    # retrain on full training set
    logging.info("Training final model on all training data...")
    final_clf = OneVsRestClassifier(
        LGBMClassifier(
            objective='binary', boosting_type='gbdt',
            learning_rate=0.05, num_leaves=31,
            n_estimators=200, random_state=42,
            categorical_feature=['TypeOfSteel_A300',
                                 'TypeOfSteel_A400',
                                 'Outside_Global_Index'],
            n_jobs=-1, verbose=-1
        ),
        n_jobs=-1
    )
    final_clf.fit(X_tr, y_tr)
    val_pred = final_clf.predict_proba(X_val)
    holdout_score = roc_auc_score(y_val, val_pred, average='macro')
    logging.info(f"Hold-out ROC AUC: {holdout_score:.4f}")

    # predict test
    logging.info("Predicting test set...")
    test_preds = final_clf.predict_proba(X_test)
    return final_clf, test_preds

def save_submission(test_ids, preds, columns, out_path):
    logging.info(f"Saving submission to {out_path}")
    submission = pd.DataFrame({'id_code': test_ids})
    for i, col in enumerate(columns):
        submission[col] = preds[:, i]
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    submission.to_csv(out_path, index=False)

def main():
    try:
        X_tr, X_val, y_tr, y_val, X_test, test_ids = preprocess_data(TRAIN_PATH, TEST_PATH)
        model, preds = train_and_predict(X_tr, y_tr, X_val, y_val, X_test)
        save_submission(test_ids, preds, y_tr.columns.tolist(), OUTPUT_PATH)
        logging.info(f"Saving trained model to {MODEL_PATH}")
        joblib.dump(model, MODEL_PATH)
        logging.info("Done.")
    except Exception as e:
        logging.exception("Error during processing")
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_a352b107.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_00963762.py ---
import os
import sys
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split

try:
    from iterstrat.ml_stratifiers import IterativeStratification
    _HAS_ITER = True
except ImportError:
    _HAS_ITER = False

def preprocess_data(file_paths):
    if isinstance(file_paths, dict):
        train_path = file_paths.get("train")
        test_path = file_paths.get("test")
        sample_path = file_paths.get("sample")
    elif isinstance(file_paths, (list, tuple)):
        if len(file_paths) < 3:
            raise ValueError("Expecting [train, test, sample] paths")
        train_path, test_path, sample_path = file_paths[:3]
    else:
        raise ValueError("file_paths must be dict or list/tuple")

    for p in (train_path, test_path, sample_path):
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    train_df = pd.read_csv(train_path)
    test_df  = pd.read_csv(test_path)
    _ = pd.read_csv(sample_path)

    if train_df.empty or test_df.empty:
        raise ValueError("Train or test data is empty")

    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    targets = ["Pastry","Z_Scratch","K_Scatch","Stains","Dirtiness","Bumps","Other_Faults"]
    for t in targets:
        if t not in train_df.columns:
            raise KeyError(f"Missing target column: {t}")

    # feature engineering
    train_df["X_Range"] = train_df["X_Maximum"] - train_df["X_Minimum"]
    test_df ["X_Range"] = test_df ["X_Maximum"] - test_df ["X_Minimum"]
    train_df["Y_Range"] = train_df["Y_Maximum"] - train_df["Y_Minimum"]
    test_df ["Y_Range"] = test_df ["Y_Maximum"] - test_df ["Y_Minimum"]
    train_df["Perimeter_to_Area"] = (train_df["X_Perimeter"] + train_df["Y_Perimeter"]) / train_df["Pixels_Areas"]
    test_df ["Perimeter_to_Area"] = (test_df ["X_Perimeter"] + test_df ["Y_Perimeter"]) / test_df ["Pixels_Areas"]
    train_df["Luminosity_per_Pixel"] = train_df["Sum_of_Luminosity"] / train_df["Pixels_Areas"]
    test_df ["Luminosity_per_Pixel"] = test_df ["Sum_of_Luminosity"] / test_df ["Pixels_Areas"]

    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test = test_df.copy()

    # correlation filter
    corr = X.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    drop_corr = [c for c in upper.columns if any(upper[c] > 0.95)]
    X.drop(columns=drop_corr, inplace=True)
    X_test.drop(columns=drop_corr, inplace=True)

    # low variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_vars = X.columns[vt.get_support()]
    X = X[keep_vars]
    X_test = X_test[keep_vars]

    # tree-based feature selection
    rf = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42), n_jobs=-1)
    rf.fit(X, y)
    imps = np.mean([est.feature_importances_ for est in rf.estimators_], axis=0)
    thresh = imps.mean()
    keep_idx = np.where(imps >= thresh)[0]
    final_feats = X.columns[keep_idx]
    X = X[final_feats]
    X_test = X_test[final_feats]

    # train/val split
    if _HAS_ITER:
        strat = IterativeStratification(n_splits=2, order=1)
        train_idx, val_idx = next(strat.split(X.values, y.values))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val   = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx]
        y_val   = y.iloc[val_idx]
    else:
        strat_lbl = y.apply(lambda row: "".join(row.astype(int).astype(str)), axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val, X_test

if __name__ == "__main__":
    try:
        paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv",
            "sample": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/sample_submission.csv"
        }
        preprocess_data(paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_00963762.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_008db28b.py ---
import os
import sys
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split

try:
    from iterstrat.ml_stratifiers import IterativeStratification
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

def preprocess_data(file_paths):
    # Validate input paths
    if isinstance(file_paths, dict):
        train_path = file_paths.get("train")
        test_path = file_paths.get("test")
    elif isinstance(file_paths, (list, tuple)):
        if len(file_paths) < 2:
            raise ValueError("Expected at least [train_path, test_path]")
        train_path, test_path = file_paths[:2]
    else:
        raise ValueError("file_paths must be a dict or list/tuple")

    for p in (train_path, test_path):
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load data
    train_df = pd.read_csv(train_path)
    test_df  = pd.read_csv(test_path)
    if train_df.empty or test_df.empty:
        raise ValueError("Train or test data is empty")

    # Drop id and duplicates
    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    # Check targets
    targets = ["Pastry","Z_Scratch","K_Scatch","Stains","Dirtiness","Bumps","Other_Faults"]
    for t in targets:
        if t not in train_df.columns:
            raise KeyError(f"Missing target column: {t}")

    # Feature engineering
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (df["X_Perimeter"] + df["Y_Perimeter"]) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = df["Sum_of_Luminosity"] / df["Pixels_Areas"]

    # Separate features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)

    # Process test features (not returned)
    X_test_full = test_df.copy()

    # Correlation filter
    corr = X.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    to_drop = [col for col in upper.columns if (upper[col] > 0.95).any()]
    X.drop(columns=to_drop, inplace=True)
    X_test_full.drop(columns=to_drop, inplace=True, errors="ignore")

    # Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep = X.columns[vt.get_support()]
    X = X[keep]
    X_test_full = X_test_full[keep]

    # Tree-based feature selection
    clf = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42), n_jobs=-1)
    clf.fit(X, y)
    imps = np.mean([est.feature_importances_ for est in clf.estimators_], axis=0)
    thresh = imps.mean()
    sel_idx = np.where(imps >= thresh)[0]
    sel_feats = X.columns[sel_idx]
    X = X[sel_feats]
    X_test_full = X_test_full[sel_feats]

    # Train/validation split
    if HAS_ITER:
        strat = IterativeStratification(2, order=1)
        train_idx, val_idx = next(strat.split(X.values, y.values))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val   = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val   = y.iloc[val_idx].reset_index(drop=True)
    else:
        strat_lbl = y.apply(lambda row: "".join(row.astype(int).astype(str)), axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv"
        }
        X_train, X_test, y_train, y_test = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_008db28b.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_fdbe3cad.py ---
import os
import sys
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from skmultilearn.model_selection import IterativeStratification

def preprocess_data(file_paths):
    """
    Preprocess data for steel_plate_defect_prediction.
    Returns: X_train, X_val, y_train, y_val, X_test
    """
    # Normalize input
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) < 3:
            raise ValueError("Expected 3 paths: train, test, sample_submission")
        train_path, test_path, sample_path = file_paths[:3]
    elif isinstance(file_paths, dict):
        train_path = file_paths.get("train")
        test_path = file_paths.get("test")
        sample_path = file_paths.get("sample")
    else:
        raise ValueError("file_paths must be list or dict")

    # Check existence
    for p in (train_path, test_path, sample_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load data
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    _ = pd.read_csv(sample_path)  # just validate

    # Validate
    if train_df.empty or test_df.empty:
        raise ValueError("Train or test dataframe is empty")

    # Data cleaning
    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    # Target columns
    target_cols = [
        "Pastry", "Z_Scratch", "K_Scatch",
        "Stains", "Dirtiness", "Bumps", "Other_Faults"
    ]
    for t in target_cols:
        if t not in train_df.columns:
            raise KeyError(f"Missing target column: {t}")

    # Feature engineering
    feats = [
        ("X_Range", "X_Maximum", "X_Minimum", lambda a, b: a - b),
        ("Y_Range", "Y_Maximum", "Y_Minimum", lambda a, b: a - b),
        ("Perimeter_to_Area", ["X_Perimeter", "Y_Perimeter"], "Pixels_Areas",
         lambda a, b: (a[0] + a[1]) / b),
        ("Luminosity_per_Pixel", "Sum_of_Luminosity", "Pixels_Areas",
         lambda a, b: a / b)
    ]
    for name, num1, num2, fn in feats:
        if isinstance(num1, list):
            train_df[name] = fn([train_df[c] for c in num1], train_df[num2])
            test_df[name]  = fn([test_df[c]  for c in num1], test_df[num2])
        else:
            train_df[name] = fn(train_df[num1], train_df[num2])
            test_df[name]  = fn(test_df[num1], test_df[num2])

    # Split features and targets
    X = train_df.drop(columns=target_cols)
    y = train_df[target_cols]
    X_test = test_df.copy()

    # 1) Correlation filter
    corr = X.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]
    X.drop(columns=to_drop, inplace=True)
    X_test.drop(columns=to_drop, inplace=True)

    # 2) Low variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    sel = vt.get_support(indices=True)
    cols_kept = X.columns[sel]
    X = X[cols_kept]
    X_test = X_test[cols_kept]

    # 3) Tree-based feature importance
    rf = MultiOutputClassifier(
        RandomForestClassifier(n_estimators=100, random_state=42),
        n_jobs=-1
    )
    rf.fit(X, y)
    # average importances across outputs
    importances = np.mean(
        [est.feature_importances_ for est in rf.estimators_], axis=0
    )
    thresh = np.mean(importances)
    keep_idx = np.where(importances >= thresh)[0]
    final_feats = X.columns[keep_idx]
    X = X[final_feats]
    X_test = X_test[final_feats]

    # 4) Iterative stratification split
    stratifier = IterativeStratification(n_splits=2, order=1)
    train_idx, val_idx = next(stratifier.split(X.values, y.values))
    X_train = X.iloc[train_idx].reset_index(drop=True)
    X_val   = X.iloc[val_idx].reset_index(drop=True)
    y_train = y.iloc[train_idx].reset_index(drop=True)
    y_val   = y.iloc[val_idx].reset_index(drop=True)

    return X_train, X_val, y_train, y_val, X_test

if __name__ == "__main__":
    try:
        file_paths = [
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv",
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv",
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/sample_submission.csv"
        ]
        X_tr, X_val, y_tr, y_val, X_test = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/temp_exec_fdbe3cad.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/states/final_assembled_code.py ---
#!/usr/bin/env python3
# -*- coding: utf-8

import os
import sys
import pandas as pd
import numpy as np

from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# Try to import iterative stratification for train/val split
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

# Try to import iterative stratification for CV
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
    HAS_MSKF = True
except ImportError:
    HAS_MSKF = False


def preprocess_data(train_path, test_path):
    """
    Load and preprocess training and test data.
    Returns:
        X_train, X_val, y_train, y_val, X_test, test_ids
    """
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Capture test IDs before dropping
    if "id_code" in test_df.columns:
        test_ids = test_df["id_code"].values
    elif "id" in test_df.columns:
        test_ids = test_df["id"].values
    else:
        test_ids = np.arange(len(test_df))

    # Drop identifier columns from features
    drop_id_cols = [c for c in ("id_code", "id") if c in train_df.columns]
    if drop_id_cols:
        train_df = train_df.drop(columns=drop_id_cols)
    drop_id_cols_test = [c for c in ("id_code", "id") if c in test_df.columns]
    if drop_id_cols_test:
        test_df = test_df.drop(columns=drop_id_cols_test)

    # Remove duplicates in train
    train_df.drop_duplicates(inplace=True)

    # Define target columns and verify
    targets = [
        "Pastry", "Z_Scratch", "K_Scatch", "Stains",
        "Dirtiness", "Bumps", "Other_Faults"
    ]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split into features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test = test_df.copy()

    # 1) Correlation filter
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [col for col in upper.columns if any(upper[col] > 0.95)]
    if drop_cols:
        X.drop(columns=drop_cols, inplace=True)
        X_test.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_feats = X.columns[vt.get_support()]
    X = X[keep_feats]
    X_test = X_test[keep_feats]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_],
        axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test = X_test[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val, X_test, test_ids


def train_and_predict(X_train, y_train, X_test,
                      n_splits=5, random_state=42):
    """
    Perform cross-validated training and return test predictions
    and the final fitted model.
    """
    fold_scores = []
    if HAS_MSKF:
        splitter = MultilabelStratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train, y_train)
    else:
        splitter = KFold(
            n_splits=n_splits, shuffle=True, random_state=random_state
        )
        splits = splitter.split(X_train)

    for fold, (tr_idx, val_idx) in enumerate(splits, start=1):
        X_tr = X_train.iloc[tr_idx]
        y_tr = y_train.iloc[tr_idx]
        X_val = X_train.iloc[val_idx]
        y_val = y_train.iloc[val_idx]

        clf = OneVsRestClassifier(
            LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
        )
        clf.fit(X_tr, y_tr)
        val_pred = clf.predict_proba(X_val)
        score = roc_auc_score(y_val, val_pred, average='macro')
        fold_scores.append(score)
        print(f"Fold {fold} ROC AUC: {score:.4f}")

    mean_score = np.mean(fold_scores)
    print(f"Mean CV ROC AUC: {mean_score:.4f}")

    final_clf = OneVsRestClassifier(
        LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
    )
    final_clf.fit(X_train, y_train)
    test_preds = final_clf.predict_proba(X_test)

    return test_preds, final_clf


if __name__ == "__main__":
    try:
        # Paths
        train_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/train.csv"
        )
        test_path = (
            "/home/shiinehata/Desktop/"
            "iSE/iSE_AutoML/model_eval/datasets/"
            "steel_plate_defect_prediction/test.csv"
        )
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/"
            "iSE_AutoML/RREFACTORED/runs/"
            "run_20250807_124725_767d46fe/submission.csv"
        )

        # Preprocess
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(
            train_path, test_path
        )
        print("Preprocessing completed.")

        # Train, CV, and predict
        preds, final_model = train_and_predict(X_train, y_train, X_test)

        # Hold-out validation
        val_pred = final_model.predict_proba(X_val)
        holdout_score = roc_auc_score(y_val, val_pred, average='macro')
        print(f"Hold-out Validation ROC AUC: {holdout_score:.4f}")

        # Build submission DataFrame
        defect_cols = y_train.columns.tolist()
        submission = pd.DataFrame({"id_code": test_ids})
        for idx, col in enumerate(defect_cols):
            submission[col] = preds[:, idx]

        # Ensure output directory exists and save
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/states/final_assembled_code.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/states/preprocessing_code_response.py ---
import os
import sys
import pandas as pd
import numpy as np
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split

# Try to import iterative stratification
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
    HAS_ITER = True
except ImportError:
    HAS_ITER = False

def preprocess_data(file_paths):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict or list/tuple with train and test csv paths.
    Returns:
        X_train, X_val, y_train, y_val
    """
    # Validate input
    if isinstance(file_paths, dict):
        train_path = file_paths.get("train") or file_paths.get("train.csv")
        test_path = file_paths.get("test") or file_paths.get("test.csv")
    elif isinstance(file_paths, (list, tuple)):
        if len(file_paths) < 2:
            raise ValueError("Expected at least [train_path, test_path]")
        train_path, test_path = file_paths[:2]
    else:
        raise ValueError("file_paths must be a dict or list/tuple")

    # Check file existence
    for p in (train_path, test_path):
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load datasets
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    if train_df.empty:
        raise ValueError("Train data is empty")
    if test_df.empty:
        raise ValueError("Test data is empty")

    # Drop 'id' and duplicates
    for df in (train_df, test_df):
        if "id" in df.columns:
            df.drop(columns=["id"], inplace=True)
    train_df.drop_duplicates(inplace=True)

    # Verify target columns
    targets = ["Pastry", "Z_Scratch", "K_Scatch", "Stains",
               "Dirtiness", "Bumps", "Other_Faults"]
    missing = [t for t in targets if t not in train_df.columns]
    if missing:
        raise KeyError(f"Missing target columns: {missing}")

    # Feature engineering on both sets
    for df in (train_df, test_df):
        df["X_Range"] = df["X_Maximum"] - df["X_Minimum"]
        df["Y_Range"] = df["Y_Maximum"] - df["Y_Minimum"]
        df["Perimeter_to_Area"] = (
            df["X_Perimeter"] + df["Y_Perimeter"]
        ) / df["Pixels_Areas"]
        df["Luminosity_per_Pixel"] = (
            df["Sum_of_Luminosity"] / df["Pixels_Areas"]
        )

    # Split features and labels
    X = train_df.drop(columns=targets)
    y = train_df[targets].reset_index(drop=True)
    X_test_full = test_df.copy()

    # 1) Correlation filter
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    drop_cols = [c for c in upper.columns if any(upper[c] > 0.95)]
    X.drop(columns=drop_cols, inplace=True)
    X_test_full.drop(columns=drop_cols, inplace=True, errors="ignore")

    # 2) Variance filter
    vt = VarianceThreshold(threshold=0.01)
    vt.fit(X)
    keep_features = X.columns[vt.get_support()]
    X = X[keep_features]
    X_test_full = X_test_full[keep_features]

    # 3) Tree-based feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    multi_rf = MultiOutputClassifier(rf, n_jobs=-1)
    multi_rf.fit(X, y)
    importances = np.mean(
        [est.feature_importances_ for est in multi_rf.estimators_], axis=0
    )
    thresh = importances.mean()
    selected_idxs = np.where(importances >= thresh)[0]
    selected_feats = X.columns[selected_idxs]
    X = X[selected_feats]
    X_test_full = X_test_full[selected_feats]

    # Train/validation split
    if HAS_ITER:
        msss = MultilabelStratifiedShuffleSplit(
            n_splits=1, test_size=0.2, random_state=42
        )
        train_idx, val_idx = next(msss.split(X, y))
        X_train = X.iloc[train_idx].reset_index(drop=True)
        X_val = X.iloc[val_idx].reset_index(drop=True)
        y_train = y.iloc[train_idx].reset_index(drop=True)
        y_val = y.iloc[val_idx].reset_index(drop=True)
    else:
        # Fallback stratify by concatenated labels
        strat_lbl = y.astype(int).astype(str).agg("".join, axis=1)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=strat_lbl
        )

    return X_train, X_val, y_train, y_val

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv"
        }
        X_train, X_test, y_train, y_test = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/states/modeling_code_response.py ---
import sys
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.multiclass import OneVsRestClassifier
from lightgbm import LGBMClassifier

# try to import iterative stratification
try:
    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
    HAS_MSKF = True
except ImportError:
    HAS_MSKF = False

def train_and_predict(X, y, X_test, n_splits=5, random_state=42):
    oof_preds = np.zeros((X.shape[0], y.shape[1]))
    test_preds = np.zeros((X_test.shape[0], y.shape[1]))
    if HAS_MSKF:
        splitter = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    else:
        splitter = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    for fold, (tr_idx, val_idx) in enumerate(splitter.split(X, y)):
        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]
        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]
        clf = OneVsRestClassifier(
            LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
        )
        clf.fit(X_tr, y_tr)
        oof_preds[val_idx] = clf.predict_proba(X_val)
        fold_score = roc_auc_score(y_val, oof_preds[val_idx])
        print(f"Fold {fold+1} ROC AUC: {fold_score:.4f}")
        test_preds += clf.predict_proba(X_test) / n_splits
    full_clf = OneVsRestClassifier(
        LGBMClassifier(random_state=random_state, n_jobs=-1, verbose=-1)
    )
    full_clf.fit(X, y)
    final_test_preds = full_clf.predict_proba(X_test)
    # you can choose to return averaged CV preds or final-fit preds
    return final_test_preds

if __name__ == "__main__":
    try:
        from preprocess import preprocess_data  # assumes preprocessing code is in preprocess.py
        # specify file paths
        train_path = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/train.csv"
        test_path  = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/test.csv"
        sample_path = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/steel_plate_defect_prediction/sample_submission.csv"
        # load and preprocess
        X_train, X_test, y_train, y_test = preprocess_data([train_path, test_path])
        # train and predict
        preds = train_and_predict(X_train, y_train, X_test)
        # build submission
        sample_sub = pd.read_csv(sample_path)
        defect_cols = y_train.columns.tolist()
        submission = sample_sub.copy()
        for i, col in enumerate(defect_cols):
            submission[col] = preds[:, i]
        submission.to_csv("submission.csv", index=False)
        print("Submission saved to submission.csv")
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124725_767d46fe/generation_iter_0/states/modeling_code_response.py ---

--- START FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_3ce48765.py ---
import os
import sys
import zipfile
import tempfile

import pandas as pd
import numpy as np
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms


class DogBreedDataset(Dataset):
    def __init__(self, items, transform=None, is_test=False):
        self.items = items
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        path, label = self.items[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        if self.is_test:
            return label, img  # label is image id
        else:
            return img, label  # label is integer class


def preprocess_data(file_paths):
    # normalize input to dict
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) != 4:
            raise ValueError("Expected 4 paths: train_zip, test_zip, labels_csv, submission_csv")
        keys = ["train_zip", "test_zip", "labels_csv", "submission_csv"]
        file_paths = dict(zip(keys, file_paths))
    elif not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a list or dict")

    train_zip = file_paths.get("train_zip")
    test_zip = file_paths.get("test_zip")
    labels_csv = file_paths.get("labels_csv")
    submission_csv = file_paths.get("submission_csv")

    for p in [train_zip, test_zip, labels_csv]:
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"Path not found: {p}")

    tmp_dir = tempfile.mkdtemp()
    train_dir = os.path.join(tmp_dir, "train")
    test_dir = os.path.join(tmp_dir, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    with zipfile.ZipFile(train_zip, "r") as z:
        z.extractall(train_dir)
    with zipfile.ZipFile(test_zip, "r") as z:
        z.extractall(test_dir)

    df = pd.read_csv(labels_csv)
    if "id" not in df.columns or "breed" not in df.columns:
        raise ValueError("labels.csv must contain 'id' and 'breed' columns")

    df["file_path"] = df["id"].apply(lambda x: os.path.join(train_dir, f"{x}.jpg"))
    df = df[df["file_path"].apply(os.path.exists)].reset_index(drop=True)
    df = df.drop(columns=["id"])  # drop unique identifier

    le = LabelEncoder()
    df["breed_idx"] = le.fit_transform(df["breed"])
    df = df.drop(columns=["breed"])

    paths = df["file_path"].values
    labels = df["breed_idx"].values
    train_p, val_p, train_l, val_l = train_test_split(
        paths, labels, test_size=0.2, stratify=labels, random_state=42
    )

    train_items = list(zip(train_p.tolist(), train_l.tolist()))
    val_items = list(zip(val_p.tolist(), val_l.tolist()))

    test_files = [f for f in os.listdir(test_dir) if f.lower().endswith(".jpg")]
    test_ids = [os.path.splitext(f)[0] for f in test_files]
    test_paths = [os.path.join(test_dir, f"{tid}.jpg") for tid in test_ids]
    test_items = list(zip(test_paths, test_ids))

    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std = [0.229, 0.224, 0.225]

    train_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(imagenet_mean, imagenet_std)
    ])
    eval_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(imagenet_mean, imagenet_std)
    ])

    train_ds = DogBreedDataset(train_items, transform=train_transform, is_test=False)
    val_ds = DogBreedDataset(val_items, transform=eval_transform, is_test=False)
    test_ds = DogBreedDataset(test_items, transform=eval_transform, is_test=True)

    return train_ds, val_ds, test_ds, le


if __name__ == "__main__":
    try:
        file_paths = {
            "train_zip": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/train.zip",
            "test_zip": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/test.zip",
            "labels_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/labels.csv",
            "submission_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/sample_submission.csv"
        }
        train_ds, val_ds, test_ds, label_encoder = preprocess_data(file_paths)
        print(f"Train size: {len(train_ds)}")
        print(f"Validation size: {len(val_ds)}")
        print(f"Test size: {len(test_ds)}")
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_3ce48765.py ---

--- START FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_b538f6ed.py ---
import os
import sys
import zipfile
import tempfile

import pandas as pd
import numpy as np
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms

class DogBreedDataset(Dataset):
    def __init__(self, items, transform=None, is_test=False):
        self.items = items
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        path, label = self.items[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return (img, label) if not self.is_test else (label, img)

def preprocess_data(file_paths):
    # Normalize input type
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) != 4:
            raise ValueError("Expected 4 paths: train_zip, test_zip, labels_csv, sample_submission")
        keys = ["train_zip", "test_zip", "labels_csv", "sample_submission"]
        file_paths = dict(zip(keys, file_paths))
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict or list of length 4")
    train_zip = file_paths.get("train_zip")
    test_zip = file_paths.get("test_zip")
    labels_csv = file_paths.get("labels_csv")
    sample_sub = file_paths.get("sample_submission")
    for p in (train_zip, test_zip, labels_csv):
        if not p or not os.path.isfile(p):
            raise FileNotFoundError(f"Path not found: {p}")

    # Extract images
    work_dir = tempfile.mkdtemp()
    train_dir = os.path.join(work_dir, "train")
    test_dir = os.path.join(work_dir, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)
    with zipfile.ZipFile(train_zip, 'r') as z:
        z.extractall(train_dir)
    with zipfile.ZipFile(test_zip, 'r') as z:
        z.extractall(test_dir)

    # Load labels
    df = pd.read_csv(labels_csv)
    if not {"id", "breed"}.issubset(df.columns):
        raise ValueError("labels.csv must contain 'id' and 'breed'")
    df["file_path"] = df["id"].apply(lambda x: os.path.join(train_dir, f"{x}.jpg"))
    df = df[df["file_path"].apply(os.path.isfile)].reset_index(drop=True)
    df = df.drop(columns=["id"])
    le = LabelEncoder()
    df["breed_idx"] = le.fit_transform(df["breed"])
    df = df.drop(columns=["breed"])

    # Split train/val
    X = df["file_path"].values
    y = df["breed_idx"].values
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    train_items = list(zip(X_train.tolist(), y_train.tolist()))
    val_items = list(zip(X_val.tolist(), y_val.tolist()))

    # Prepare test set
    test_files = [f for f in os.listdir(test_dir) if f.lower().endswith(".jpg")]
    test_items = []
    for fname in test_files:
        img_id = os.path.splitext(fname)[0]
        path = os.path.join(test_dir, fname)
        if os.path.isfile(path):
            test_items.append((path, img_id))

    # Transforms
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    train_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    eval_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    train_ds = DogBreedDataset(train_items, transform=train_transform, is_test=False)
    val_ds = DogBreedDataset(val_items, transform=eval_transform, is_test=False)
    test_ds = DogBreedDataset(test_items, transform=eval_transform, is_test=True)

    return train_ds, val_ds, test_ds, le

if __name__ == "__main__":
    try:
        file_paths = [
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/train.zip",
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/test.zip",
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/labels.csv",
            "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/sample_submission.csv"
        ]
        train_ds, val_ds, test_ds, encoder = preprocess_data(file_paths)
        print(f"Train set size: {len(train_ds)}")
        print(f"Validation set size: {len(val_ds)}")
        print(f"Test set size: {len(test_ds)}")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_b538f6ed.py ---

--- START FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_4238bc33.py ---
import os
import sys
import zipfile
import tempfile

import pandas as pd
import numpy as np
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms

class DogBreedDataset(Dataset):
    def __init__(self, items, transform=None, is_test=False):
        self.items = items
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        path, label = self.items[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return (img, label) if not self.is_test else (label, img)

def preprocess_data(file_paths):
    # normalize input
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) != 4:
            raise ValueError("Expected 4 paths: train_zip, test_zip, labels_csv, sample_submission")
        keys = ["train_zip", "test_zip", "labels_csv", "sample_submission"]
        file_paths = dict(zip(keys, file_paths))
    elif not isinstance(file_paths, dict):
        raise ValueError("file_paths must be list or dict")

    train_zip = file_paths.get("train_zip")
    test_zip = file_paths.get("test_zip")
    labels_csv = file_paths.get("labels_csv")
    # validation only on train; sample_submission provided for format
    for p in (train_zip, test_zip, labels_csv):
        if not p or not os.path.exists(p):
            raise FileNotFoundError(f"Path not found: {p}")

    tmp = tempfile.mkdtemp()
    train_dir = os.path.join(tmp, "train")
    test_dir = os.path.join(tmp, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)
    with zipfile.ZipFile(train_zip, 'r') as z:
        z.extractall(train_dir)
    with zipfile.ZipFile(test_zip, 'r') as z:
        z.extractall(test_dir)

    df = pd.read_csv(labels_csv)
    if not {"id", "breed"}.issubset(df.columns):
        raise ValueError("labels.csv must contain 'id' and 'breed'")
    df["file_path"] = df["id"].apply(lambda x: os.path.join(train_dir, f"{x}.jpg"))
    df = df[df["file_path"].apply(os.path.exists)].reset_index(drop=True)
    df = df.drop(columns=["id"])

    le = LabelEncoder()
    df["breed_idx"] = le.fit_transform(df["breed"])
    df = df.drop(columns=["breed"])

    X = df["file_path"].values
    y = df["breed_idx"].values
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    train_items = list(zip(X_train.tolist(), y_train.tolist()))
    val_items = list(zip(X_val.tolist(), y_val.tolist()))

    test_files = [f for f in os.listdir(test_dir) if f.lower().endswith(".jpg")]
    test_ids = [os.path.splitext(f)[0] for f in test_files]
    test_paths = [os.path.join(test_dir, f"{tid}.jpg") for tid in test_ids]
    test_items = list(zip(test_paths, test_ids))

    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    train_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    eval_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    train_ds = DogBreedDataset(train_items, transform=train_transform, is_test=False)
    val_ds = DogBreedDataset(val_items, transform=eval_transform, is_test=False)
    test_ds = DogBreedDataset(test_items, transform=eval_transform, is_test=True)

    return train_ds, val_ds, test_ds, le

if __name__ == "__main__":
    try:
        file_paths = {
            "train_zip": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/train.zip",
            "test_zip": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/test.zip",
            "labels_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/labels.csv",
            "sample_submission": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/sample_submission.csv"
        }
        train_ds, val_ds, test_ds, encoder = preprocess_data(file_paths)
        print(f"Train set size: {len(train_ds)}")
        print(f"Validation set size: {len(val_ds)}")
        print(f"Test set size: {len(test_ds)}")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_4238bc33.py ---

--- START FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_3ca9bccb.py ---
# import necessary libraries
import os
import sys
import zipfile
import tempfile

import pandas as pd
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms

class DogBreedDataset(Dataset):
    def __init__(self, items, transform=None, is_test=False):
        self.items = items
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        path, label = self.items[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return (img, label) if not self.is_test else (img, label)

def preprocess_data(file_paths):
    # normalize input
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) != 4:
            raise ValueError("Expected 4 paths: train_zip, test_zip, labels_csv, sample_submission")
        keys = ["train_zip", "test_zip", "labels_csv", "sample_submission"]
        file_paths = dict(zip(keys, file_paths))
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict or list of length 4")

    # assign
    train_zip = file_paths.get("train_zip")
    test_zip = file_paths.get("test_zip")
    labels_csv = file_paths.get("labels_csv")
    # sample_sub = file_paths.get("sample_submission")  # not used for preprocessing

    # verify existence or try alternate folder name
    def resolve_path(p):
        if os.path.isfile(p):
            return p
        alt = p.replace("dog_breed_classification", "dog_breed_identification")
        if os.path.isfile(alt):
            return alt
        raise FileNotFoundError(f"Cannot find file: {p}")
    train_zip = resolve_path(train_zip)
    test_zip = resolve_path(test_zip)
    labels_csv = resolve_path(labels_csv)

    # create working dirs
    work_dir = tempfile.mkdtemp()
    train_dir = os.path.join(work_dir, "train")
    test_dir = os.path.join(work_dir, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # extract zips
    with zipfile.ZipFile(train_zip, 'r') as z:
        z.extractall(train_dir)
    with zipfile.ZipFile(test_zip, 'r') as z:
        z.extractall(test_dir)

    # map train images
    train_map = {}
    for root, _, files in os.walk(train_dir):
        for fname in files:
            if fname.lower().endswith(".jpg"):
                img_id = os.path.splitext(fname)[0]
                train_map[img_id] = os.path.join(root, fname)

    # load labels
    df = pd.read_csv(labels_csv)
    if not {"id", "breed"}.issubset(df.columns):
        raise ValueError("labels.csv must contain 'id' and 'breed'")
    # keep only existing files
    df["file_path"] = df["id"].map(train_map)
    df = df.dropna(subset=["file_path"]).reset_index(drop=True)
    df = df.drop(columns=["id"])

    # encode labels
    le = LabelEncoder()
    df["breed_idx"] = le.fit_transform(df["breed"])
    df = df.drop(columns=["breed"])

    # split train/val stratified
    X = df["file_path"].values
    y = df["breed_idx"].values
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    train_items = list(zip(X_train.tolist(), y_train.tolist()))
    val_items = list(zip(X_val.tolist(), y_val.tolist()))

    # map test images
    test_items = []
    for root, _, files in os.walk(test_dir):
        for fname in files:
            if fname.lower().endswith(".jpg"):
                img_id = os.path.splitext(fname)[0]
                path = os.path.join(root, fname)
                test_items.append((path, img_id))

    # define transforms
    mean = [0.485, 0.456, 0.406]
    std  = [0.229, 0.224, 0.225]
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    eval_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    train_ds = DogBreedDataset(train_items, transform=train_transform, is_test=False)
    val_ds   = DogBreedDataset(val_items,   transform=eval_transform, is_test=False)
    test_ds  = DogBreedDataset(test_items,  transform=eval_transform, is_test=True)

    return train_ds, val_ds, test_ds, le

# Test the function
if __name__ == "__main__":
    try:
        file_paths = {
            "train_zip": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/train.zip",
            "test_zip":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/test.zip",
            "labels_csv":" /home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/labels.csv",
            "sample_submission": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/sample_submission.csv"
        }
        train_ds, val_ds, test_ds, encoder = preprocess_data(file_paths)
        print(f"Train set size: {len(train_ds)}")
        print(f"Validation set size: {len(val_ds)}")
        print(f"Test set size: {len(test_ds)}")
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_3ca9bccb.py ---

--- START FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_0b4feaa7.py ---
# import necessary libraries
import os
import sys
import zipfile
import tempfile

import pandas as pd
import numpy as np
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms


class DogBreedDataset(Dataset):
    def __init__(self, items, transform=None):
        """
        items: list of tuples (img_path, label_or_id)
        transform: torchvision transforms
        """
        self.items = items
        self.transform = transform
        # determine if this is test data by checking type of second element
        self.is_test = not isinstance(self.items[0][1], int)

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        img_path, lbl = self.items[idx]
        img = Image.open(img_path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        if self.is_test:
            return lbl, img  # lbl here is image id
        else:
            return img, lbl   # lbl is encoded breed


def preprocess_data(file_paths):
    """
    file_paths: dict or list containing paths to
      [train_zip, test_zip, labels_csv, sample_submission_csv]
    Returns: train_dataset, val_dataset, test_dataset, label_encoder
    """
    # map list to dict if necessary
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) != 4:
            raise ValueError("Expected 4 paths: train.zip, test.zip, labels.csv, sample_submission.csv")
        train_zip, test_zip, labels_csv, sample_sub_csv = file_paths
    elif isinstance(file_paths, dict):
        train_zip = file_paths.get("train_zip")
        test_zip = file_paths.get("test_zip")
        labels_csv = file_paths.get("labels_csv")
        sample_sub_csv = file_paths.get("submission_csv")
    else:
        raise ValueError("file_paths must be a list or dict")

    # validate paths
    for p in [train_zip, test_zip, labels_csv]:
        if not os.path.exists(p):
            raise FileNotFoundError(f"Path not found: {p}")

    # create temp dirs
    tmp_root = tempfile.mkdtemp()
    train_dir = os.path.join(tmp_root, "train")
    test_dir = os.path.join(tmp_root, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # extract zip files
    with zipfile.ZipFile(train_zip, "r") as z:
        z.extractall(train_dir)
    with zipfile.ZipFile(test_zip, "r") as z:
        z.extractall(test_dir)

    # load labels
    df = pd.read_csv(labels_csv)
    if "id" not in df.columns or "breed" not in df.columns:
        raise ValueError("labels.csv must contain 'id' and 'breed' columns")

    # build full image paths and filter missing
    df["file_path"] = df["id"].apply(lambda x: os.path.join(train_dir, f"{x}.jpg"))
    df = df[df["file_path"].apply(os.path.exists)].reset_index(drop=True)

    # encode breeds
    le = LabelEncoder()
    df["breed_idx"] = le.fit_transform(df["breed"])

    # prepare items list
    items = list(zip(df["file_path"].tolist(), df["breed_idx"].tolist()))

    # stratified split
    paths = df["file_path"].values
    labels = df["breed_idx"].values
    train_paths, val_paths, train_lbls, val_lbls = train_test_split(
        paths, labels, test_size=0.2, stratify=labels, random_state=42
    )
    train_items = list(zip(train_paths.tolist(), train_lbls.tolist()))
    val_items = list(zip(val_paths.tolist(), val_lbls.tolist()))

    # prepare test items
    test_filenames = [
        f for f in os.listdir(test_dir) if f.lower().endswith(".jpg")
    ]
    test_ids = [os.path.splitext(f)[0] for f in test_filenames]
    test_paths = [os.path.join(test_dir, f"{tid}.jpg") for tid in test_ids]
    test_items = list(zip(test_paths, test_ids))

    # define transforms
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std = [0.229, 0.224, 0.225]

    train_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(imagenet_mean, imagenet_std)
    ])
    eval_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(imagenet_mean, imagenet_std)
    ])

    # create datasets
    train_dataset = DogBreedDataset(train_items, transform=train_transform)
    val_dataset = DogBreedDataset(val_items, transform=eval_transform)
    test_dataset = DogBreedDataset(test_items, transform=eval_transform)

    return train_dataset, val_dataset, test_dataset, le


# Test the function
if __name__ == "__main__":
    try:
        file_paths = [
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/train.zip',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/test.zip',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/labels.csv',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_classification/sample_submission.csv'
        ]
        train_ds, val_ds, test_ds, label_encoder = preprocess_data(file_paths)
        print(f"Train samples: {len(train_ds)}")
        print(f"Validation samples: {len(val_ds)}")
        print(f"Test samples: {len(test_ds)}")
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/temp_exec_0b4feaa7.py ---

--- START FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/states/preprocessing_code_response.py ---
# import necessary libraries
import os
import sys
import zipfile
import tempfile

import pandas as pd
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms

class DogBreedDataset(Dataset):
    def __init__(self, items, transform=None, is_test=False):
        self.items = items
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        path, label = self.items[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return (img, label) if not self.is_test else (img, label)

def preprocess_data(file_paths):
    # normalize input
    if isinstance(file_paths, (list, tuple)):
        if len(file_paths) != 4:
            raise ValueError("Expected 4 paths: train_zip, test_zip, labels_csv, sample_submission")
        keys = ["train_zip", "test_zip", "labels_csv", "sample_submission"]
        file_paths = dict(zip(keys, file_paths))
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict or list of length 4")

    # assign
    train_zip = file_paths.get("train_zip")
    test_zip = file_paths.get("test_zip")
    labels_csv = file_paths.get("labels_csv")
    # sample_sub = file_paths.get("sample_submission")  # not used for preprocessing

    # verify existence or try alternate folder name
    def resolve_path(p):
        if os.path.isfile(p):
            return p
        alt = p.replace("dog_breed_classification", "dog_breed_identification")
        if os.path.isfile(alt):
            return alt
        raise FileNotFoundError(f"Cannot find file: {p}")
    train_zip = resolve_path(train_zip)
    test_zip = resolve_path(test_zip)
    labels_csv = resolve_path(labels_csv)

    # create working dirs
    work_dir = tempfile.mkdtemp()
    train_dir = os.path.join(work_dir, "train")
    test_dir = os.path.join(work_dir, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # extract zips
    with zipfile.ZipFile(train_zip, 'r') as z:
        z.extractall(train_dir)
    with zipfile.ZipFile(test_zip, 'r') as z:
        z.extractall(test_dir)

    # map train images
    train_map = {}
    for root, _, files in os.walk(train_dir):
        for fname in files:
            if fname.lower().endswith(".jpg"):
                img_id = os.path.splitext(fname)[0]
                train_map[img_id] = os.path.join(root, fname)

    # load labels
    df = pd.read_csv(labels_csv)
    if not {"id", "breed"}.issubset(df.columns):
        raise ValueError("labels.csv must contain 'id' and 'breed'")
    # keep only existing files
    df["file_path"] = df["id"].map(train_map)
    df = df.dropna(subset=["file_path"]).reset_index(drop=True)
    df = df.drop(columns=["id"])

    # encode labels
    le = LabelEncoder()
    df["breed_idx"] = le.fit_transform(df["breed"])
    df = df.drop(columns=["breed"])

    # split train/val stratified
    X = df["file_path"].values
    y = df["breed_idx"].values
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    train_items = list(zip(X_train.tolist(), y_train.tolist()))
    val_items = list(zip(X_val.tolist(), y_val.tolist()))

    # map test images
    test_items = []
    for root, _, files in os.walk(test_dir):
        for fname in files:
            if fname.lower().endswith(".jpg"):
                img_id = os.path.splitext(fname)[0]
                path = os.path.join(root, fname)
                test_items.append((path, img_id))

    # define transforms
    mean = [0.485, 0.456, 0.406]
    std  = [0.229, 0.224, 0.225]
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    eval_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    train_ds = DogBreedDataset(train_items, transform=train_transform, is_test=False)
    val_ds   = DogBreedDataset(val_items,   transform=eval_transform, is_test=False)
    test_ds  = DogBreedDataset(test_items,  transform=eval_transform, is_test=True)

    return train_ds, val_ds, test_ds, le

# Test the function
if __name__ == "__main__":
    try:
        file_paths = {
            "train_zip": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/train.zip",
            "test_zip":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/test.zip",
            "labels_csv":" /home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/labels.csv",
            "sample_submission": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/dog_breed_identification/sample_submission.csv"
        }
        train_ds, val_ds, test_ds, encoder = preprocess_data(file_paths)
        print(f"Train set size: {len(train_ds)}")
        print(f"Validation set size: {len(val_ds)}")
        print(f"Test set size: {len(test_ds)}")
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124847_23a93c15/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_90a5e8a2.py ---
#!/usr/bin/env python3
# Candidate #2: Enhanced preprocessing, early stopping, class probabilities, macro-F1 evaluation

import os
import sys
import random
import numpy as np
import pandas as pd
from scipy import sparse
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import accuracy_score, classification_report, f1_score
from xgboost import XGBClassifier

# Seed everything for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# Paths (preserved as per original script)
TRAIN_CSV = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train.csv'
TEST_CSV  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test.csv'
SAMPLE_SUB = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/sample_submission.csv'
# Output path preserved
OUT_PATH = '/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/RREFACTORED/runs/run_20250807_124601_21be393f/submission.csv'

def clean_text(s: str) -> str:
    # Basic cleaning: lowercase, remove non-alphanumeric except spaces
    s = s.lower()
    return "".join(ch if ch.isalnum() or ch.isspace() else " " for ch in s)

def load_and_preprocess():
    # Load data
    train = pd.read_csv(TRAIN_CSV, encoding='utf-8')
    test  = pd.read_csv(TEST_CSV , encoding='utf-8')
    # Drop unused txt dirs, not used in this script

    # Keep IDs
    train_ids = train['discourse_id'].values
    test_ids  = test['discourse_id'].values
    # Target
    if 'discourse_effectiveness' not in train.columns:
        raise ValueError("Missing target column in train CSV")
    y_raw = train['discourse_effectiveness'].astype(str).values
    train = train.drop(columns=['discourse_id', 'discourse_effectiveness'])
    test  = test .drop(columns=['discourse_id'])

    # Text cleaning
    train['discourse_text'] = train['discourse_text'].astype(str).map(clean_text)
    test ['discourse_text'] = test ['discourse_text'].astype(str).map(clean_text)

    # Simple text stats
    for df in (train, test):
        df['word_count']      = df['discourse_text'].str.split().apply(len)
        df['char_count']      = df['discourse_text'].str.len()
        df['avg_word_length'] = df['char_count'] / df['word_count'].replace(0,1)

    # Compute relative position of each discourse in its essay
    for df in (train, test):
        grp = df.groupby('essay_id').discourse_text.transform('count')
        seq = df.groupby('essay_id').cumcount()
        df['rel_pos'] = seq / (grp.replace(1,2) - 1)

    # Encode target
    le = LabelEncoder()
    y_all = le.fit_transform(y_raw)

    return train, test, y_all, le, train_ids, test_ids

def build_features(train, test, y_all):
    # Text: TF-IDF with stop-word removal, bigrams
    tfidf = TfidfVectorizer(
        ngram_range=(1,2),
        max_features=10000,
        stop_words='english',
        token_pattern=r'\b\w\w+\b'
    )
    X_text_train = tfidf.fit_transform(train['discourse_text'])
    X_text_test  = tfidf.transform(test ['discourse_text'])

    # Chi2 selection
    skb = SelectKBest(chi2, k=5000)
    X_text_train = skb.fit_transform(X_text_train, y_all)
    X_text_test  = skb.transform(X_text_test)

    # One-hot encode discourse_type
    ohe = OneHotEncoder(sparse_output=True, handle_unknown='ignore')
    X_type_train = ohe.fit_transform(train[['discourse_type']])
    X_type_test  = ohe.transform(test [['discourse_type']])

    # Numeric features: word_count, char_count, avg_word_length, rel_pos
    num_cols = ['word_count', 'char_count', 'avg_word_length', 'rel_pos']
    scaler = StandardScaler(with_mean=False)
    X_num_train = scaler.fit_transform(train[num_cols].values)
    X_num_test  = scaler.transform(test[num_cols].values)
    X_num_train = sparse.csr_matrix(X_num_train)
    X_num_test  = sparse.csr_matrix(X_num_test)

    # Stack all
    X_train_full = sparse.hstack([X_text_train, X_type_train, X_num_train], format='csr')
    X_test_full  = sparse.hstack([X_text_test,  X_type_test,  X_num_test ], format='csr')

    return X_train_full, X_test_full

def split_train_val(X, y, groups):
    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)
    train_idx, val_idx = next(sgkf.split(X, y, groups))
    return train_idx, val_idx

def train_and_evaluate(X_train, X_val, y_train, y_val):
    # XGBoost with early stopping on validation set
    model = XGBClassifier(
        objective='multi:softprob',
        num_class=len(np.unique(y_train)),
        random_state=SEED,
        use_label_encoder=False,
        eval_metric='mlogloss',
        learning_rate=0.1,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        n_estimators=1000
    )
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=50,
        verbose=False
    )
    # Predictions
    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)
    acc = accuracy_score(y_val, y_pred)
    f1m = f1_score(y_val, y_pred, average='macro')
    print(f"Validation Accuracy: {acc:.4f}, Macro-F1: {f1m:.4f}")
    print("Classification Report:\n", classification_report(y_val, y_pred, target_names=le.classes_))
    return model

if __name__ == "__main__":
    try:
        # Load & preprocess
        train_df, test_df, y_all, le, train_ids, test_ids = load_and_preprocess()
        # Build features
        X_full, X_test = build_features(train_df, test_df, y_all)
        groups = train_df['essay_id'].values

        # Split
        train_idx, val_idx = split_train_val(X_full, y_all, groups)
        X_tr, X_val = X_full[train_idx], X_full[val_idx]
        y_tr, y_val = y_all[train_idx], y_all[val_idx]

        # Train & eval
        model = train_and_evaluate(X_tr, X_val, y_tr, y_val)

        # Retrain on full set (train+val)
        model_full = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            random_state=SEED,
            use_label_encoder=False,
            eval_metric='mlogloss',
            learning_rate=0.1,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            n_estimators=model.best_iteration + 10
        )
        model_full.fit(sparse.vstack([X_tr, X_val], format='csr'),
                       np.concatenate([y_tr, y_val]),
                       verbose=False)

        # Predict probabilities on test
        proba = model_full.predict_proba(X_test)
        # Build submission DataFrame
        sub = pd.DataFrame(proba, columns=le.classes_)
        sub.insert(0, 'discourse_id', test_ids)

        # Ensure sample_submission columns order
        sample = pd.read_csv(SAMPLE_SUB, dtype=str)
        cols = sample.columns.tolist()
        sub = sub[cols]
        os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)
        sub.to_csv(OUT_PATH, index=False)
        print(f"Submission saved to: {OUT_PATH}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_90a5e8a2.py ---

--- START FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_b83cc236.py ---
#!/usr/bin/env python3
# Candidate #3: Enhanced pipeline with better text cleaning, essay-position features,
# probability outputs, early stopping, and logging.
import os
import sys
import re
import logging
from collections import defaultdict

import numpy as np
import pandas as pd
import scipy.sparse as sparse

from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from xgboost import XGBClassifier

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure NLTK data is available
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Absolute paths (must be preserved)
TRAIN_CSV = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train.csv'
TEST_CSV  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test.csv'
TRAIN_DIR = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train/'
TEST_DIR  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test/'
SAMPLE_SUB = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/sample_submission.csv'
OUTPUT_SUB = '/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/RREFACTORED/runs/run_20250807_21be393f/submission.csv'

# Text cleaning utilities
STOP_WORDS = set(stopwords.words('english'))
LEMMATIZER = WordNetLemmatizer()
CLEAN_REGEX = re.compile(r'[^a-z0-9\s]')

def clean_text(text: str) -> str:
    """Lowercase, remove non-alphanum, remove stopwords, and lemmatize."""
    text = text.lower()
    text = CLEAN_REGEX.sub(' ', text)
    tokens = text.split()
    tokens = [LEMMATIZER.lemmatize(tok) for tok in tokens if tok not in STOP_WORDS]
    return ' '.join(tokens)

def load_data():
    """Load train/test CSVs and check files."""
    for path in [TRAIN_CSV, TEST_CSV, SAMPLE_SUB]:
        if not os.path.exists(path):
            logger.error(f"Required file not found: {path}")
            sys.exit(1)
    if not os.path.isdir(TRAIN_DIR) or not os.path.isdir(TEST_DIR):
        logger.error("Train/test text directories not found.")
        sys.exit(1)

    train = pd.read_csv(TRAIN_CSV, encoding='utf-8')
    test  = pd.read_csv(TEST_CSV,  encoding='utf-8')
    return train, test

def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    """Clean text, add numeric stats and essay-position features."""
    # Clean discourse text
    df['discourse_text'] = df['discourse_text'].fillna('').astype(str).map(clean_text)

    # Word/char counts
    df['word_count'] = df['discourse_text'].str.split().apply(len)
    df['char_count'] = df['discourse_text'].str.len()
    df['avg_word_length'] = df['char_count'] / df['word_count'].replace(0, 1)

    # Essay-position features
    # We assume discourse elements are in order in the CSV for each essay.
    pos = []
    total = {}
    for essay_id, group in df.groupby('essay_id'):
        n = len(group)
        total[essay_id] = n
        # assign 1..n
        pos.extend(list(range(1, n+1)))
    df['position'] = pos
    df['rel_position'] = df.apply(lambda row: row['position'] / total[row['essay_id']], axis=1)
    return df

def preprocess_data():
    """Full preprocessing: encoding, TF-IDF, chi2 selection, stacking."""
    train_df, test_df = load_data()
    # Keep discourse_id for submission
    train_ids = train_df['discourse_id'].values
    test_ids  = test_df['discourse_id'].values

    # Target
    if 'discourse_effectiveness' not in train_df.columns:
        logger.error("Missing target 'discourse_effectiveness' in train CSV.")
        sys.exit(1)
    y = LabelEncoder().fit_transform(train_df['discourse_effectiveness'].astype(str))
    label_encoder = LabelEncoder()
    label_encoder.fit(train_df['discourse_effectiveness'].astype(str))

    # Drop unused columns
    train_df = train_df.drop(columns=['discourse_id', 'discourse_effectiveness'])
    test_df  = test_df.drop(columns=['discourse_id'])

    # Feature engineering
    train_df = engineer_features(train_df)
    test_df  = engineer_features(test_df)

    # TF-IDF + chi2-select
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=10000)
    X_text_train = tfidf.fit_transform(train_df['discourse_text'])
    X_text_test  = tfidf.transform(test_df['discourse_text'])
    skb = SelectKBest(chi2, k=5000)
    X_text_train = skb.fit_transform(X_text_train, y)
    X_text_test  = skb.transform(X_text_test)

    # One-hot encode discourse_type
    ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')
    X_type_train = ohe.fit_transform(train_df[['discourse_type']])
    X_type_test  = ohe.transform(test_df[['discourse_type']])

    # Numeric features & scaling
    num_cols = ['word_count', 'char_count', 'avg_word_length', 'position', 'rel_position']
    scaler = StandardScaler(with_mean=False)
    X_num_train = scaler.fit_transform(train_df[num_cols])
    X_num_test  = scaler.transform(test_df[num_cols])
    X_num_train = sparse.csr_matrix(X_num_train)
    X_num_test  = sparse.csr_matrix(X_num_test)

    # Stack all
    X_train_full = sparse.hstack([X_text_train, X_type_train, X_num_train], format='csr')
    X_test_full  = sparse.hstack([X_text_test,  X_type_test,  X_num_test ], format='csr')

    # Grouped stratified split (one hold-out fold)
    groups = train_df['essay_id'].values
    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)
    train_idx, val_idx = next(sgkf.split(X_train_full, y, groups))
    X_tr, X_val = X_train_full[train_idx], X_train_full[val_idx]
    y_tr, y_val = y[train_idx], y[val_idx]

    # L1-based feature selection with saga solver
    l1 = LogisticRegression(penalty='l1', solver='saga', C=1.0, random_state=42, max_iter=1000)
    selector = SelectFromModel(l1)
    selector.fit(X_tr, y_tr)
    X_tr_sel = selector.transform(X_tr)
    X_val_sel = selector.transform(X_val)
    X_test_sel = selector.transform(X_test_full)

    return X_tr_sel, X_val_sel, y_tr, y_val, X_test_sel, test_ids, label_encoder

def main():
    try:
        X_tr, X_val, y_tr, y_val, X_test, test_ids, le = preprocess_data()
        logger.info("Preprocessing complete. Training XGBoost with early stopping...")

        # XGBoost classifier with early stopping
        xgb = XGBClassifier(
            objective='multi:softprob', num_class=len(le.classes_),
            use_label_encoder=False, eval_metric='mlogloss',
            random_state=42
        )
        xgb.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=50,
            verbose=10
        )

        # Validation performance
        val_preds = xgb.predict(X_val)
        acc = accuracy_score(y_val, val_preds)
        logger.info(f"Validation Accuracy: {acc:.4f}")
        logger.info("Validation Classification Report:\n" +
                    classification_report(y_val, val_preds, target_names=le.classes_))

        # Retrain on full data with optimal rounds
        best_ntree = xgb.best_ntree_limit or xgb.n_estimators
        logger.info(f"Retraining on full data for {best_ntree} rounds...")
        X_full = sparse.vstack([X_tr, X_val], format='csr')
        y_full = np.concatenate([y_tr, y_val])
        xgb_full = XGBClassifier(
            objective='multi:softprob', num_class=len(le.classes_),
            use_label_encoder=False, eval_metric='mlogloss',
            random_state=42, n_estimators=best_ntree
        )
        xgb_full.fit(X_full, y_full, verbose=False)

        # Test predictions (probabilities)
        proba = xgb_full.predict_proba(X_test)
        # Build submission DataFrame
        col_probs = le.inverse_transform(np.arange(len(le.classes_)))
        sub_df = pd.DataFrame(proba, columns=le.classes_)
        sub_df.insert(0, 'discourse_id', test_ids)

        # Save submission
        os.makedirs(os.path.dirname(OUTPUT_SUB), exist_ok=True)
        sub_df.to_csv(OUTPUT_SUB, index=False)
        logger.info(f"Submission saved to: {OUTPUT_SUB}")

    except Exception as e:
        logger.exception("An error occurred during execution.")
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_b83cc236.py ---

--- START FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_b6c666d5.py ---
#!/usr/bin/env python3
import os
import sys
import re
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier

# Download NLTK resources
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)

# Paths (must remain as absolute)
TRAIN_CSV = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train.csv'
TEST_CSV  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test.csv'
SAMPLE_SUB = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/sample_submission.csv'
OUT_PATH = '/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/RREFACTORED/runs/run_20250807_124601_21be393f/submission.csv'

# Text cleaning: remove punctuation, stopwords, lemmatize
STOP_WORDS = set(stopwords.words('english'))
LEMMATIZER = WordNetLemmatizer()

def clean_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    tokens = nltk.word_tokenize(text)
    tokens = [
        LEMMATIZER.lemmatize(tok)
        for tok in tokens
        if tok.isalpha() and tok not in STOP_WORDS
    ]
    return ' '.join(tokens)

def load_and_preprocess():
    # Load CSVs
    train_df = pd.read_csv(TRAIN_CSV, encoding='utf-8')
    test_df  = pd.read_csv(TEST_CSV, encoding='utf-8')

    # Validate columns
    for df, name in ((train_df, 'train'), (test_df, 'test')):
        for col in ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type']:
            if col not in df.columns:
                raise ValueError(f"Missing column '{col}' in {name} data")

    # Keep IDs for submission
    test_ids = test_df['discourse_id'].values

    # Clean and normalize text
    train_df['clean_text'] = train_df['discourse_text'].astype(str).map(clean_text)
    test_df ['clean_text'] = test_df ['discourse_text'].astype(str).map(clean_text)

    # Numeric text stats
    for df in (train_df, test_df):
        df['word_count'] = df['clean_text'].str.split().apply(len)
        df['char_count'] = df['clean_text'].str.len()
        df['avg_word_length'] = df['char_count'] / df['word_count'].replace(0, 1)

    # Encode target
    le = LabelEncoder()
    y_all = le.fit_transform(train_df['discourse_effectiveness'].astype(str).values)

    # Prepare feature DataFrame
    feature_cols = ['clean_text', 'discourse_type',
                    'word_count', 'char_count', 'avg_word_length']
    df_features = train_df[feature_cols + ['essay_id']].copy()
    df_test_feat = test_df[feature_cols].copy()

    return df_features, y_all, df_test_feat, test_ids, le

def main():
    try:
        # Load and preprocess data
        df_feat, y_all, df_test, test_ids, le = load_and_preprocess()

        # Group-aware stratified split
        groups = df_feat['essay_id'].values
        sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)
        train_idx, val_idx = next(sgkf.split(df_feat, y_all, groups))

        X_train_df = df_feat.iloc[train_idx].drop(columns=['essay_id'])
        y_train    = y_all[train_idx]
        X_val_df   = df_feat.iloc[val_idx].drop(columns=['essay_id'])
        y_val      = y_all[val_idx]

        # Pipeline: text, categorical, numeric
        text_pipe = Pipeline([
            ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=10000))
        ])
        cat_pipe = OneHotEncoder(handle_unknown='ignore', sparse=True)
        num_pipe = Pipeline([
            ('scale', StandardScaler())
        ])
        preprocessor = ColumnTransformer([
            ('text', text_pipe, 'clean_text'),
            ('type', cat_pipe, ['discourse_type']),
            ('num', num_pipe, ['word_count','char_count','avg_word_length']),
        ], sparse_threshold=0.1)

        # XGBoost with early stopping
        xgb = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        pipeline = Pipeline([
            ('pre', preprocessor),
            ('clf', xgb)
        ])

        # Fit with early stopping on validation
        pipeline.fit(
            X_train_df,
            y_train,
            clf__eval_set=[(X_val_df, y_val)],
            clf__early_stopping_rounds=50,
            clf__verbose=False
        )

        # Validation performance
        val_preds = pipeline.predict(X_val_df)
        print("Validation Classification Report:")
        print(classification_report(y_val, val_preds, target_names=le.classes_))
        print(f"Validation Accuracy: {accuracy_score(y_val, val_preds):.4f}")

        # Retrain on full training set using best n_estimators
        best_iter = pipeline.named_steps['clf'].best_iteration
        xgb_full = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss',
            n_estimators=best_iter
        )
        pipeline_full = Pipeline([
            ('pre', preprocessor),
            ('clf', xgb_full)
        ])
        X_full_df = df_feat.drop(columns=['essay_id'])
        pipeline_full.fit(X_full_df, y_all)

        # Predict probabilities on test set
        proba = pipeline_full.predict_proba(df_test)
        # Align columns: sample_submission order is ['discourse_id','Ineffective','Adequate','Effective']
        classes = list(le.classes_)
        idx_I = classes.index('Ineffective')
        idx_A = classes.index('Adequate')
        idx_E = classes.index('Effective')
        submission_df = pd.DataFrame({
            'discourse_id': test_ids,
            'Ineffective': proba[:, idx_I],
            'Adequate':    proba[:, idx_A],
            'Effective':   proba[:, idx_E]
        })

        # Save submission
        os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)
        submission_df.to_csv(OUT_PATH, index=False)
        print(f"Submission saved to: {OUT_PATH}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_b6c666d5.py ---

--- START FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_d4e6443a.py ---
#!/usr/bin/env python3
import os
import sys
import re
import logging
from typing import Dict, Tuple

import numpy as np
import pandas as pd
from scipy import sparse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier

# -----------------------------
# Configuration of file paths
# -----------------------------
DATA_PATHS: Dict[str, str] = {
    'train_csv':         '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train.csv',
    'train_txt_dir':     '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train/',
    'test_csv':          '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test.csv',
    'test_txt_dir':      '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test/',
    'sample_submission':'/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/sample_submission.csv',
    'output_submission':'/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/RREFACTORED/runs/run_20250807_124601_21be393f/submission.csv'
}

# -----------------------------
# Logging configuration
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# -----------------------------
# Utility functions
# -----------------------------
def clean_text(text: str) -> str:
    """Lowercase, remove non-alphanumeric characters, collapse whitespace."""
    text = text.lower()
    # Remove punctuation & non-alphanumeric
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    # Collapse multiple spaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def load_essay_texts(txt_dir: str) -> Dict[str, str]:
    """Read all .txt files in a directory into a dict essay_id -> text."""
    texts = {}
    for fname in os.listdir(txt_dir):
        if not fname.endswith('.txt'):
            continue
        essay_id = os.path.splitext(fname)[0]
        path = os.path.join(txt_dir, fname)
        try:
            with open(path, 'r', encoding='utf-8') as f:
                texts[essay_id] = f.read().lower()
        except Exception as e:
            logger.warning(f"Could not read {path}: {e}")
            texts[essay_id] = ""
    return texts

def compute_position_pct(df: pd.DataFrame, essay_texts: Dict[str, str]) -> np.ndarray:
    """Compute position of each discourse_text in its essay (0-1)."""
    pos = []
    for essay_id, disc in zip(df['essay_id'], df['discourse_text']):
        full = essay_texts.get(essay_id, "")
        idx = full.find(disc)
        if idx >= 0 and len(full) > 0:
            pos.append(idx / len(full))
        else:
            pos.append(0.0)
    return np.array(pos, dtype=float)

# -----------------------------
# Preprocessing
# -----------------------------
def preprocess_data(paths: Dict[str, str]
                   ) -> Tuple[sparse.csr_matrix, sparse.csr_matrix,
                              np.ndarray, np.ndarray,
                              sparse.csr_matrix, np.ndarray,
                              LabelEncoder]:
    # Validate paths
    for key in ['train_csv', 'test_csv', 'sample_submission']:
        if not os.path.isfile(paths[key]):
            raise FileNotFoundError(f"Missing file: {paths[key]}")
    for key in ['train_txt_dir', 'test_txt_dir']:
        if not os.path.isdir(paths[key]):
            raise FileNotFoundError(f"Missing directory: {paths[key]}")

    logger.info("Loading train/test CSVs")
    train_df = pd.read_csv(paths['train_csv'], encoding='utf-8')
    test_df  = pd.read_csv(paths['test_csv'], encoding='utf-8')

    # Save IDs & drop
    train_ids = train_df['discourse_id'].values
    test_ids  = test_df['discourse_id'].values
    train_df = train_df.drop(columns=['discourse_id'])
    test_df  = test_df.drop(columns=['discourse_id'])

    # Clean discourse_text
    logger.info("Cleaning text")
    train_df['discourse_text'] = train_df['discourse_text'].astype(str).map(clean_text)
    test_df ['discourse_text'] = test_df ['discourse_text'].astype(str).map(clean_text)

    # Load full essay texts
    logger.info("Loading full essay text content")
    essay_texts_train = load_essay_texts(paths['train_txt_dir'])
    essay_texts_test  = load_essay_texts(paths['test_txt_dir'])

    # Add basic text stats
    for df in (train_df, test_df):
        df['word_count'] = df['discourse_text'].str.split().apply(len)
        df['char_count'] = df['discourse_text'].str.len()
        df['avg_word_length'] = df['char_count'] / df['word_count'].replace(0, 1)

    # Position in essay
    logger.info("Computing position feature")
    train_pos = compute_position_pct(train_df, essay_texts_train)
    test_pos  = compute_position_pct(test_df,  essay_texts_test)
    train_df['position_pct'] = train_pos
    test_df ['position_pct'] = test_pos

    # Encode target
    logger.info("Encoding target labels")
    y_all = LabelEncoder().fit_transform(train_df['discourse_effectiveness'])
    le = LabelEncoder().fit(train_df['discourse_effectiveness'])
    train_df = train_df.drop(columns=['discourse_effectiveness'])

    # TF-IDF vectorization w/ stop words
    logger.info("TF-IDF feature extraction")
    tfidf = TfidfVectorizer(ngram_range=(1,2),
                            max_features=10000,
                            stop_words='english')
    X_text_train = tfidf.fit_transform(train_df['discourse_text'])
    X_text_test  = tfidf.transform(test_df['discourse_text'])
    skb = SelectKBest(chi2, k=5000)
    X_text_train = skb.fit_transform(X_text_train, y_all)
    X_text_test  = skb.transform(X_text_test)

    # One-hot discourse_type
    logger.info("One-hot encoding discourse_type")
    ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')
    X_type_train = ohe.fit_transform(train_df[['discourse_type']])
    X_type_test  = ohe.transform(test_df[['discourse_type']])

    # Numeric features scaling
    num_cols = ['word_count', 'char_count', 'avg_word_length', 'position_pct']
    logger.info("Scaling numeric features")
    scaler = StandardScaler()
    X_num_train = scaler.fit_transform(train_df[num_cols])
    X_num_test  = scaler.transform(test_df[num_cols])
    X_num_train = sparse.csr_matrix(X_num_train)
    X_num_test  = sparse.csr_matrix(X_num_test)

    # Combine all features
    logger.info("Stacking all features")
    X_full_train = sparse.hstack([X_text_train, X_type_train, X_num_train], format='csr')
    X_full_test  = sparse.hstack([X_text_test,  X_type_test,  X_num_test ], format='csr')

    # Stratified group split
    logger.info("Performing group-stratified split")
    groups = train_df['essay_id'].values
    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)
    train_idx, val_idx = next(sgkf.split(X_full_train, y_all, groups))
    X_train, X_val = X_full_train[train_idx], X_full_train[val_idx]
    y_train, y_val = y_all[train_idx], y_all[val_idx]

    # L1-based feature selection (faster 'saga' solver)
    logger.info("Selecting features via L1 logistic regression")
    l1_lr = LogisticRegression(penalty='l1', solver='saga', C=1.0,
                               random_state=42, max_iter=1000)
    selector = SelectFromModel(l1_lr).fit(X_train, y_train)
    X_train_sel = selector.transform(X_train)
    X_val_sel   = selector.transform(X_val)
    X_test_sel  = selector.transform(X_full_test)

    return X_train_sel, X_val_sel, y_train, y_val, X_test_sel, test_ids, le

# -----------------------------
# Main execution
# -----------------------------
def main():
    try:
        paths = DATA_PATHS
        X_tr, X_val, y_tr, y_val, X_te, test_ids, label_enc = preprocess_data(paths)

        # Train XGBoost with early stopping
        logger.info("Training XGBoost with early stopping")
        xgb = XGBClassifier(
            objective='multi:softprob',
            num_class=len(label_enc.classes_),
            learning_rate=0.1,
            n_estimators=500,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        xgb.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=20,
            verbose=False
        )

        # Validation metrics
        logger.info("Evaluating on validation set")
        val_pred = xgb.predict(X_val)
        val_prob = xgb.predict_proba(X_val)
        logger.info(f"Validation Accuracy: {accuracy_score(y_val, val_pred):.4f}")
        report = classification_report(y_val, val_pred,
                                       target_names=label_enc.classes_)
        logger.info("Classification Report:\n" + report)

        # Retrain on full dataset
        logger.info("Retraining on full data")
        X_full = sparse.vstack([X_tr, X_val], format='csr')
        y_full = np.concatenate([y_tr, y_val])
        xgb_full = XGBClassifier(
            objective='multi:softprob',
            num_class=len(label_enc.classes_),
            learning_rate=0.1,
            n_estimators=xgb.best_iteration + 10,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        xgb_full.fit(X_full, y_full, verbose=False)

        # Predict probabilities for submission
        logger.info("Predicting probabilities for test set")
        probs = xgb_full.predict_proba(X_te)  # shape (n_samples, 3)
        sample_sub = pd.read_csv(paths['sample_submission'], dtype={'discourse_id': str})
        cols = list(sample_sub.columns)
        # Expect ['discourse_id', 'Ineffective', 'Adequate', 'Effective']
        submission = pd.DataFrame(probs, columns=label_enc.classes_)
        submission.insert(0, cols[0], test_ids)

        # Save output
        out_path = paths['output_submission']
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        submission.to_csv(out_path, index=False)
        logger.info(f"Submission saved to: {out_path}")

    except Exception as e:
        logger.exception("Pipeline failed")
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_d4e6443a.py ---

--- START FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_c7ebca65.py ---
#!/usr/bin/env python3
"""
Candidate #3: Enhanced preprocessing with punctuation removal, stop-word filtering,
lemmatization; early stopping in XGBoost; outputs class probabilities matching
sample_submission format; uses logging for transparency.
"""

import os
import re
import logging
import sys

import numpy as np
import pandas as pd

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from scipy import sparse

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import StratifiedKFold

from xgboost import XGBClassifier

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize stop-words and lemmatizer
STOPWORDS = set(stopwords.words('english'))
LEMMATIZER = WordNetLemmatizer()

# Absolute paths (must not change)
TRAIN_CSV = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train.csv'
TEST_CSV = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test.csv'
SAMPLE_SUB = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/sample_submission.csv'

# Output path (parameterized)
OUTPUT_PATH = '/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/RREFACTORED/runs/run_20250807_124601_21be393f/submission.csv'


def clean_text(text: str) -> str:
    """
    Lowercase, remove non-alphanumeric chars, remove stopwords, lemmatize.
    """
    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')
    text = text.lower()
    # Remove punctuation and non-alphanumeric
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]
    lemmas = [LEMMATIZER.lemmatize(t) for t in tokens]
    return " ".join(lemmas)


def preprocess_data(train_csv: str, test_csv: str):
    """
    Reads train/test CSVs, cleans text, builds features:
     - TF-IDF (1-2grams, max 10k) + chi2 selectKBest(5k)
     - One-hot discourse_type
     - numeric stats: word_count, char_count, avg_word_length
    Encodes target labels.
    Returns train/test feature mats, y, LabelEncoder.
    """
    logging.info("Loading data")
    train_df = pd.read_csv(train_csv, encoding='utf-8')
    test_df = pd.read_csv(test_csv, encoding='utf-8')

    # sanity checks
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        for col in ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type']:
            if col not in df.columns:
                raise ValueError(f"Missing column '{col}' in {name} data")

    # extract IDs
    train_ids = train_df['discourse_id'].values
    test_ids = test_df['discourse_id'].values
    train_df = train_df.drop(columns=['discourse_id'])
    test_df = test_df.drop(columns=['discourse_id'])

    # clean text
    logging.info("Cleaning text")
    train_df['discourse_text'] = train_df['discourse_text'].astype(str).map(clean_text)
    test_df['discourse_text'] = test_df['discourse_text'].astype(str).map(clean_text)

    # text stats
    for df in (train_df, test_df):
        df['word_count'] = df['discourse_text'].str.split().apply(len)
        df['char_count'] = df['discourse_text'].str.len()
        df['avg_word_length'] = df['char_count'] / df['word_count'].replace(0, 1)

    # encode target
    if 'discourse_effectiveness' not in train_df.columns:
        raise ValueError("Target 'discourse_effectiveness' missing in train data")
    le = LabelEncoder()
    y = le.fit_transform(train_df['discourse_effectiveness'].astype(str).values)
    train_df = train_df.drop(columns=['discourse_effectiveness'])

    # TF-IDF
    logging.info("Building TF-IDF features")
    tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)
    X_text_train = tfidf.fit_transform(train_df['discourse_text'])
    X_text_test = tfidf.transform(test_df['discourse_text'])

    # chi2 feature selection
    logging.info("Selecting top 5000 TF-IDF features via chi2")
    skb = SelectKBest(chi2, k=5000)
    X_text_train = skb.fit_transform(X_text_train, y)
    X_text_test = skb.transform(X_text_test)

    # one-hot discourse_type
    logging.info("One-hot encoding discourse_type")
    ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')
    X_type_train = ohe.fit_transform(train_df[['discourse_type']])
    X_type_test = ohe.transform(test_df[['discourse_type']])

    # numeric features
    num_cols = ['word_count', 'char_count', 'avg_word_length']
    X_num_train = sparse.csr_matrix(train_df[num_cols].values)
    X_num_test = sparse.csr_matrix(test_df[num_cols].values)

    # combine all
    logging.info("Stacking feature matrices")
    X_train = sparse.hstack([X_text_train, X_type_train, X_num_train], format='csr')
    X_test = sparse.hstack([X_text_test, X_type_test, X_num_test], format='csr')

    return X_train, X_test, y, train_ids, test_ids, le


def main():
    try:
        # Preprocess
        X_train, X_test, y, train_ids, test_ids, le = preprocess_data(TRAIN_CSV, TEST_CSV)

        # Hold-out split (80/20) with stratification on y
        logging.info("Splitting train/validation hold-out")
        X_tr, X_val, y_tr, y_val = train_test_split(
            X_train, y, test_size=0.2,
            stratify=y, random_state=42
        )

        # Train XGBoost with early stopping
        logging.info("Training XGBoostClassifier with early stopping")
        model = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            learning_rate=0.1,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            n_estimators=1000,
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=30,
            verbose=False
        )

        # Validate
        logging.info("Evaluating on validation set")
        val_preds = model.predict(X_val)
        val_proba = model.predict_proba(X_val)
        acc = accuracy_score(y_val, val_preds)
        logging.info(f"Validation Accuracy: {acc:.4f}")
        logging.info("Classification Report:\n" +
                     classification_report(y_val, val_preds, target_names=le.classes_))

        # Retrain on full training data
        logging.info("Retraining on full training data")
        model_full = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            learning_rate=0.1,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            n_estimators=model.best_iteration + 1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        model_full.fit(sparse.vstack([X_tr, X_val]), np.concatenate([y_tr, y_val]), verbose=False)

        # Predict probabilities on test set
        logging.info("Predicting probabilities for test set")
        proba = model_full.predict_proba(X_test)  # shape [n_samples, n_classes]

        # Prepare submission
        logging.info("Building submission DataFrame")
        sample_header = pd.read_csv(SAMPLE_SUB, nrows=0)
        cols = sample_header.columns.tolist()[1:]  # ['Ineffective','Adequate','Effective']
        # map LabelEncoder classes to indices
        class_indices = {cls: idx for idx, cls in enumerate(le.classes_)}
        submission = pd.DataFrame({'discourse_id': test_ids})
        for col in cols:
            if col not in class_indices:
                raise ValueError(f"Expected class '{col}' not in trained LabelEncoder classes: {le.classes_}")
            submission[col] = proba[:, class_indices[col]]

        # Save
        os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
        submission.to_csv(OUTPUT_PATH, index=False)
        logging.info(f"Submission saved to: {OUTPUT_PATH}")

    except Exception as e:
        logging.error(f"An error occurred: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_c7ebca65.py ---

--- START FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_50711109.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import re
import logging
import numpy as np
import pandas as pd
from scipy import sparse
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

def clean_text(text: str) -> str:
    """
    Lowercase, remove non-alphanumeric characters (except spaces), and collapse whitespace.
    """
    text = text.lower()
    # remove everything except letters, numbers, spaces
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    # collapse multiple spaces
    text = re.sub(r"\s+", " ", text).strip()
    return text

def preprocess_data(paths: dict):
    """
    Preprocess training and test data:
      - Read CSVs
      - Clean discourse_text
      - Build TF-IDF features (1-2 grams, stopwords removal)
      - Chi2 select top 5000 text features
      - One-hot encode discourse_type
      - Compute simple numeric features
      - Stack all features
      - StratifiedGroupKFold split (one holdout fold)
      - L1-based feature selection (LogisticRegression with saga)
      - Encode targets
    Returns:
      X_train_sel, X_val_sel, y_train, y_val, X_test_sel, test_ids, label_encoder
    """
    # Required paths
    for key in ['train_csv', 'test_csv', 'sample_submission']:
        if key not in paths or not os.path.exists(paths[key]):
            raise FileNotFoundError(f"Missing or invalid path for '{key}'")

    # Load data
    train_df = pd.read_csv(paths['train_csv'], encoding='utf-8')
    test_df  = pd.read_csv(paths['test_csv'], encoding='utf-8')

    # Ensure required columns present
    for col in ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type']:
        if col not in train_df.columns or col not in test_df.columns:
            raise ValueError(f"Required column '{col}' missing in train or test")

    # Extract and drop IDs
    train_ids = train_df['discourse_id'].values
    test_ids  = test_df['discourse_id'].values
    train_df  = train_df.drop(columns=['discourse_id'])
    test_df   = test_df.drop(columns=['discourse_id'])

    # Clean text
    train_df['discourse_text'] = train_df['discourse_text'].astype(str).apply(clean_text)
    test_df['discourse_text']  = test_df['discourse_text'].astype(str).apply(clean_text)

    # Numeric text stats
    for df in (train_df, test_df):
        df['word_count'] = df['discourse_text'].str.split().apply(len)
        df['char_count'] = df['discourse_text'].str.len()
        df['avg_word_length'] = df['char_count'] / df['word_count'].replace(0, 1)

    # Encode target
    if 'discourse_effectiveness' not in train_df.columns:
        raise ValueError("Target column 'discourse_effectiveness' missing in train data")
    le = LabelEncoder()
    y_all = le.fit_transform(train_df['discourse_effectiveness'].astype(str))
    train_df = train_df.drop(columns=['discourse_effectiveness'])

    # TF-IDF vectorization with stop-word removal
    tfidf = TfidfVectorizer(
        ngram_range=(1, 2),
        max_features=10000,
        stop_words='english'
    )
    X_text_train = tfidf.fit_transform(train_df['discourse_text'])
    X_text_test  = tfidf.transform(test_df['discourse_text'])

    # Select top 5000 text features
    skb = SelectKBest(chi2, k=5000)
    X_text_train = skb.fit_transform(X_text_train, y_all)
    X_text_test  = skb.transform(X_text_test)

    # One-hot encode discourse_type
    ohe = OneHotEncoder(sparse_output=True, handle_unknown='ignore')
    X_type_train = ohe.fit_transform(train_df[['discourse_type']])
    X_type_test  = ohe.transform(test_df[['discourse_type']])

    # Numeric features
    num_cols = ['word_count', 'char_count', 'avg_word_length']
    X_num_train = sparse.csr_matrix(train_df[num_cols].values)
    X_num_test  = sparse.csr_matrix(test_df[num_cols].values)

    # Stack features together
    X_full_train = sparse.hstack(
        [X_text_train, X_type_train, X_num_train], format='csr'
    )
    X_full_test = sparse.hstack(
        [X_text_test, X_type_test, X_num_test], format='csr'
    )

    # Stratified group split: hold out one fold
    groups = train_df['essay_id'].values
    sgkf = StratifiedGroupKFold(
        n_splits=5, shuffle=True, random_state=42
    )
    train_idx, val_idx = next(sgkf.split(X_full_train, y_all, groups))
    X_train, X_val = X_full_train[train_idx], X_full_train[val_idx]
    y_train, y_val = y_all[train_idx], y_all[val_idx]

    # L1-based feature selection (LogisticRegression with saga)
    l1_selector = LogisticRegression(
        penalty='l1',
        solver='saga',
        multi_class='multinomial',
        C=1.0,
        random_state=42,
        max_iter=500
    )
    sfm = SelectFromModel(l1_selector)
    sfm.fit(X_train, y_train)
    X_train_sel = sfm.transform(X_train)
    X_val_sel   = sfm.transform(X_val)
    X_test_sel  = sfm.transform(X_full_test)

    return X_train_sel, X_val_sel, y_train, y_val, X_test_sel, test_ids, le

def main():
    try:
        # Paths to data (preserved absolute paths)
        paths = {
            'train_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/train.csv',
            'test_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/test.csv',
            'sample_submission': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_effective_arguments/sample_submission.csv'
        }

        # Preprocess data
        X_train_sel, X_val_sel, y_train, y_val, X_test_sel, test_ids, le = preprocess_data(paths)

        # Train XGBoost with early stopping
        logging.info("Training XGBoostClassifier with early stopping...")
        xgb = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        xgb.fit(
            X_train_sel, y_train,
            eval_set=[(X_val_sel, y_val)],
            early_stopping_rounds=30,
            verbose=False
        )

        # Validation predictions and metrics
        val_preds = xgb.predict(X_val_sel)
        val_probs = xgb.predict_proba(X_val_sel)
        acc = accuracy_score(y_val, val_preds)
        report = classification_report(
            y_val, val_preds, target_names=le.classes_, zero_division=0
        )
        logging.info(f"Validation Accuracy: {acc:.4f}")
        logging.info("Validation Classification Report:\n" + report)

        # Retrain on full data (train + val)
        logging.info("Retraining on full data...")
        X_full = sparse.vstack([X_train_sel, X_val_sel], format='csr')
        y_full = np.concatenate([y_train, y_val])
        xgb_final = XGBClassifier(
            objective='multi:softprob',
            num_class=len(le.classes_),
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        xgb_final.fit(X_full, y_full)

        # Predict probabilities on test set
        logging.info("Predicting probabilities on test set...")
        test_probs = xgb_final.predict_proba(X_test_sel)

        # Prepare submission DataFrame
        sample_sub = pd.read_csv(paths['sample_submission'], encoding='utf-8')
        id_col = sample_sub.columns[0]
        class_cols = list(sample_sub.columns[1:])
        # le.classes_ should match class_cols order
        # Map: le.classes_ -> columns
        # Build a dict from class name to column index
        col_idx = {cls: i for i, cls in enumerate(le.classes_)}
        # Rearrange test_probs columns to match sample_submission class order
        probs_df = pd.DataFrame({
            cls: test_probs[:, col_idx[cls]]
            for cls in class_cols
        })
        submission = pd.DataFrame({id_col: test_ids})
        submission = pd.concat([submission, probs_df], axis=1)

        # Save submission
        out_path = '/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/RREFACTORED/runs/run_20250807_124601_21be393f/submission.csv'
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        submission.to_csv(out_path, index=False)
        logging.info(f"Submission saved to: {out_path}")

    except Exception as e:
        logging.exception("An error occurred during execution")
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124601_21be393f/generation_iter_0/temp_exec_50711109.py ---

--- START FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_70f30017.py ---
import os
import sys
import pandas as pd
import numpy as np

from PIL import Image, UnidentifiedImageError
import torch
import torchvision.transforms as T
import torchvision.models as models

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import PCA
from skmultilearn.model_selection import iterative_train_test_split

from transformers import BertTokenizer, BertModel

def preprocess_data(file_paths):
    """
    Preprocess train and test data for multi-label image+text classification.
    Returns:
        X_train, X_val, y_train, y_val, X_test, test_ids
    """
    # resolve paths
    if isinstance(file_paths, dict):
        train_csv = file_paths["train_csv"]
        test_csv = file_paths["test_csv"]
        img_dir   = file_paths["img_dir"]
    else:
        train_csv, test_csv, img_dir = file_paths

    # load csvs
    train_df = pd.read_csv(train_csv)
    test_df  = pd.read_csv(test_csv)

    # validate and drop missing/corrupt images
    def validate_df(df):
        good_idx = []
        for idx, row in df.iterrows():
            path = os.path.join(img_dir, f"{row['ImageID']}.jpg")
            if not os.path.isfile(path):
                continue
            try:
                Image.open(path).verify()
                good_idx.append(idx)
            except (UnidentifiedImageError, OSError):
                continue
        return df.loc[good_idx].reset_index(drop=True)

    train_df = validate_df(train_df)
    test_df  = validate_df(test_df)

    # parse label lists
    train_df['label_list'] = train_df['Labels'].astype(str).apply(lambda x: [int(i) for i in x.split()])

    # filter out labels with freq < 10
    from collections import Counter
    cnt = Counter([lbl for sub in train_df['label_list'] for lbl in sub])
    valid_labels = sorted([lbl for lbl, c in cnt.items() if c >= 10])
    train_df['label_list'] = train_df['label_list'].apply(lambda ls: [l for l in ls if l in valid_labels])
    train_df = train_df[train_df['label_list'].map(len) > 0].reset_index(drop=True)

    # binarize labels
    mlb = MultiLabelBinarizer(classes=valid_labels)
    y = mlb.fit_transform(train_df['label_list'])

    # image transforms
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std  = [0.229, 0.224, 0.225]
    train_transform = T.Compose([
        T.Resize(256),
        T.RandomResizedCrop(224),
        T.RandomHorizontalFlip(),
        T.RandomRotation(15),
        T.ColorJitter(),
        T.ToTensor(),
        T.Normalize(mean=imagenet_mean, std=imagenet_std)
    ])
    eval_transform = T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=imagenet_mean, std=imagenet_std)
    ])

    # load models
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = torch.nn.Identity()
    resnet = resnet.to(device).eval()
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert = BertModel.from_pretrained('bert-base-uncased').to(device).eval()

    # feature extractors
    def img_feat(img_id, train=True):
        path = os.path.join(img_dir, f"{img_id}.jpg")
        img = Image.open(path).convert('RGB')
        tf = train_transform if train else eval_transform
        t = tf(img).unsqueeze(0).to(device)
        with torch.no_grad():
            f = resnet(t)
        return f.squeeze(0).cpu().numpy()

    def txt_emb(caption):
        cap = caption if isinstance(caption, str) else ""
        toks = tokenizer(cap, padding='max_length', truncation=True,
                         max_length=128, return_tensors='pt')
        toks = {k: v.to(device) for k, v in toks.items()}
        with torch.no_grad():
            out = bert(**toks)
        return out.pooler_output.squeeze(0).cpu().numpy()

    # build train features
    img_feats, txt_feats = [], []
    for _, row in train_df.iterrows():
        img_feats.append(img_feat(row['ImageID'], train=True))
        txt_feats.append(txt_emb(row['Caption']))
    X_img = np.vstack(img_feats)
    X_txt = np.vstack(txt_feats)

    # PCA on text features
    pca = PCA(n_components=128, random_state=42)
    X_txt_red = pca.fit_transform(X_txt)

    # concatenate
    X = np.hstack([X_img, X_txt_red])

    # iterative stratified split
    X_train, y_train, X_val, y_val = iterative_train_test_split(X, y, test_size=0.2)

    # build test features
    test_ids = test_df['ImageID'].tolist()
    img_feats_t, txt_feats_t = [], []
    for _, row in test_df.iterrows():
        img_feats_t.append(img_feat(row['ImageID'], train=False))
        txt_feats_t.append(txt_emb(row['Caption']))
    X_img_t = np.vstack(img_feats_t)
    X_txt_t = np.vstack(txt_feats_t)
    X_txt_t_red = pca.transform(X_txt_t)
    X_test = np.hstack([X_img_t, X_txt_t_red])

    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            "train_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv",
            "test_csv":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv",
            "img_dir":   "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data/"
        }
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_70f30017.py ---

--- START FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_3888b329.py ---
import os
import sys
import pandas as pd
import numpy as np
from collections import Counter

from PIL import Image
import torch
from torch import nn
import torchvision.transforms as T
import torchvision.models as models

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import PCA

from transformers import BertTokenizer, BertModel

from iterstrat.ml_stratifiers import iterative_train_test_split


def preprocess_data(file_paths):
    """
    Preprocesses train and test data for a multi-label image+text classification task.
    Args:
        file_paths: list or dict with keys/positions:
            - train_csv
            - test_csv
            - img_dir
    Returns:
        X_train: np.ndarray, shape (n_train, feat_dim)
        X_val:   np.ndarray, shape (n_val, feat_dim)
        y_train: np.ndarray, shape (n_train, n_classes)
        y_val:   np.ndarray, shape (n_val, n_classes)
        X_test:  np.ndarray, shape (n_test, feat_dim)
        test_ids: list of ImageID for test samples
    """
    # resolve paths
    if isinstance(file_paths, dict):
        train_csv = file_paths["train_csv"]
        test_csv = file_paths["test_csv"]
        img_dir   = file_paths["img_dir"]
    else:
        train_csv, test_csv, img_dir = file_paths

    # load
    train_df = pd.read_csv(train_csv)
    test_df  = pd.read_csv(test_csv)

    # validate image files
    def check_and_drop(df):
        ok = []
        for idx, row in df.iterrows():
            img_path = os.path.join(img_dir, f"{row['ImageID']}.jpg")
            if os.path.exists(img_path):
                ok.append(True)
            else:
                ok.append(False)
        df = df.loc[ok].reset_index(drop=True)
        return df

    train_df = check_and_drop(train_df)
    test_df  = check_and_drop(test_df)

    # parse labels
    train_df['label_list'] = train_df['Labels'].astype(str).apply(lambda x: [int(i) for i in x.split()])

    # filter labels with freq < 10
    cnt = Counter([lbl for sub in train_df['label_list'] for lbl in sub])
    valid_labels = sorted([lbl for lbl, c in cnt.items() if c >= 10])
    train_df['label_list'] = train_df['label_list'].apply(lambda ls: [l for l in ls if l in valid_labels])
    train_df = train_df[train_df['label_list'].map(len) > 0].reset_index(drop=True)

    # binarize
    mlb = MultiLabelBinarizer(classes=valid_labels)
    y = mlb.fit_transform(train_df['label_list'])

    # image transforms (for feature extraction we use deterministic transforms)
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std  = [0.229, 0.224, 0.225]
    base_transform = T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=imagenet_mean, std=imagenet_std)
    ])

    # load resnet50 for feature extraction
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = nn.Identity()
    resnet = resnet.to(device).eval()

    # load BERT
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert = BertModel.from_pretrained('bert-base-uncased').to(device).eval()

    # helper: extract one image feature
    def extract_img_feat(image_id):
        img = Image.open(os.path.join(img_dir, f"{image_id}.jpg")).convert('RGB')
        img_t = base_transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = resnet(img_t)
        return feat.squeeze(0).cpu().numpy()

    # helper: extract one text embedding
    def extract_txt_emb(caption):
        tokens = tokenizer(caption if isinstance(caption, str) else "",
                           padding='max_length',
                           truncation=True,
                           max_length=128,
                           return_tensors='pt')
        tokens = {k: v.to(device) for k, v in tokens.items()}
        with torch.no_grad():
            out = bert(**tokens)
        return out.pooler_output.squeeze(0).cpu().numpy()

    # extract train features
    img_feats = []
    txt_embs = []
    for idx, row in train_df.iterrows():
        img_feats.append(extract_img_feat(row['ImageID']))
        txt_embs.append(extract_txt_emb(row['Caption']))
    X_img = np.vstack(img_feats)
    X_txt = np.vstack(txt_embs)

    # reduce text dim if needed
    pca = PCA(n_components=128)
    X_txt_red = pca.fit_transform(X_txt)

    # concat
    X = np.hstack([X_img, X_txt_red])

    # split train/val with iterative stratification
    X_train, X_val, y_train, y_val = iterative_train_test_split(X, y, test_size=0.2)

    # preprocess test set
    test_ids = test_df['ImageID'].tolist()
    img_feats_t = []
    txt_embs_t = []
    for idx, row in test_df.iterrows():
        img_feats_t.append(extract_img_feat(row['ImageID']))
        txt_embs_t.append(extract_txt_emb(row['Caption']))
    X_img_t = np.vstack(img_feats_t)
    X_txt_t = np.vstack(txt_embs_t)
    X_txt_t_red = pca.transform(X_txt_t)
    X_test = np.hstack([X_img_t, X_txt_t_red])

    return X_train, X_val, y_train, y_val, X_test, test_ids


if __name__ == "__main__":
    try:
        file_paths = {
            "train_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv",
            "test_csv":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv",
            "img_dir":   "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data/"
        }
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_3888b329.py ---

--- START FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_f12d3515.py ---
import os
import sys
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
import torch
import torchvision.transforms as T
import torchvision.models as models
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import PCA
from iterstrat.ml_stratifiers import iterative_train_test_split
from transformers import BertTokenizer, BertModel

def preprocess_data(file_paths: dict):
    if isinstance(file_paths, dict):
        train_csv = file_paths.get("train_csv")
        test_csv  = file_paths.get("test_csv")
        img_dir   = file_paths.get("img_dir")
    else:
        train_csv, test_csv, img_dir = file_paths

    train_df = pd.read_csv(train_csv)
    test_df  = pd.read_csv(test_csv)

    def validate(df):
        good = []
        for idx, img_id in enumerate(df['ImageID']):
            path = os.path.join(img_dir, f"{img_id}.jpg")
            if not os.path.isfile(path):
                continue
            try:
                Image.open(path).verify()
                good.append(idx)
            except (UnidentifiedImageError, OSError):
                continue
        return df.iloc[good].reset_index(drop=True)

    train_df = validate(train_df)
    test_df  = validate(test_df)

    # parse and filter labels
    train_df['label_list'] = train_df['Labels'].astype(str).apply(lambda x: [int(i) for i in x.split()])
    from collections import Counter
    counts = Counter([lbl for sub in train_df['label_list'] for lbl in sub])
    valid_labels = sorted([lbl for lbl, c in counts.items() if c >= 10])
    train_df['label_list'] = train_df['label_list'].apply(lambda ls: [l for l in ls if l in valid_labels])
    train_df = train_df[train_df['label_list'].map(len) > 0].reset_index(drop=True)

    mlb = MultiLabelBinarizer(classes=valid_labels)
    y_all = mlb.fit_transform(train_df['label_list'])

    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std  = [0.229, 0.224, 0.225]
    train_tf = T.Compose([
        T.Resize(256), T.RandomResizedCrop(224),
        T.RandomHorizontalFlip(),
        T.RandomRotation(15),
        T.ColorJitter(),
        T.ToTensor(),
        T.Normalize(mean=imagenet_mean, std=imagenet_std)
    ])
    eval_tf = T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=imagenet_mean, std=imagenet_std)
    ])

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = torch.nn.Identity()
    resnet = resnet.to(device).eval()

    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert = BertModel.from_pretrained('bert-base-uncased').to(device).eval()

    def extract_img(id_, train=True):
        p = os.path.join(img_dir, f"{id_}.jpg")
        img = Image.open(p).convert('RGB')
        tfm = train_tf if train else eval_tf
        t = tfm(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = resnet(t)
        return feat.squeeze(0).cpu().numpy()

    def extract_txt(caption):
        cap = caption if isinstance(caption, str) else ""
        toks = tokenizer(cap, padding='max_length', truncation=True,
                         max_length=128, return_tensors='pt')
        toks = {k: v.to(device) for k, v in toks.items()}
        with torch.no_grad():
            out = bert(**toks)
        return out.pooler_output.squeeze(0).cpu().numpy()

    img_feats = []
    txt_feats = []
    for _, row in train_df.iterrows():
        img_feats.append(extract_img(row['ImageID'], train=True))
        txt_feats.append(extract_txt(row['Caption']))
    X_img = np.vstack(img_feats)
    X_txt = np.vstack(txt_feats)

    pca = PCA(n_components=128, random_state=42)
    X_txt_red = pca.fit_transform(X_txt)
    X_all = np.hstack([X_img, X_txt_red])

    X_train, X_val, y_train, y_val = iterative_train_test_split(
        X_all, y_all, test_size=0.2
    )

    test_ids = test_df['ImageID'].tolist()
    img_t = [extract_img(i, train=False) for i in test_ids]
    txt_t = [extract_txt(c) for c in test_df['Caption']]
    X_img_t = np.vstack(img_t)
    X_txt_t = np.vstack(txt_t)
    X_txt_t_red = pca.transform(X_txt_t)
    X_test = np.hstack([X_img_t, X_txt_t_red])

    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            "train_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv",
            "test_csv":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv",
            "img_dir":   "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data/"
        }
        preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_f12d3515.py ---

--- START FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_f0ae4c57.py ---
import os
import sys
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
import torch
import torchvision.transforms as T
import torchvision.models as models
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import PCA
from skmultilearn.model_selection import iterative_train_test_split
from transformers import BertTokenizer, BertModel

def preprocess_data(file_paths):
    # resolve paths
    if isinstance(file_paths, dict):
        train_csv = file_paths["train_csv"]
        test_csv = file_paths["test_csv"]
        img_dir = file_paths["img_dir"]
    else:
        train_csv, test_csv, img_dir = file_paths

    # load data
    train_df = pd.read_csv(train_csv)
    test_df  = pd.read_csv(test_csv)

    # validate image files
    def validate(df):
        idxs = []
        for i, img_id in df['ImageID'].items():
            path = os.path.join(img_dir, f"{img_id}.jpg")
            if not os.path.isfile(path):
                continue
            try:
                Image.open(path).verify()
                idxs.append(i)
            except (UnidentifiedImageError, OSError):
                continue
        return df.loc[idxs].reset_index(drop=True)

    train_df = validate(train_df)
    test_df  = validate(test_df)

    # parse labels and filter rare ones
    train_df['label_list'] = train_df['Labels'].astype(str).apply(lambda x: [int(i) for i in x.split()])
    from collections import Counter
    cnt = Counter([lab for row in train_df['label_list'] for lab in row])
    valid_labels = sorted([lab for lab, c in cnt.items() if c >= 10])
    train_df['label_list'] = train_df['label_list'].apply(lambda ls: [l for l in ls if l in valid_labels])
    train_df = train_df[train_df['label_list'].map(len) > 0].reset_index(drop=True)

    # binarize labels
    mlb = MultiLabelBinarizer(classes=valid_labels)
    y = mlb.fit_transform(train_df['label_list'])

    # image transforms
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std  = [0.229, 0.224, 0.225]
    train_tf = T.Compose([
        T.Resize(256), T.RandomResizedCrop(224),
        T.RandomHorizontalFlip(), T.RandomRotation(15),
        T.ColorJitter(), T.ToTensor(),
        T.Normalize(imagenet_mean, imagenet_std)
    ])
    eval_tf = T.Compose([
        T.Resize(256), T.CenterCrop(224),
        T.ToTensor(), T.Normalize(imagenet_mean, imagenet_std)
    ])

    # load models
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = torch.nn.Identity()
    resnet = resnet.to(device).eval()
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert = BertModel.from_pretrained('bert-base-uncased').to(device).eval()

    def img_feat(img_id, train=True):
        p = os.path.join(img_dir, f"{img_id}.jpg")
        img = Image.open(p).convert('RGB')
        t = (train_tf if train else eval_tf)(img).unsqueeze(0).to(device)
        with torch.no_grad():
            return resnet(t).squeeze(0).cpu().numpy()

    def txt_feat(caption):
        toks = tokenizer(str(caption), padding='max_length', truncation=True,
                         max_length=128, return_tensors='pt')
        toks = {k: v.to(device) for k, v in toks.items()}
        with torch.no_grad():
            return bert(**toks).pooler_output.squeeze(0).cpu().numpy()

    # extract train features
    img_feats, txt_feats = [], []
    for _, row in train_df.iterrows():
        img_feats.append(img_feat(row['ImageID'], train=True))
        txt_feats.append(txt_feat(row['Caption']))
    X_img = np.vstack(img_feats)
    X_txt = np.vstack(txt_feats)

    # reduce text dim
    pca = PCA(n_components=128, random_state=42)
    X_txt_red = pca.fit_transform(X_txt)
    X = np.hstack([X_img, X_txt_red])

    # iterative stratified split
    X_train, y_train, X_val, y_val = iterative_train_test_split(X, y, test_size=0.2)

    # prepare test set
    test_ids = test_df['ImageID'].tolist()
    img_t, txt_t = [], []
    for img_id, cap in zip(test_df['ImageID'], test_df['Caption']):
        img_t.append(img_feat(img_id, train=False))
        txt_t.append(txt_feat(cap))
    X_test = np.hstack([np.vstack(img_t), pca.transform(np.vstack(txt_t))])

    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        paths = {
            "train_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv",
            "test_csv":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv",
            "img_dir":   "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data/"
        }
        preprocess_data(paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_f0ae4c57.py ---

--- START FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_68186c18.py ---
import os
import sys
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
import torch
import torchvision.transforms as T
import torchvision.models as models
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import PCA
from transformers import BertTokenizer, BertModel
from iterstrat.ml_stratifiers import iterative_train_test_split

def preprocess_data(file_paths: dict):
    # Resolve paths
    if isinstance(file_paths, dict):
        train_csv = file_paths.get("train_csv")
        test_csv = file_paths.get("test_csv")
        img_dir   = file_paths.get("img_dir")
    else:
        train_csv, test_csv, img_dir = file_paths

    # Load data
    train_df = pd.read_csv(train_csv)
    test_df  = pd.read_csv(test_csv)

    # Validate images
    def validate_images(df):
        valid_idx = []
        for idx, img_id in df["ImageID"].items():
            img_path = os.path.join(img_dir, f"{img_id}.jpg")
            if not os.path.isfile(img_path):
                continue
            try:
                Image.open(img_path).verify()
                valid_idx.append(idx)
            except (UnidentifiedImageError, OSError):
                continue
        return df.loc[valid_idx].reset_index(drop=True)

    train_df = validate_images(train_df)
    test_df  = validate_images(test_df)

    # Parse and filter labels
    train_df["label_list"] = train_df["Labels"].astype(str).apply(
        lambda s: [int(x) for x in s.replace(",", " ").split() if x.isdigit()]
    )
    from collections import Counter
    counter = Counter(l for sub in train_df["label_list"] for l in sub)
    valid_labels = sorted([l for l, cnt in counter.items() if cnt >= 10])
    train_df["label_list"] = train_df["label_list"].apply(lambda lst: [l for l in lst if l in valid_labels])
    train_df = train_df[train_df["label_list"].map(len) > 0].reset_index(drop=True)

    # Binarize labels
    mlb = MultiLabelBinarizer(classes=valid_labels)
    y = mlb.fit_transform(train_df["label_list"])

    # Image transforms
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std  = [0.229, 0.224, 0.225]
    train_tf = T.Compose([
        T.Resize(256),
        T.RandomResizedCrop(224),
        T.RandomHorizontalFlip(),
        T.RandomRotation(15),
        T.ColorJitter(),
        T.ToTensor(),
        T.Normalize(imagenet_mean, imagenet_std)
    ])
    eval_tf = T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(imagenet_mean, imagenet_std)
    ])

    # Load models
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = torch.nn.Identity()
    resnet = resnet.to(device).eval()
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    bert = BertModel.from_pretrained("bert-base-uncased").to(device).eval()

    # Feature extraction
    def get_image_feat(img_id, augment=True):
        img_path = os.path.join(img_dir, f"{img_id}.jpg")
        img = Image.open(img_path).convert("RGB")
        transform = train_tf if augment else eval_tf
        tensor = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = resnet(tensor).squeeze(0).cpu().numpy()
        return feat

    def get_text_feat(text):
        toks = tokenizer(str(text), padding="max_length",
                         truncation=True, max_length=128,
                         return_tensors="pt").to(device)
        with torch.no_grad():
            out = bert(**toks)
            return out.pooler_output.squeeze(0).cpu().numpy()

    # Extract train features
    img_feats = []
    txt_feats = []
    for _, row in train_df.iterrows():
        img_feats.append(get_image_feat(row["ImageID"], augment=True))
        txt_feats.append(get_text_feat(row["Caption"]))
    X_img = np.vstack(img_feats)
    X_txt = np.vstack(txt_feats)

    # Reduce text dims
    pca = PCA(n_components=128, random_state=42)
    X_txt_red = pca.fit_transform(X_txt)

    # Combine features
    X = np.hstack([X_img, X_txt_red])

    # Split train/val with iterative stratification
    X_train, X_val, y_train, y_val = iterative_train_test_split(
        X, y, test_size=0.2
    )

    # Prepare test set
    test_ids = test_df["ImageID"].tolist()
    img_feats_t = []
    txt_feats_t = []
    for _, row in test_df.iterrows():
        img_feats_t.append(get_image_feat(row["ImageID"], augment=False))
        txt_feats_t.append(get_text_feat(row["Caption"]))
    X_img_t = np.vstack(img_feats_t)
    X_txt_t = np.vstack(txt_feats_t)
    X_txt_t_red = pca.transform(X_txt_t)
    X_test = np.hstack([X_img_t, X_txt_t_red])

    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        paths = {
            "train_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv",
            "test_csv":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv",
            "img_dir":   "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data/"
        }
        preprocess_data(paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/temp_exec_68186c18.py ---

--- START FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/states/preprocessing_code_response.py ---
import os
import sys
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
import torch
import torchvision.transforms as T
import torchvision.models as models
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import PCA
from transformers import BertTokenizer, BertModel
from iterstrat.ml_stratifiers import iterative_train_test_split

def preprocess_data(file_paths: dict):
    # Resolve paths
    if isinstance(file_paths, dict):
        train_csv = file_paths.get("train_csv")
        test_csv = file_paths.get("test_csv")
        img_dir   = file_paths.get("img_dir")
    else:
        train_csv, test_csv, img_dir = file_paths

    # Load data
    train_df = pd.read_csv(train_csv)
    test_df  = pd.read_csv(test_csv)

    # Validate images
    def validate_images(df):
        valid_idx = []
        for idx, img_id in df["ImageID"].items():
            img_path = os.path.join(img_dir, f"{img_id}.jpg")
            if not os.path.isfile(img_path):
                continue
            try:
                Image.open(img_path).verify()
                valid_idx.append(idx)
            except (UnidentifiedImageError, OSError):
                continue
        return df.loc[valid_idx].reset_index(drop=True)

    train_df = validate_images(train_df)
    test_df  = validate_images(test_df)

    # Parse and filter labels
    train_df["label_list"] = train_df["Labels"].astype(str).apply(
        lambda s: [int(x) for x in s.replace(",", " ").split() if x.isdigit()]
    )
    from collections import Counter
    counter = Counter(l for sub in train_df["label_list"] for l in sub)
    valid_labels = sorted([l for l, cnt in counter.items() if cnt >= 10])
    train_df["label_list"] = train_df["label_list"].apply(lambda lst: [l for l in lst if l in valid_labels])
    train_df = train_df[train_df["label_list"].map(len) > 0].reset_index(drop=True)

    # Binarize labels
    mlb = MultiLabelBinarizer(classes=valid_labels)
    y = mlb.fit_transform(train_df["label_list"])

    # Image transforms
    imagenet_mean = [0.485, 0.456, 0.406]
    imagenet_std  = [0.229, 0.224, 0.225]
    train_tf = T.Compose([
        T.Resize(256),
        T.RandomResizedCrop(224),
        T.RandomHorizontalFlip(),
        T.RandomRotation(15),
        T.ColorJitter(),
        T.ToTensor(),
        T.Normalize(imagenet_mean, imagenet_std)
    ])
    eval_tf = T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(imagenet_mean, imagenet_std)
    ])

    # Load models
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = torch.nn.Identity()
    resnet = resnet.to(device).eval()
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    bert = BertModel.from_pretrained("bert-base-uncased").to(device).eval()

    # Feature extraction
    def get_image_feat(img_id, augment=True):
        img_path = os.path.join(img_dir, f"{img_id}.jpg")
        img = Image.open(img_path).convert("RGB")
        transform = train_tf if augment else eval_tf
        tensor = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = resnet(tensor).squeeze(0).cpu().numpy()
        return feat

    def get_text_feat(text):
        toks = tokenizer(str(text), padding="max_length",
                         truncation=True, max_length=128,
                         return_tensors="pt").to(device)
        with torch.no_grad():
            out = bert(**toks)
            return out.pooler_output.squeeze(0).cpu().numpy()

    # Extract train features
    img_feats = []
    txt_feats = []
    for _, row in train_df.iterrows():
        img_feats.append(get_image_feat(row["ImageID"], augment=True))
        txt_feats.append(get_text_feat(row["Caption"]))
    X_img = np.vstack(img_feats)
    X_txt = np.vstack(txt_feats)

    # Reduce text dims
    pca = PCA(n_components=128, random_state=42)
    X_txt_red = pca.fit_transform(X_txt)

    # Combine features
    X = np.hstack([X_img, X_txt_red])

    # Split train/val with iterative stratification
    X_train, X_val, y_train, y_val = iterative_train_test_split(
        X, y, test_size=0.2
    )

    # Prepare test set
    test_ids = test_df["ImageID"].tolist()
    img_feats_t = []
    txt_feats_t = []
    for _, row in test_df.iterrows():
        img_feats_t.append(get_image_feat(row["ImageID"], augment=False))
        txt_feats_t.append(get_text_feat(row["Caption"]))
    X_img_t = np.vstack(img_feats_t)
    X_txt_t = np.vstack(txt_feats_t)
    X_txt_t_red = pca.transform(X_txt_t)
    X_test = np.hstack([X_img_t, X_txt_t_red])

    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        paths = {
            "train_csv": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv",
            "test_csv":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv",
            "img_dir":   "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data/"
        }
        preprocess_data(paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124818_c955b65d/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/states/final_preprocessing_code.py ---
# import necessary libraries
import sys
import os
import re
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from scipy.sparse import hstack

def simple_lemmatize(token: str) -> str:
    if token.endswith('ies') and len(token) > 3:
        return token[:-3] + 'y'
    if token.endswith('ing') and len(token) > 4:
        return token[:-3]
    if token.endswith('ed') and len(token) > 3:
        return token[:-2]
    if token.endswith('s') and len(token) > 2:
        return token[:-1]
    return token

def preprocess_data(file_paths: dict):
    # validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")
    # load data
    train_df = pd.read_csv(file_paths["train"])
    test_df = pd.read_csv(file_paths["test"])
    # check columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")
    # impute missing
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"] = test_df["Title"].fillna("")
    # preserve test IDs
    test_ids = test_df["ID"].values
    # drop ID
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])
    # strip whitespace
    train_df["Title"] = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"] = test_df["Title"].str.strip()
    # remove duplicates
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)
    # text cleaning
    stop_words = ENGLISH_STOP_WORDS
    def clean_text(txt: str) -> str:
        txt = txt.lower()
        txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
        tokens = txt.split()
        tokens = [t for t in tokens if t and t not in stop_words]
        tokens = [simple_lemmatize(t) for t in tokens]
        return " ".join(tokens)
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)
    # title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda x: len(x.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda x: len(x.split()))
    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])
    # feature selection
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)
    # combine with title_length
    X_len_train = train_df["title_length"].values.reshape(-1, 1)
    X_len_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_len_train])
    X_full_test  = hstack([X_sel_test, X_len_test])
    # stratified split
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )
    return X_train, X_val, y_train, y_val, X_full_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }
        results = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/states/final_preprocessing_code.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/states/final_executable_code.py ---
#!/usr/bin/env python3

import sys
import os
import re
import pandas as pd
import numpy as np
from scipy.sparse import hstack
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

def simple_lemmatize(token: str) -> str:
    if token.endswith('ies') and len(token) > 3:
        return token[:-3] + 'y'
    if token.endswith('ing') and len(token) > 4:
        return token[:-3]
    if token.endswith('ed') and len(token) > 3:
        return token[:-2]
    if token.endswith('s') and len(token) > 2:
        return token[:-1]
    return token

def preprocess_data(file_paths: dict):
    # validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # load data
    train_df = pd.read_csv(file_paths["train"])
    test_df  = pd.read_csv(file_paths["test"])

    # check required columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")

    # fill missing titles
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"]  = test_df["Title"].fillna("")

    # preserve test IDs
    test_ids = test_df["ID"].values

    # drop ID columns
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])

    # strip whitespace
    train_df["Title"]  = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"]   = test_df["Title"].str.strip()

    # remove duplicate titles in train
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    # text cleaning function
    stop_words = ENGLISH_STOP_WORDS
    def clean_text(txt: str) -> str:
        txt = txt.lower()
        txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
        tokens = txt.split()
        tokens = [t for t in tokens if t and t not in stop_words]
        tokens = [simple_lemmatize(t) for t in tokens]
        return " ".join(tokens)

    # apply cleaning
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)

    # title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda x: len(x.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda x: len(x.split()))

    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])

    # feature selection
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)

    # combine with title_length
    X_len_train = train_df["title_length"].values.reshape(-1, 1)
    X_len_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_len_train])
    X_full_test  = hstack([X_sel_test,  X_len_test])

    # stratified split for validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )

    return X_train, X_val, y_train, y_val, X_full_test, test_ids

def train_and_get_model(X_train, y_train):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
    param_grid = {'C': [0.01, 0.1, 1, 10]}
    grid = GridSearchCV(
        estimator=lr,
        param_grid=param_grid,
        cv=skf,
        scoring='f1_macro',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    return grid.best_estimator_

if __name__ == "__main__":
    try:
        # define file paths
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }

        # preprocessing
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing completed successfully.")

        # training and validation
        best_model = train_and_get_model(X_train, y_train)
        val_preds = best_model.predict(X_val)
        val_score = f1_score(y_val, val_preds, average='macro')
        print(f"Validation F1 Macro Score: {val_score:.4f}")

        # prediction on test set
        test_preds = best_model.predict(X_test)
        submission = pd.DataFrame({
            "ID": test_ids,
            "Domain": test_preds
        })

        # save submission
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
        )
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to: {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/states/final_executable_code.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_375858ec.py ---
# import necessary libraries
import sys
import os
import re
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from scipy.sparse import hstack

def simple_lemmatize(token: str) -> str:
    if token.endswith('ies') and len(token) > 3:
        return token[:-3] + 'y'
    if token.endswith('ing') and len(token) > 4:
        return token[:-3]
    if token.endswith('ed') and len(token) > 3:
        return token[:-2]
    if token.endswith('s') and len(token) > 2:
        return token[:-1]
    return token

def preprocess_data(file_paths: dict):
    # validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")
    # load data
    train_df = pd.read_csv(file_paths["train"])
    test_df = pd.read_csv(file_paths["test"])
    # check columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")
    # impute missing
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"] = test_df["Title"].fillna("")
    # preserve test IDs
    test_ids = test_df["ID"].values
    # drop ID
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])
    # strip whitespace
    train_df["Title"] = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"] = test_df["Title"].str.strip()
    # remove duplicates
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)
    # text cleaning
    stop_words = ENGLISH_STOP_WORDS
    def clean_text(txt: str) -> str:
        txt = txt.lower()
        txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
        tokens = txt.split()
        tokens = [t for t in tokens if t and t not in stop_words]
        tokens = [simple_lemmatize(t) for t in tokens]
        return " ".join(tokens)
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)
    # title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda x: len(x.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda x: len(x.split()))
    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])
    # feature selection
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)
    # combine with title_length
    X_len_train = train_df["title_length"].values.reshape(-1, 1)
    X_len_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_len_train])
    X_full_test  = hstack([X_sel_test, X_len_test])
    # stratified split
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )
    return X_train, X_val, y_train, y_val, X_full_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }
        results = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_375858ec.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_321ffb06.py ---
#!/usr/bin/env python3

import sys
import os
import re
import pandas as pd
import numpy as np
from scipy.sparse import hstack
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

def simple_lemmatize(token: str) -> str:
    if token.endswith('ies') and len(token) > 3:
        return token[:-3] + 'y'
    if token.endswith('ing') and len(token) > 4:
        return token[:-3]
    if token.endswith('ed') and len(token) > 3:
        return token[:-2]
    if token.endswith('s') and len(token) > 2:
        return token[:-1]
    return token

def preprocess_data(file_paths: dict):
    # validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # load data
    train_df = pd.read_csv(file_paths["train"])
    test_df  = pd.read_csv(file_paths["test"])

    # check required columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")

    # fill missing titles
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"]  = test_df["Title"].fillna("")

    # preserve test IDs
    test_ids = test_df["ID"].values

    # drop ID columns
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])

    # strip whitespace
    train_df["Title"]  = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"]   = test_df["Title"].str.strip()

    # remove duplicate titles in train
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    # text cleaning function
    stop_words = ENGLISH_STOP_WORDS
    def clean_text(txt: str) -> str:
        txt = txt.lower()
        txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
        tokens = txt.split()
        tokens = [t for t in tokens if t and t not in stop_words]
        tokens = [simple_lemmatize(t) for t in tokens]
        return " ".join(tokens)

    # apply cleaning
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)

    # title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda x: len(x.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda x: len(x.split()))

    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])

    # feature selection
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)

    # combine with title_length
    X_len_train = train_df["title_length"].values.reshape(-1, 1)
    X_len_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_len_train])
    X_full_test  = hstack([X_sel_test,  X_len_test])

    # stratified split for validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )

    return X_train, X_val, y_train, y_val, X_full_test, test_ids

def train_and_get_model(X_train, y_train):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
    param_grid = {'C': [0.01, 0.1, 1, 10]}
    grid = GridSearchCV(
        estimator=lr,
        param_grid=param_grid,
        cv=skf,
        scoring='f1_macro',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    return grid.best_estimator_

if __name__ == "__main__":
    try:
        # define file paths
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }

        # preprocessing
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing completed successfully.")

        # training and validation
        best_model = train_and_get_model(X_train, y_train)
        val_preds = best_model.predict(X_val)
        val_score = f1_score(y_val, val_preds, average='macro')
        print(f"Validation F1 Macro Score: {val_score:.4f}")

        # prediction on test set
        test_preds = best_model.predict(X_test)
        submission = pd.DataFrame({
            "ID": test_ids,
            "Domain": test_preds
        })

        # save submission
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
        )
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to: {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_321ffb06.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_ccc4519f.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved pipeline for multiclass text classification of queries into domains.
Incorporates:
 - sklearn Pipeline with TF-IDF, model-based feature selection, and classifier
 - NLTK WordNet lemmatization for text normalization
 - Configurable hyperparameters (hardcoded dict)
 - Class-weight balancing for imbalanced classes
 - GridSearchCV with refit on full data
 - Logging and model persistence via joblib
"""

import os
import re
import logging
from pathlib import Path

import pandas as pd
import numpy as np
from nltk import download
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split
from sklearn.metrics import f1_score, classification_report, confusion_matrix
import joblib

# Ensure NLTK resources are available
download('wordnet', quiet=True)
download('omw-1.4', quiet=True)
download('stopwords', quiet=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Hardcoded file paths (preserved as per instructions)
TRAIN_PATH = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv"
TEST_PATH  = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
OUTPUT_SUBMISSION_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
)
MODEL_DUMP_PATH = os.path.join(
    Path(OUTPUT_SUBMISSION_PATH).parent,
    "final_model.joblib"
)

# Custom text cleaning and tokenization
class LemmaTokenizer:
    def __init__(self, extra_stopwords=None):
        self.wnl = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        if extra_stopwords:
            self.stop_words |= set(extra_stopwords)

    def __call__(self, text):
        # Lowercase
        text = text.lower()
        # Remove non-alphanumeric characters
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        # Tokenize on whitespace
        tokens = text.split()
        # Remove stopwords and lemmatize
        return [
            self.wnl.lemmatize(tok)
            for tok in tokens
            if tok not in self.stop_words and len(tok) > 1
        ]

def load_and_clean_data(train_path, test_path):
    # Load datasets
    train_df = pd.read_csv(train_path)
    test_df  = pd.read_csv(test_path)

    # Basic schema checks
    required_train_cols = {'ID', 'Title', 'Domain'}
    required_test_cols  = {'ID', 'Title'}
    if not required_train_cols.issubset(train_df.columns):
        raise ValueError(f"train.csv must contain columns: {required_train_cols}")
    if not required_test_cols.issubset(test_df.columns):
        raise ValueError(f"test.csv must contain columns: {required_test_cols}")

    # Impute missing titles
    train_df['Title'] = train_df['Title'].fillna("").astype(str)
    test_df['Title']  = test_df['Title'].fillna("").astype(str)

    # Strip whitespace
    train_df['Title']  = train_df['Title'].str.strip()
    train_df['Domain'] = train_df['Domain'].astype(str).str.strip()
    test_df['Title']   = test_df['Title'].str.strip()

    # Drop duplicates on Title
    train_df = train_df.drop_duplicates(subset=['Title']).reset_index(drop=True)

    return train_df, test_df

def build_pipeline(config):
    """
    Build a sklearn Pipeline with TF-IDF vectorizer, model-based feature selection,
    and a logistic regression classifier.
    """
    tokenizer = LemmaTokenizer(extra_stopwords=config['extra_stopwords'])

    tfidf = TfidfVectorizer(
        tokenizer=tokenizer,
        ngram_range=config['tfidf__ngram_range'],
        min_df=config['tfidf__min_df'],
        max_df=config['tfidf__max_df'],
        max_features=config['tfidf__max_features']
    )

    # model-based selector: L1-penalized logistic regression
    selector = SelectFromModel(
        LogisticRegression(
            penalty='l1',
            solver='saga',
            class_weight='balanced',
            C=config['selector__C'],
            max_iter=1000,
            random_state=42
        ),
        threshold='mean'
    )

    clf = LogisticRegression(
        penalty='l2',
        solver='saga',
        class_weight='balanced',
        multi_class='multinomial',
        max_iter=1000,
        random_state=42
    )

    pipeline = Pipeline([
        ('tfidf', tfidf),
        ('selector', selector),
        ('clf', clf),
    ])

    return pipeline

def main():
    # Configuration (hyperparameters)
    config = {
        'tfidf__ngram_range': (1, 2),
        'tfidf__min_df': 2,
        'tfidf__max_df': 0.8,
        'tfidf__max_features': 5000,
        'selector__C': 1.0,
        'clf__C': 1.0,
        'extra_stopwords': ['what', 'how', 'using', 'use', 'get', 'using']
    }

    logger.info("Loading and cleaning data...")
    train_df, test_df = load_and_clean_data(TRAIN_PATH, TEST_PATH)
    logger.info(f"Training samples: {train_df.shape[0]}, Test samples: {test_df.shape[0]}")

    X = train_df['Title'].values
    y = train_df['Domain'].values
    X_test = test_df['Title'].values
    test_ids = test_df['ID'].values

    # Train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    logger.info(f"Train/Val split: {X_train.shape[0]}/{X_val.shape[0]}")

    # Build pipeline
    pipeline = build_pipeline(config)

    # Hyperparameter grid
    param_grid = {
        'selector__estimator__C': [0.1, 1.0, 10.0],
        'clf__C': [0.1, 1.0, 10.0]
    }

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    grid = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        cv=cv,
        scoring='f1_macro',
        n_jobs=-1,
        refit=True,
        verbose=1
    )

    logger.info("Starting GridSearchCV...")
    grid.fit(X_train, y_train)
    logger.info(f"Best params: {grid.best_params_}")
    logger.info(f"Best CV f1_macro: {grid.best_score_:.4f}")

    # Evaluate on validation set
    val_preds = grid.predict(X_val)
    val_f1 = f1_score(y_val, val_preds, average='macro')
    logger.info(f"Validation F1 Macro: {val_f1:.4f}")
    logger.info("Validation classification report:\n" +
                classification_report(y_val, val_preds))
    logger.info("Validation confusion matrix:\n" +
                str(confusion_matrix(y_val, val_preds)))

    # Retrain on full training data (GridSearchCV with refit already did this)
    logger.info("Retraining on full training data with best estimator...")
    best_model = grid.best_estimator_

    # Persist the model
    logger.info(f"Saving model to {MODEL_DUMP_PATH} ...")
    joblib.dump(best_model, MODEL_DUMP_PATH)

    # Predict on test set
    logger.info("Predicting on test data...")
    test_preds = best_model.predict(X_test)

    submission = pd.DataFrame({
        'ID': test_ids,
        'Domain': test_preds
    })
    os.makedirs(os.path.dirname(OUTPUT_SUBMISSION_PATH), exist_ok=True)
    submission.to_csv(OUTPUT_SUBMISSION_PATH, index=False)
    logger.info(f"Submission saved to: {OUTPUT_SUBMISSION_PATH}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.exception("An error occurred during execution.")
        raise
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_ccc4519f.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_1b821a36.py ---
#!/usr/bin/env python3
import os
import re
import logging
import joblib
import numpy as np
import pandas as pd
import nltk

from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import f1_score
from sklearn.pipeline import Pipeline
from lightgbm import LGBMClassifier

# Ensure NLTK resources are available
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# Configuration
CONFIG = {
    "paths": {
        "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
        "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv",
        "submission": (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
        )
    },
    "random_state": 42,
    "tfidf": {
        "ngram_range": (1, 2),
        "min_df": 2,
        "max_df": 0.8,
        "max_features": 5000
    },
    "svd_n_components": 300,
    "cv": {
        "n_splits": 5,
        "shuffle": True
    },
    "search": {
        "n_iter": 10,
        "cv": 5,
        "scoring": "f1_macro"
    },
    "lgbm_params": {
        "random_state": 42,
        "n_jobs": -1,
        "class_weight": "balanced"
    },
    "param_distributions": {
        "clf__n_estimators": [100, 200, 300],
        "clf__num_leaves": [31, 50, 100],
        "clf__learning_rate": [0.01, 0.05, 0.1],
        "clf__max_depth": [-1, 10, 20]
    }
}

class TextPreprocessor:
    """
    Simple text cleaner: lowercase, remove punctuation, stop-words, lemmatize.
    """
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        extra_sw = {"what", "how", "why", "using", "use", "get", "please", "would"}
        self.stop_words = ENGLISH_STOP_WORDS.union(extra_sw)

    def __call__(self, texts):
        cleaned = []
        for doc in texts:
            txt = str(doc).lower()
            # keep alphanumerics and spaces
            txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
            tokens = txt.split()
            tokens = [t for t in tokens if t not in self.stop_words]
            lemmas = [self.lemmatizer.lemmatize(t) for t in tokens]
            cleaned.append(" ".join(lemmas))
        return cleaned

def load_data(train_path, test_path):
    if not os.path.isfile(train_path):
        raise FileNotFoundError(f"Training file not found: {train_path}")
    if not os.path.isfile(test_path):
        raise FileNotFoundError(f"Test file not found: {test_path}")

    train = pd.read_csv(train_path)
    test  = pd.read_csv(test_path)

    # Schema checks
    if not {"ID", "Title", "Domain"}.issubset(train.columns):
        raise ValueError("train.csv must contain ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test.columns):
        raise ValueError("test.csv must contain ID, Title")

    # Fill and strip
    train["Title"] = train["Title"].fillna("").astype(str).str.strip()
    train["Domain"] = train["Domain"].astype(str).str.strip()
    test["Title"]  = test["Title"].fillna("").astype(str).str.strip()

    # Drop duplicate titles
    train = train.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    test_ids = test["ID"].values
    return train, test, test_ids

def main():
    logging.info("Loading data...")
    train_df, test_df, test_ids = load_data(
        CONFIG["paths"]["train"],
        CONFIG["paths"]["test"]
    )

    X = train_df["Title"].values
    y = train_df["Domain"].values
    X_test_raw = test_df["Title"].values

    # Hold-out split
    logging.info("Splitting train/validation sets...")
    X_train_raw, X_val_raw, y_train, y_val = train_test_split(
        X, y,
        test_size=0.2,
        stratify=y,
        random_state=CONFIG["random_state"]
    )
    logging.info(f"Train size: {len(X_train_raw)}, Val size: {len(X_val_raw)}")

    # Build pipeline
    logging.info("Constructing pipeline...")
    pipe = Pipeline([
        ("pre", TextPreprocessor()),
        ("tfidf", TfidfVectorizer(**CONFIG["tfidf"])),
        ("svd", TruncatedSVD(
            n_components=CONFIG["svd_n_components"],
            random_state=CONFIG["random_state"]
        )),
        ("clf", LGBMClassifier(**CONFIG["lgbm_params"]))
    ])

    # Hyperparameter search
    logging.info("Starting hyperparameter search (RandomizedSearchCV)...")
    cv = StratifiedKFold(
        n_splits=CONFIG["cv"]["n_splits"],
        shuffle=CONFIG["cv"]["shuffle"],
        random_state=CONFIG["random_state"]
    )
    search = RandomizedSearchCV(
        estimator=pipe,
        param_distributions=CONFIG["param_distributions"],
        n_iter=CONFIG["search"]["n_iter"],
        scoring=CONFIG["search"]["scoring"],
        cv=cv,
        refit=True,
        random_state=CONFIG["random_state"],
        n_jobs=-1,
        verbose=1
    )
    search.fit(X_train_raw, y_train)
    logging.info(f"Best parameters: {search.best_params_}")

    # Validation
    logging.info("Evaluating on validation set...")
    y_val_pred = search.predict(X_val_raw)
    val_f1 = f1_score(y_val, y_val_pred, average="macro")
    logging.info(f"Validation F1 Macro: {val_f1:.4f}")

    # Refit on full data
    logging.info("Refitting best model on full training data...")
    best_pipeline = search.best_estimator_
    best_pipeline.fit(X, y)

    # Predict test
    logging.info("Predicting test set...")
    test_preds = best_pipeline.predict(X_test_raw)

    # Save submission
    submission = pd.DataFrame({
        "ID": test_ids,
        "Domain": test_preds
    })
    os.makedirs(os.path.dirname(CONFIG["paths"]["submission"]), exist_ok=True)
    submission.to_csv(CONFIG["paths"]["submission"], index=False)
    logging.info(f"Submission saved to {CONFIG['paths']['submission']}")

    # Persist model
    model_path = os.path.splitext(CONFIG["paths"]["submission"])[0] + "_pipeline.joblib"
    joblib.dump(best_pipeline, model_path)
    logging.info(f"Trained pipeline saved to {model_path}")

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_1b821a36.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_a40a7ae2.py ---
#!/usr/bin/env python3
import os
import re
import logging
import pandas as pd
import numpy as np
import nltk
from nltk.stem import WordNetLemmatizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import f1_score
import joblib

# Download required NLTK data quietly
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# Configuration dictionary
CONFIG = {
    "file_paths": {
        "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
        "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
    },
    "output_path": (
        "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
        "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
    ),
    "random_state": 42,
    "tfidf": {
        "ngram_range": (1, 2),
        "min_df": 2,
        "max_df": 0.8,
        "max_features": 5000
    },
    "grid_params": {
        "clf__C": [0.01, 0.1, 1, 10]
    },
    "selector": {
        "penalty": "l1",
        "solver": "saga",
        "class_weight": "balanced",
        "max_iter": 1000
    },
    "cv_splits": 5
}

class TextCleaner(BaseEstimator, TransformerMixin):
    """
    Transformer that lowercases, strips punctuation, removes stopwords,
    and lemmatizes input text.
    """
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        # extend sklearn stop words with common question tokens
        extra = {"what", "how", "why", "using", "use", "get", "please", "help"}
        self.stop_words = ENGLISH_STOP_WORDS.union(extra)

    def fit(self, X, y=None):
        return self

    def _clean(self, text: str) -> str:
        text = text.lower()
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        tokens = text.split()
        tokens = [t for t in tokens if t not in self.stop_words]
        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]
        return " ".join(tokens)

    def transform(self, X, y=None):
        return np.array([self._clean(str(x)) for x in X])

def load_and_prepare_data(paths):
    # Validate files exist
    for key in ("train", "test"):
        p = paths.get(key)
        if not p or not os.path.isfile(p):
            raise FileNotFoundError(f"Missing or invalid path for '{key}': {p}")

    # Load
    train = pd.read_csv(paths["train"])
    test  = pd.read_csv(paths["test"])

    # Basic schema checks
    if not {"ID", "Title", "Domain"}.issubset(train.columns):
        raise ValueError("train.csv must have ID, Title, Domain columns")
    if not {"ID", "Title"}.issubset(test.columns):
        raise ValueError("test.csv must have ID, Title columns")

    # Fill missing titles
    train["Title"] = train["Title"].fillna("").astype(str)
    test["Title"]  = test["Title"].fillna("").astype(str)

    # Strip whitespace
    train["Title"]  = train["Title"].str.strip()
    train["Domain"] = train["Domain"].astype(str).str.strip()
    test["Title"]   = test["Title"].str.strip()

    # Drop duplicate titles
    train = train.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    # Preserve test IDs
    test_ids = test["ID"].values

    return train, test, test_ids

def main():
    try:
        # Load data
        train_df, test_df, test_ids = load_and_prepare_data(CONFIG["file_paths"])
        X = train_df["Title"].values
        y = train_df["Domain"].values
        X_test_raw = test_df["Title"].values

        # Stratified split
        X_train_raw, X_val_raw, y_train, y_val = train_test_split(
            X, y,
            test_size=0.2,
            stratify=y,
            random_state=CONFIG["random_state"]
        )
        logging.info(f"Train set: {len(X_train_raw)} rows, Validation set: {len(X_val_raw)} rows")

        # Build pipeline
        from sklearn.pipeline import Pipeline
        pipeline = Pipeline([
            ("cleaner", TextCleaner()),
            ("vect", TfidfVectorizer(**CONFIG["tfidf"])),
            ("sel", SelectFromModel(
                LogisticRegression(**CONFIG["selector"]),
                max_features=1000
            )),
            ("clf", LogisticRegression(
                class_weight="balanced",
                solver="liblinear",
                max_iter=1000,
                random_state=CONFIG["random_state"]
            ))
        ])

        # Grid search over classifier C
        cv = StratifiedKFold(n_splits=CONFIG["cv_splits"], shuffle=True, random_state=CONFIG["random_state"])
        gs = GridSearchCV(
            estimator=pipeline,
            param_grid=CONFIG["grid_params"],
            cv=cv,
            scoring="f1_macro",
            n_jobs=-1,
            refit=True,
            verbose=1
        )
        logging.info("Starting grid search...")
        gs.fit(X_train_raw, y_train)
        logging.info(f"Best params: {gs.best_params_}")
        best_pipe = gs.best_estimator_

        # Validate on hold-out
        y_val_pred = best_pipe.predict(X_val_raw)
        val_f1 = f1_score(y_val, y_val_pred, average="macro")
        logging.info(f"Validation F1 Macro: {val_f1:.4f}")

        # Refit on full training data
        logging.info("Refitting best model on full training set...")
        best_pipe.fit(X, y)

        # Predict test set
        logging.info("Predicting test set...")
        test_preds = best_pipe.predict(X_test_raw)

        # Prepare submission
        submission = pd.DataFrame({
            "ID": test_ids,
            "Domain": test_preds
        })
        os.makedirs(os.path.dirname(CONFIG["output_path"]), exist_ok=True)
        submission.to_csv(CONFIG["output_path"], index=False)
        logging.info(f"Submission saved to {CONFIG['output_path']}")

        # Persist model and vectorizer
        model_path = os.path.splitext(CONFIG["output_path"])[0] + "_model.joblib"
        joblib.dump(best_pipe, model_path)
        logging.info(f"Trained pipeline saved to {model_path}")

    except Exception as e:
        logging.exception("An error occurred during execution")
        raise

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_a40a7ae2.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_75da7fd3.py ---
import sys
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from scipy.sparse import hstack
import spacy

def preprocess_data(file_paths: dict):
    # Validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load data
    train_df = pd.read_csv(file_paths["train"])
    test_df  = pd.read_csv(file_paths["test"])

    # Validate columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")

    # Impute missing titles
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"]  = test_df["Title"].fillna("")

    # Preserve IDs
    test_ids = test_df["ID"].values

    # Drop ID columns
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])

    # Strip whitespace
    train_df["Title"]  = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"]   = test_df["Title"].str.strip()

    # Remove duplicates by Title
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    # Load spaCy model for tokenization & lemmatization
    try:
        nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
    except OSError:
        raise RuntimeError("spaCy model 'en_core_web_sm' not found. Please install it.")

    stop_words = ENGLISH_STOP_WORDS

    def clean_text(text: str) -> str:
        doc = nlp(text.lower())
        tokens = [
            token.lemma_ for token in doc
            if token.is_alpha and token.lemma_.strip() and token.text.lower() not in stop_words
        ]
        return " ".join(tokens)

    # Apply text cleaning
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)

    # Title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda s: len(s.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda s: len(s.split()))

    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])

    # Feature selection via chi2
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)

    # Combine TF-IDF and title_length
    X_add_train = train_df["title_length"].values.reshape(-1, 1)
    X_add_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_add_train])
    X_full_test  = hstack([X_sel_test, X_add_test])

    # Stratified train/val split
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )

    return X_train, X_val, y_train, y_val, X_full_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv",
            "submission": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/Submission_file01.csv"
        }
        preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_75da7fd3.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_c8690537.py ---
#!/usr/bin/env python3
"""
A complete, standalone Python script for multiclass classification of
search/query titles into domains. Uses a sklearn Pipeline combining
TF-IDF + TruncatedSVD dimensionality reduction and LightGBM with
RandomizedSearchCV. Re-fits best model on all available training data
before generating final test predictions.
"""

import os
import re
import logging
import pandas as pd
import numpy as np
import joblib

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import f1_score

from lightgbm import LGBMClassifier

# -----------------------------------------------------------------------------
# CONFIGURATION
# -----------------------------------------------------------------------------
CONFIG = {
    "file_paths": {
        "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
        "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
    },
    "output_path": (
        "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
        "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
    ),
    "random_state": 42,
    "tfidf": {
        "ngram_range": (1, 2),
        "min_df": 2,
        "max_df": 0.8,
        "max_features": 5000
    },
    "svd_components": [100, 200, 300],
    "lgbm_params_dist": {
        "clf__num_leaves": [31, 50, 100],
        "clf__learning_rate": [0.01, 0.05, 0.1],
        "clf__n_estimators": [100, 200, 500],
        "features__text_svd__svd__n_components": [100, 200, 300]
    },
    "n_iter_search": 20,
    "cv_splits": 5,
    "test_size": 0.2
}

# -----------------------------------------------------------------------------
# LOGGING SETUP
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# -----------------------------------------------------------------------------
# TRANSFORMERS
# -----------------------------------------------------------------------------
class TextCleaner(BaseEstimator, TransformerMixin):
    """
    Transformer to lowercase, strip non-alphanumeric chars, remove
    stopwords and normalize whitespace.
    """
    def __init__(self):
        extra = {"what", "how", "why", "using", "use", "get", "please", "help"}
        self.stop_words = ENGLISH_STOP_WORDS.union(extra)
        self._pattern = re.compile(r'[^a-z0-9\s]')

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        cleaned = []
        for txt in X:
            txt = str(txt).lower()
            txt = self._pattern.sub(" ", txt)
            tokens = [t for t in txt.split() if t not in self.stop_words]
            cleaned.append(" ".join(tokens))
        return np.array(cleaned)

class LengthExtractor(BaseEstimator, TransformerMixin):
    """
    Transformer to extract the title length (in tokens) as a numeric feature.
    """
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        lengths = np.array([[len(str(txt).split())] for txt in X], dtype=np.float32)
        return lengths

# -----------------------------------------------------------------------------
# MAIN WORKFLOW
# -----------------------------------------------------------------------------
def load_and_clean(paths: dict):
    # validate files
    for key in ("train", "test"):
        p = paths.get(key)
        if not p or not os.path.isfile(p):
            raise FileNotFoundError(f"Path for '{key}' is invalid: {p}")

    # load
    train = pd.read_csv(paths["train"])
    test  = pd.read_csv(paths["test"])

    # schema check
    if not {"ID", "Title", "Domain"}.issubset(train.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test.columns):
        raise ValueError("test.csv must contain columns: ID, Title")

    # fill missing titles
    train["Title"] = train["Title"].fillna("").astype(str).str.strip()
    test["Title"]  = test["Title"].fillna("").astype(str).str.strip()
    train["Domain"] = train["Domain"].astype(str).str.strip()

    # drop duplicates
    train = train.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    test_ids = test["ID"].values
    X_train_text = train["Title"].values
    y_train = train["Domain"].values
    X_test_text = test["Title"].values

    return X_train_text, y_train, X_test_text, test_ids

def build_pipeline(config):
    """
    Constructs a sklearn Pipeline:
      - Text cleaning
      - FeatureUnion of:
         * TF-IDF -> TruncatedSVD
         * Title length
      - LightGBM classifier
    """
    tfidf_cfg = config["tfidf"]
    pipe = Pipeline([
        ("clean", TextCleaner()),
        ("features", FeatureUnion([
            ("text_svd", Pipeline([
                ("vect", TfidfVectorizer(
                    ngram_range=tfidf_cfg["ngram_range"],
                    min_df=tfidf_cfg["min_df"],
                    max_df=tfidf_cfg["max_df"],
                    max_features=tfidf_cfg["max_features"]
                )),
                ("svd", TruncatedSVD(n_components=200, random_state=config["random_state"]))
            ])),
            ("length", LengthExtractor())
        ])),
        ("clf", LGBMClassifier(
            class_weight="balanced",
            random_state=config["random_state"]
        ))
    ])
    return pipe

def main():
    try:
        cfg = CONFIG
        logger.info("Loading and cleaning data...")
        X_text, y, X_test_text, test_ids = load_and_clean(cfg["file_paths"])

        # split for validation
        X_tr, X_val, y_tr, y_val = train_test_split(
            X_text, y,
            test_size=cfg["test_size"],
            stratify=y,
            random_state=cfg["random_state"]
        )
        logger.info(f"Train size: {len(X_tr)}, Validation size: {len(X_val)}")

        pipeline = build_pipeline(cfg)

        # hyperparameter search
        logger.info("Starting RandomizedSearchCV for LightGBM pipeline...")
        cv = StratifiedKFold(
            n_splits=cfg["cv_splits"],
            shuffle=True,
            random_state=cfg["random_state"]
        )
        rnd_search = RandomizedSearchCV(
            estimator=pipeline,
            param_distributions=cfg["lgbm_params_dist"],
            n_iter=cfg["n_iter_search"],
            scoring="f1_macro",
            cv=cv,
            random_state=cfg["random_state"],
            n_jobs=-1,
            verbose=1,
            refit=True
        )
        rnd_search.fit(X_tr, y_tr)
        logger.info(f"Best parameters: {rnd_search.best_params_}")

        # validation
        best_pipe = rnd_search.best_estimator_
        val_preds = best_pipe.predict(X_val)
        val_f1 = f1_score(y_val, val_preds, average="macro")
        logger.info(f"Validation F1 Macro: {val_f1:.4f}")

        # re-fit on full training data
        logger.info("Refitting best pipeline on all training data...")
        best_pipe.fit(X_text, y)

        # predict on test
        logger.info("Predicting test set...")
        test_preds = best_pipe.predict(X_test_text)

        # save submission
        sub_df = pd.DataFrame({
            "ID": test_ids,
            "Domain": test_preds
        })
        out_dir = os.path.dirname(cfg["output_path"])
        os.makedirs(out_dir, exist_ok=True)
        sub_df.to_csv(cfg["output_path"], index=False)
        logger.info(f"Submission saved to: {cfg['output_path']}")

        # persist model
        model_path = os.path.splitext(cfg["output_path"])[0] + "_lgbm_pipeline.joblib"
        joblib.dump(best_pipe, model_path)
        logger.info(f"Trained pipeline saved to: {model_path}")

    except Exception as e:
        logger.exception("An error occurred during processing")
        raise

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/temp_exec_c8690537.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/states/final_assembled_code.py ---
#!/usr/bin/env python3

import sys
import os
import re
import pandas as pd
import numpy as np
from scipy.sparse import hstack
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

def simple_lemmatize(token: str) -> str:
    if token.endswith('ies') and len(token) > 3:
        return token[:-3] + 'y'
    if token.endswith('ing') and len(token) > 4:
        return token[:-3]
    if token.endswith('ed') and len(token) > 3:
        return token[:-2]
    if token.endswith('s') and len(token) > 2:
        return token[:-1]
    return token

def preprocess_data(file_paths: dict):
    # validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # load data
    train_df = pd.read_csv(file_paths["train"])
    test_df  = pd.read_csv(file_paths["test"])

    # check required columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")

    # fill missing titles
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"]  = test_df["Title"].fillna("")

    # preserve test IDs
    test_ids = test_df["ID"].values

    # drop ID columns
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])

    # strip whitespace
    train_df["Title"]  = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"]   = test_df["Title"].str.strip()

    # remove duplicate titles in train
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)

    # text cleaning function
    stop_words = ENGLISH_STOP_WORDS
    def clean_text(txt: str) -> str:
        txt = txt.lower()
        txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
        tokens = txt.split()
        tokens = [t for t in tokens if t and t not in stop_words]
        tokens = [simple_lemmatize(t) for t in tokens]
        return " ".join(tokens)

    # apply cleaning
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)

    # title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda x: len(x.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda x: len(x.split()))

    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])

    # feature selection
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)

    # combine with title_length
    X_len_train = train_df["title_length"].values.reshape(-1, 1)
    X_len_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_len_train])
    X_full_test  = hstack([X_sel_test,  X_len_test])

    # stratified split for validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )

    return X_train, X_val, y_train, y_val, X_full_test, test_ids

def train_and_get_model(X_train, y_train):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
    param_grid = {'C': [0.01, 0.1, 1, 10]}
    grid = GridSearchCV(
        estimator=lr,
        param_grid=param_grid,
        cv=skf,
        scoring='f1_macro',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    return grid.best_estimator_

if __name__ == "__main__":
    try:
        # define file paths
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }

        # preprocessing
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing completed successfully.")

        # training and validation
        best_model = train_and_get_model(X_train, y_train)
        val_preds = best_model.predict(X_val)
        val_score = f1_score(y_val, val_preds, average='macro')
        print(f"Validation F1 Macro Score: {val_score:.4f}")

        # prediction on test set
        test_preds = best_model.predict(X_test)
        submission = pd.DataFrame({
            "ID": test_ids,
            "Domain": test_preds
        })

        # save submission
        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_150523_c390a90d/submission.csv"
        )
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to: {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/states/final_assembled_code.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/states/preprocessing_code_response.py ---
# import necessary libraries
import sys
import os
import re
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from scipy.sparse import hstack

def simple_lemmatize(token: str) -> str:
    if token.endswith('ies') and len(token) > 3:
        return token[:-3] + 'y'
    if token.endswith('ing') and len(token) > 4:
        return token[:-3]
    if token.endswith('ed') and len(token) > 3:
        return token[:-2]
    if token.endswith('s') and len(token) > 2:
        return token[:-1]
    return token

def preprocess_data(file_paths: dict):
    # validate input
    if not isinstance(file_paths, dict):
        raise ValueError("file_paths must be a dict with keys 'train' and 'test'.")
    for key in ("train", "test"):
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' path in file_paths.")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")
    # load data
    train_df = pd.read_csv(file_paths["train"])
    test_df = pd.read_csv(file_paths["test"])
    # check columns
    if not {"ID", "Title", "Domain"}.issubset(train_df.columns):
        raise ValueError("train.csv must contain columns: ID, Title, Domain")
    if not {"ID", "Title"}.issubset(test_df.columns):
        raise ValueError("test.csv must contain columns: ID, Title")
    # impute missing
    train_df["Title"] = train_df["Title"].fillna("")
    test_df["Title"] = test_df["Title"].fillna("")
    # preserve test IDs
    test_ids = test_df["ID"].values
    # drop ID
    train_df = train_df.drop(columns=["ID"])
    test_df  = test_df.drop(columns=["ID"])
    # strip whitespace
    train_df["Title"] = train_df["Title"].str.strip()
    train_df["Domain"] = train_df["Domain"].astype(str).str.strip()
    test_df["Title"] = test_df["Title"].str.strip()
    # remove duplicates
    train_df = train_df.drop_duplicates(subset=["Title"]).reset_index(drop=True)
    # text cleaning
    stop_words = ENGLISH_STOP_WORDS
    def clean_text(txt: str) -> str:
        txt = txt.lower()
        txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
        tokens = txt.split()
        tokens = [t for t in tokens if t and t not in stop_words]
        tokens = [simple_lemmatize(t) for t in tokens]
        return " ".join(tokens)
    train_df["clean_text"] = train_df["Title"].apply(clean_text)
    test_df["clean_text"]  = test_df["Title"].apply(clean_text)
    # title length feature
    train_df["title_length"] = train_df["clean_text"].apply(lambda x: len(x.split()))
    test_df["title_length"]  = test_df["clean_text"].apply(lambda x: len(x.split()))
    # TF-IDF vectorization
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
    X_tfidf_train = vectorizer.fit_transform(train_df["clean_text"])
    X_tfidf_test  = vectorizer.transform(test_df["clean_text"])
    # feature selection
    y = train_df["Domain"]
    selector = SelectKBest(chi2, k=3000)
    X_sel_train = selector.fit_transform(X_tfidf_train, y)
    X_sel_test  = selector.transform(X_tfidf_test)
    # combine with title_length
    X_len_train = train_df["title_length"].values.reshape(-1, 1)
    X_len_test  = test_df["title_length"].values.reshape(-1, 1)
    X_full_train = hstack([X_sel_train, X_len_train])
    X_full_test  = hstack([X_sel_test, X_len_test])
    # stratified split
    X_train, X_val, y_train, y_val = train_test_split(
        X_full_train, y, test_size=0.2, stratify=y, random_state=42
    )
    return X_train, X_val, y_train, y_val, X_full_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }
        results = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/states/modeling_code_response.py ---
import sys
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, StratifiedKFold

def train_and_predict(X_train, y_train, X_test):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
    param_grid = {'C': [0.01, 0.1, 1, 10]}
    grid = GridSearchCV(
        lr,
        param_grid=param_grid,
        cv=skf,
        scoring='f1_macro',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    return best_model.predict(X_test)

if __name__ == "__main__":
    try:
        file_paths = {
            "train": "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv",
            "test":  "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv"
        }
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        preds = train_and_predict(X_train, y_train, X_test)
        submission = pd.DataFrame({
            "ID": test_ids,
            "Domain": preds
        })
        submission.to_csv("submission.csv", index=False)
        print("Modeling script executed successfully and submission.csv created!")
    except Exception as e:
        print(f"An error occurred during modeling: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_150523_c390a90d/generation_iter_0/states/modeling_code_response.py ---

--- START FILE: runs/run_20250807_131833_5dcdce5c/states/final_preprocessing_code.py ---
# import necessary libraries
import sys
import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack

def preprocess_data(file_paths):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict or list containing paths for train.csv, test.csv, Submission_file01.csv
    Returns:
        tuple: (X_train, X_val, y_train, y_val, X_test, test_ids)
    """
    # Resolve input
    if isinstance(file_paths, dict):
        train_path = file_paths.get('train')
        test_path = file_paths.get('test')
    elif isinstance(file_paths, (list, tuple)) and len(file_paths) >= 2:
        train_path, test_path = file_paths[0], file_paths[1]
    else:
        raise ValueError("file_paths must be a dict with keys 'train' and 'test', or a list of length>=2")
    # Check existence
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load data
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)

    # Basic validation
    for col in ['ID', 'Title', 'Domain']:
        if col not in df_train.columns:
            raise ValueError(f"Training data missing required column: {col}")
    for col in ['ID', 'Title']:
        if col not in df_test.columns:
            raise ValueError(f"Test data missing required column: {col}")

    # Preserve test IDs
    test_ids = df_test['ID'].copy()

    # Drop ID columns
    df_train = df_train.drop(columns=['ID'])
    df_test = df_test.drop(columns=['ID'])

    # Drop missing titles (~0.3%)
    df_train = df_train.dropna(subset=['Title'])
    df_test  = df_test.dropna(subset=['Title'])

    # Remove duplicates based on Title
    df_train = df_train.drop_duplicates(subset=['Title'])
    df_test  = df_test.drop_duplicates(subset=['Title'])

    # Lowercase and strip whitespace
    df_train['Title'] = df_train['Title'].str.lower().str.strip()
    df_test['Title']  = df_test['Title'].str.lower().str.strip()

    # Feature engineering: title_word_count
    df_train['title_word_count'] = df_train['Title'].str.split().apply(len)
    df_test['title_word_count']  = df_test['Title'].str.split().apply(len)

    # TF-IDF vectorization
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_train_tfidf = tfidf.fit_transform(df_train['Title'])
    X_test_tfidf  = tfidf.transform(df_test['Title'])

    # Label-encode target
    le = LabelEncoder()
    y = le.fit_transform(df_train['Domain'])

    # Feature selection with chi-squared
    selector = SelectKBest(chi2, k=3000)
    X_train_sel = selector.fit_transform(X_train_tfidf, y)
    X_test_sel  = selector.transform(X_test_tfidf)

    # Combine TF-IDF features with title_word_count
    X_train_full = hstack([X_train_sel, np.array(df_train['title_word_count']).reshape(-1, 1)])
    X_test_full  = hstack([X_test_sel,  np.array(df_test['title_word_count']).reshape(-1, 1)])

    # Stratified train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y, test_size=0.2, random_state=42, stratify=y
    )

    return X_train, X_val, y_train, y_val, X_test_full, test_ids

# Test the function
if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv',
            'test':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv',
            'submission': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/Submission_file01.csv'
        }
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_131833_5dcdce5c/states/final_preprocessing_code.py ---

--- START FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/temp_exec_ce0cced5.py ---
#!/usr/bin/env python3

import sys
import os
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from scipy.sparse import hstack


def preprocess_data(file_paths):
    """
    Preprocess data: load CSVs, clean text, vectorize, select features, and split.
    Returns:
        X_train, X_val, y_train, y_val, X_test, test_ids
    """
    # Resolve input paths
    if isinstance(file_paths, dict):
        train_path = file_paths.get('train')
        test_path = file_paths.get('test')
    elif isinstance(file_paths, (list, tuple)) and len(file_paths) >= 2:
        train_path, test_path = file_paths[0], file_paths[1]
    else:
        raise ValueError("file_paths must be a dict with keys 'train' and 'test', or a list of length>=2")

    # Check files exist
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)

    # Required columns
    for col in ['ID', 'Title', 'Domain']:
        if col not in df_train.columns:
            raise ValueError(f"Training data missing required column: {col}")
    for col in ['ID', 'Title']:
        if col not in df_test.columns:
            raise ValueError(f"Test data missing required column: {col}")

    # Preserve test IDs
    test_ids = df_test['ID'].copy()

    # Drop IDs
    df_train = df_train.drop(columns=['ID'])
    df_test = df_test.drop(columns=['ID'])

    # Drop missing titles
    df_train = df_train.dropna(subset=['Title'])
    df_test = df_test.dropna(subset=['Title'])

    # Drop duplicate titles
    df_train = df_train.drop_duplicates(subset=['Title'])
    df_test = df_test.drop_duplicates(subset=['Title'])

    # Normalize text
    df_train['Title'] = df_train['Title'].str.lower().str.strip()
    df_test['Title'] = df_test['Title'].str.lower().str.strip()

    # Feature: word count
    df_train['title_word_count'] = df_train['Title'].str.split().apply(len)
    df_test['title_word_count'] = df_test['Title'].str.split().apply(len)

    # TF-IDF
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_train_tfidf = tfidf.fit_transform(df_train['Title'])
    X_test_tfidf = tfidf.transform(df_test['Title'])

    # Encode labels
    le = LabelEncoder()
    y = le.fit_transform(df_train['Domain'])

    # Feature selection
    selector = SelectKBest(chi2, k=3000)
    X_train_sel = selector.fit_transform(X_train_tfidf, y)
    X_test_sel = selector.transform(X_test_tfidf)

    # Combine with word count
    X_train_full = hstack([
        X_train_sel,
        np.array(df_train['title_word_count']).reshape(-1, 1)
    ])
    X_test_full = hstack([
        X_test_sel,
        np.array(df_test['title_word_count']).reshape(-1, 1)
    ])

    # Train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y, test_size=0.2,
        random_state=42, stratify=y
    )

    return X_train, X_val, y_train, y_val, X_test_full, test_ids


def train_and_predict(X_train, y_train, X_val, y_val, X_test):
    """
    Trains a logistic regression model, reports CV and validation score,
    and returns test-set predictions (numeric).
    """
    model = LogisticRegression(max_iter=1000, random_state=42)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(
        model, X_train, y_train, cv=cv, scoring='f1_macro'
    )
    print(f"CV f1_macro scores: {cv_scores}")
    print(f"Mean CV f1_macro: {cv_scores.mean():.4f}")

    # Fit on full training data
    model.fit(X_train, y_train)

    # Validation
    val_preds = model.predict(X_val)
    val_score = f1_score(y_val, val_preds, average='macro')
    print(f"Validation f1_macro: {val_score:.4f}")

    # Predict on test set
    test_preds = model.predict(X_test)
    return test_preds


if __name__ == "__main__":
    try:
        # Define input file paths
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv',
            'test': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv'
        }

        # Preprocess
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)

        # Train and get numeric predictions
        preds_numeric = train_and_predict(
            X_train, y_train, X_val, y_val, X_test
        )

        # Map back to original labels
        df_train = pd.read_csv(file_paths['train'])
        le = LabelEncoder().fit(df_train['Domain'])
        preds_labels = le.inverse_transform(preds_numeric)

        # Prepare submission DataFrame
        submission_df = pd.DataFrame({
            'ID': test_ids,
            'Domain': preds_labels
        })

        # Save submission
        submission_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_131833_5dcdce5c/submission.csv"
        )
        submission_df.to_csv(submission_path, index=False)
        print(f"Submission saved to {submission_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/temp_exec_ce0cced5.py ---

--- START FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/temp_exec_b5e8c540.py ---
# import necessary libraries
import sys
import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack

def preprocess_data(file_paths):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict or list containing paths for train.csv, test.csv, Submission_file01.csv
    Returns:
        tuple: (X_train, X_val, y_train, y_val, X_test, test_ids)
    """
    # Resolve input
    if isinstance(file_paths, dict):
        train_path = file_paths.get('train')
        test_path = file_paths.get('test')
    elif isinstance(file_paths, (list, tuple)) and len(file_paths) >= 2:
        train_path, test_path = file_paths[0], file_paths[1]
    else:
        raise ValueError("file_paths must be a dict with keys 'train' and 'test', or a list of length>=2")
    # Check existence
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load data
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)

    # Basic validation
    for col in ['ID', 'Title', 'Domain']:
        if col not in df_train.columns:
            raise ValueError(f"Training data missing required column: {col}")
    for col in ['ID', 'Title']:
        if col not in df_test.columns:
            raise ValueError(f"Test data missing required column: {col}")

    # Preserve test IDs
    test_ids = df_test['ID'].copy()

    # Drop ID columns
    df_train = df_train.drop(columns=['ID'])
    df_test = df_test.drop(columns=['ID'])

    # Drop missing titles (~0.3%)
    df_train = df_train.dropna(subset=['Title'])
    df_test  = df_test.dropna(subset=['Title'])

    # Remove duplicates based on Title
    df_train = df_train.drop_duplicates(subset=['Title'])
    df_test  = df_test.drop_duplicates(subset=['Title'])

    # Lowercase and strip whitespace
    df_train['Title'] = df_train['Title'].str.lower().str.strip()
    df_test['Title']  = df_test['Title'].str.lower().str.strip()

    # Feature engineering: title_word_count
    df_train['title_word_count'] = df_train['Title'].str.split().apply(len)
    df_test['title_word_count']  = df_test['Title'].str.split().apply(len)

    # TF-IDF vectorization
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_train_tfidf = tfidf.fit_transform(df_train['Title'])
    X_test_tfidf  = tfidf.transform(df_test['Title'])

    # Label-encode target
    le = LabelEncoder()
    y = le.fit_transform(df_train['Domain'])

    # Feature selection with chi-squared
    selector = SelectKBest(chi2, k=3000)
    X_train_sel = selector.fit_transform(X_train_tfidf, y)
    X_test_sel  = selector.transform(X_test_tfidf)

    # Combine TF-IDF features with title_word_count
    X_train_full = hstack([X_train_sel, np.array(df_train['title_word_count']).reshape(-1, 1)])
    X_test_full  = hstack([X_test_sel,  np.array(df_test['title_word_count']).reshape(-1, 1)])

    # Stratified train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y, test_size=0.2, random_state=42, stratify=y
    )

    return X_train, X_val, y_train, y_val, X_test_full, test_ids

# Test the function
if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv',
            'test':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv',
            'submission': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/Submission_file01.csv'
        }
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/temp_exec_b5e8c540.py ---

--- START FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/states/final_assembled_code.py ---
#!/usr/bin/env python3

import sys
import os
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from scipy.sparse import hstack


def preprocess_data(file_paths):
    """
    Preprocess data: load CSVs, clean text, vectorize, select features, and split.
    Returns:
        X_train, X_val, y_train, y_val, X_test, test_ids
    """
    # Resolve input paths
    if isinstance(file_paths, dict):
        train_path = file_paths.get('train')
        test_path = file_paths.get('test')
    elif isinstance(file_paths, (list, tuple)) and len(file_paths) >= 2:
        train_path, test_path = file_paths[0], file_paths[1]
    else:
        raise ValueError("file_paths must be a dict with keys 'train' and 'test', or a list of length>=2")

    # Check files exist
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)

    # Required columns
    for col in ['ID', 'Title', 'Domain']:
        if col not in df_train.columns:
            raise ValueError(f"Training data missing required column: {col}")
    for col in ['ID', 'Title']:
        if col not in df_test.columns:
            raise ValueError(f"Test data missing required column: {col}")

    # Preserve test IDs
    test_ids = df_test['ID'].copy()

    # Drop IDs
    df_train = df_train.drop(columns=['ID'])
    df_test = df_test.drop(columns=['ID'])

    # Drop missing titles
    df_train = df_train.dropna(subset=['Title'])
    df_test = df_test.dropna(subset=['Title'])

    # Drop duplicate titles
    df_train = df_train.drop_duplicates(subset=['Title'])
    df_test = df_test.drop_duplicates(subset=['Title'])

    # Normalize text
    df_train['Title'] = df_train['Title'].str.lower().str.strip()
    df_test['Title'] = df_test['Title'].str.lower().str.strip()

    # Feature: word count
    df_train['title_word_count'] = df_train['Title'].str.split().apply(len)
    df_test['title_word_count'] = df_test['Title'].str.split().apply(len)

    # TF-IDF
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_train_tfidf = tfidf.fit_transform(df_train['Title'])
    X_test_tfidf = tfidf.transform(df_test['Title'])

    # Encode labels
    le = LabelEncoder()
    y = le.fit_transform(df_train['Domain'])

    # Feature selection
    selector = SelectKBest(chi2, k=3000)
    X_train_sel = selector.fit_transform(X_train_tfidf, y)
    X_test_sel = selector.transform(X_test_tfidf)

    # Combine with word count
    X_train_full = hstack([
        X_train_sel,
        np.array(df_train['title_word_count']).reshape(-1, 1)
    ])
    X_test_full = hstack([
        X_test_sel,
        np.array(df_test['title_word_count']).reshape(-1, 1)
    ])

    # Train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y, test_size=0.2,
        random_state=42, stratify=y
    )

    return X_train, X_val, y_train, y_val, X_test_full, test_ids


def train_and_predict(X_train, y_train, X_val, y_val, X_test):
    """
    Trains a logistic regression model, reports CV and validation score,
    and returns test-set predictions (numeric).
    """
    model = LogisticRegression(max_iter=1000, random_state=42)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(
        model, X_train, y_train, cv=cv, scoring='f1_macro'
    )
    print(f"CV f1_macro scores: {cv_scores}")
    print(f"Mean CV f1_macro: {cv_scores.mean():.4f}")

    # Fit on full training data
    model.fit(X_train, y_train)

    # Validation
    val_preds = model.predict(X_val)
    val_score = f1_score(y_val, val_preds, average='macro')
    print(f"Validation f1_macro: {val_score:.4f}")

    # Predict on test set
    test_preds = model.predict(X_test)
    return test_preds


if __name__ == "__main__":
    try:
        # Define input file paths
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv',
            'test': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv'
        }

        # Preprocess
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)

        # Train and get numeric predictions
        preds_numeric = train_and_predict(
            X_train, y_train, X_val, y_val, X_test
        )

        # Map back to original labels
        df_train = pd.read_csv(file_paths['train'])
        le = LabelEncoder().fit(df_train['Domain'])
        preds_labels = le.inverse_transform(preds_numeric)

        # Prepare submission DataFrame
        submission_df = pd.DataFrame({
            'ID': test_ids,
            'Domain': preds_labels
        })

        # Save submission
        submission_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_131833_5dcdce5c/submission.csv"
        )
        submission_df.to_csv(submission_path, index=False)
        print(f"Submission saved to {submission_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/states/final_assembled_code.py ---

--- START FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/states/preprocessing_code_response.py ---
# import necessary libraries
import sys
import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack

def preprocess_data(file_paths):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict or list containing paths for train.csv, test.csv, Submission_file01.csv
    Returns:
        tuple: (X_train, X_val, y_train, y_val, X_test, test_ids)
    """
    # Resolve input
    if isinstance(file_paths, dict):
        train_path = file_paths.get('train')
        test_path = file_paths.get('test')
    elif isinstance(file_paths, (list, tuple)) and len(file_paths) >= 2:
        train_path, test_path = file_paths[0], file_paths[1]
    else:
        raise ValueError("file_paths must be a dict with keys 'train' and 'test', or a list of length>=2")
    # Check existence
    for p in (train_path, test_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"File not found: {p}")

    # Load data
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)

    # Basic validation
    for col in ['ID', 'Title', 'Domain']:
        if col not in df_train.columns:
            raise ValueError(f"Training data missing required column: {col}")
    for col in ['ID', 'Title']:
        if col not in df_test.columns:
            raise ValueError(f"Test data missing required column: {col}")

    # Preserve test IDs
    test_ids = df_test['ID'].copy()

    # Drop ID columns
    df_train = df_train.drop(columns=['ID'])
    df_test = df_test.drop(columns=['ID'])

    # Drop missing titles (~0.3%)
    df_train = df_train.dropna(subset=['Title'])
    df_test  = df_test.dropna(subset=['Title'])

    # Remove duplicates based on Title
    df_train = df_train.drop_duplicates(subset=['Title'])
    df_test  = df_test.drop_duplicates(subset=['Title'])

    # Lowercase and strip whitespace
    df_train['Title'] = df_train['Title'].str.lower().str.strip()
    df_test['Title']  = df_test['Title'].str.lower().str.strip()

    # Feature engineering: title_word_count
    df_train['title_word_count'] = df_train['Title'].str.split().apply(len)
    df_test['title_word_count']  = df_test['Title'].str.split().apply(len)

    # TF-IDF vectorization
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_train_tfidf = tfidf.fit_transform(df_train['Title'])
    X_test_tfidf  = tfidf.transform(df_test['Title'])

    # Label-encode target
    le = LabelEncoder()
    y = le.fit_transform(df_train['Domain'])

    # Feature selection with chi-squared
    selector = SelectKBest(chi2, k=3000)
    X_train_sel = selector.fit_transform(X_train_tfidf, y)
    X_test_sel  = selector.transform(X_test_tfidf)

    # Combine TF-IDF features with title_word_count
    X_train_full = hstack([X_train_sel, np.array(df_train['title_word_count']).reshape(-1, 1)])
    X_test_full  = hstack([X_test_sel,  np.array(df_test['title_word_count']).reshape(-1, 1)])

    # Stratified train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y, test_size=0.2, random_state=42, stratify=y
    )

    return X_train, X_val, y_train, y_val, X_test_full, test_ids

# Test the function
if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv',
            'test':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv',
            'submission': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/Submission_file01.csv'
        }
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/states/modeling_code_response.py ---
import sys
import os
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder

def train_and_predict(X_train, y_train, X_test):
    model = LogisticRegression(max_iter=1000, random_state=42)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_macro')
    print(f"CV f1_macro scores: {scores}")
    print(f"Mean CV f1_macro: {scores.mean():.4f}")
    model.fit(X_train, y_train)
    return model.predict(X_test)

if __name__ == "__main__":
    try:
        # File paths for preprocessing
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/train.csv',
            'test':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/test.csv',
            'submission': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/query_domain_classification/Submission_file01.csv'
        }

        # 1. Preprocess data
        X_train, X_val, y_train, y_val, X_test, test_ids = preprocess_data(file_paths)

        # 2. Train model and predict
        preds_numeric = train_and_predict(X_train, y_train, X_test)

        # 3. Map numeric predictions back to domain labels
        df_train = pd.read_csv(file_paths['train'])
        le = LabelEncoder()
        le.fit(df_train['Domain'])
        preds_labels = le.inverse_transform(preds_numeric)

        # 4. Prepare and save submission
        submission_df = pd.DataFrame({
            'ID': test_ids,
            'Domain': preds_labels
        })
        submission_df.to_csv("submission.csv", index=False)
        print("Modeling complete, 'submission.csv' created successfully.")

    except Exception as e:
        print(f"An error occurred during modeling: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_131833_5dcdce5c/generation_iter_0/states/modeling_code_response.py ---

--- START FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_aa451b26.py ---
import os
import sys
import re
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
from sentence_transformers import SentenceTransformer
import torch
import torchvision.transforms as T
import torchvision.models as models

def preprocess_data(file_paths: dict):
    train_csv = file_paths.get('train_csv')
    test_csv = file_paths.get('test_csv')
    img_dir   = file_paths.get('image_dir')
    if not train_csv or not test_csv or not img_dir:
        raise ValueError("file_paths must include 'train_csv', 'test_csv', and 'image_dir'")

    # load data
    df_train = pd.read_csv(train_csv)
    df_test  = pd.read_csv(test_csv)

    # helper: verify image exists and is valid
    def is_valid_image(img_path):
        try:
            with Image.open(img_path) as im:
                im.verify()
            return True
        except (FileNotFoundError, UnidentifiedImageError, OSError):
            return False

    # filter train and test for valid images
    df_train['img_path'] = df_train['ImageID'].astype(str).apply(lambda x: os.path.join(img_dir, f"{x}.jpg"))
    df_train = df_train[df_train['img_path'].apply(is_valid_image)].reset_index(drop=True)
    df_test['img_path']  = df_test['ImageID'].astype(str).apply(lambda x: os.path.join(img_dir, f"{x}.jpg"))
    df_test  = df_test[df_test['img_path'].apply(is_valid_image)].reset_index(drop=True)

    # parse labels into list of ints
    df_train['label_list'] = (df_train['Labels']
                              .fillna('')
                              .astype(str)
                              .apply(lambda s: [int(v) for v in s.split() if v.isdigit()]))

    # clean captions
    def clean_caption(text):
        txt = str(text).lower()
        txt = re.sub(r'[^\w\s]', ' ', txt)
        txt = re.sub(r'\s+', ' ', txt).strip()
        return txt

    df_train['cap_clean'] = df_train['Caption'].fillna('').apply(clean_caption)
    df_test ['cap_clean'] = df_test['Caption'].fillna('').apply(clean_caption)

    # device and models for image embeddings
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet = models.resnet50(pretrained=True)
    resnet.fc = torch.nn.Identity()
    resnet = resnet.to(device).eval()
    transform = T.Compose([
        T.Resize(256),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    ])

    def get_img_emb(path):
        img = Image.open(path).convert('RGB')
        tensor = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = resnet(tensor)
        return feat.cpu().numpy().reshape(-1)

    # compute image embeddings
    X_img_train = np.vstack([get_img_emb(p) for p in df_train['img_path']])
    X_img_test  = np.vstack([get_img_emb(p) for p in df_test['img_path']])

    # text embeddings
    sbert = SentenceTransformer('all-MiniLM-L6-v2')
    X_txt_train = sbert.encode(df_train['cap_clean'].tolist(), convert_to_numpy=True)
    X_txt_test  = sbert.encode(df_test ['cap_clean'].tolist(), convert_to_numpy=True)

    # combine features
    X_full_train = np.hstack([X_img_train, X_txt_train])
    X_test       = np.hstack([X_img_test,  X_txt_test])

    # multi-hot encode labels (length 316)
    num_classes = 316
    y_full = np.zeros((len(df_train), num_classes), dtype=int)
    for i, labs in enumerate(df_train['label_list']):
        for l in labs:
            if 1 <= l <= num_classes:
                y_full[i, l-1] = 1

    # drop labels with fewer than 5 occurrences
    keep = np.where(y_full.sum(axis=0) >= 5)[0]
    y_full = y_full[:, keep]

    # stratified train/val split
    splitter = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    train_idx, val_idx = next(splitter.split(X_full_train, y_full))
    X_train = X_full_train[train_idx]
    X_val   = X_full_train[val_idx]
    y_train = y_full[train_idx]
    y_val   = y_full[val_idx]

    test_ids = df_test['ImageID'].tolist()
    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            'train_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv',
            'test_csv':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv',
            'image_dir': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data'
        }
        X_tr, X_val, y_tr, y_val, X_te, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_aa451b26.py ---

--- START FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_3be51793.py ---
import os
import sys
import re
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
from sklearn.preprocessing import MultiLabelBinarizer
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
from sentence_transformers import SentenceTransformer
import torch
from torchvision import transforms
from torchvision.models import resnet50, ResNet50_Weights

def preprocess_data(file_paths: dict):
    train_csv = file_paths.get('train_csv')
    test_csv  = file_paths.get('test_csv')
    img_dir   = file_paths.get('image_dir')
    if not train_csv or not test_csv or not img_dir:
        raise ValueError("file_paths must include 'train_csv', 'test_csv', and 'image_dir'")
    if not os.path.isfile(train_csv):
        raise FileNotFoundError(f"Train CSV not found: {train_csv}")
    if not os.path.isfile(test_csv):
        raise FileNotFoundError(f"Test CSV not found: {test_csv}")
    if not os.path.isdir(img_dir):
        raise FileNotFoundError(f"Image directory not found: {img_dir}")

    df_train = pd.read_csv(train_csv)
    df_test  = pd.read_csv(test_csv)

    # build set of valid image IDs
    valid_ids = set()
    for fname in os.listdir(img_dir):
        base, ext = os.path.splitext(fname)
        if ext.lower() in ['.jpg', '.jpeg', '.png']:
            valid_ids.add(base)

    def img_path_for(id_):
        for ext in ['.jpg', '.jpeg', '.png']:
            p = os.path.join(img_dir, f"{id_}{ext}")
            if os.path.isfile(p):
                return p
        return None

    def is_valid_image(path):
        try:
            with Image.open(path) as img:
                img.verify()
            return True
        except Exception:
            return False

    # attach paths
    df_train['img_path'] = df_train['ImageID'].astype(str).apply(img_path_for)
    df_test ['img_path'] = df_test ['ImageID'].astype(str).apply(img_path_for)

    # filter out missing or corrupted
    df_train = df_train[df_train['img_path'].notnull()].copy()
    df_train = df_train[df_train['img_path'].apply(is_valid_image)].reset_index(drop=True)
    df_test  = df_test [df_test ['img_path'].notnull()].copy()
    df_test  = df_test [df_test ['img_path'].apply(is_valid_image)].reset_index(drop=True)

    if df_train.empty:
        raise ValueError("No valid training images found")

    # parse labels
    df_train['label_list'] = df_train['Labels'].fillna('').astype(str) \
        .apply(lambda s: [int(x) for x in s.split() if x.isdigit()])

    # clean captions
    def clean_text(t):
        s = str(t).lower()
        s = re.sub(r'[^\w\s]', ' ', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    df_train['cap_clean'] = df_train['Caption'].fillna('').apply(clean_text)
    df_test ['cap_clean'] = df_test ['Caption'].fillna('').apply(clean_text)

    # image embeddings
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    weights = ResNet50_Weights.IMAGENET1K_V2
    model = resnet50(weights=weights)
    model.fc = torch.nn.Identity()
    model = model.to(device).eval()
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=weights.meta['mean'], std=weights.meta['std'])
    ])
    def img_emb(path):
        img = Image.open(path).convert('RGB')
        x = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            out = model(x)
        return out.cpu().numpy().reshape(-1)

    X_img_train = np.vstack([img_emb(p) for p in df_train['img_path']])
    X_img_test  = np.vstack([img_emb(p) for p in df_test ['img_path']])

    # text embeddings
    sbert = SentenceTransformer('all-MiniLM-L6-v2')
    X_txt_train = sbert.encode(df_train['cap_clean'].tolist(), convert_to_numpy=True)
    X_txt_test  = sbert.encode(df_test ['cap_clean'].tolist(), convert_to_numpy=True)

    # combine features
    X_full_train = np.hstack([X_img_train, X_txt_train])
    X_test       = np.hstack([X_img_test,  X_txt_test])

    # multi-hot labels
    mlb = MultiLabelBinarizer()
    y_full = mlb.fit_transform(df_train['label_list'])

    # drop rare labels
    freq = y_full.sum(axis=0)
    keep = np.where(freq >= 5)[0]
    if keep.size == 0:
        raise ValueError("No labels with at least 5 occurrences")
    y_full = y_full[:, keep]

    # split train/val
    splitter = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    tr_idx, val_idx = next(splitter.split(X_full_train, y_full))
    X_train, X_val = X_full_train[tr_idx], X_full_train[val_idx]
    y_train, y_val = y_full[tr_idx], y_full[val_idx]

    test_ids = df_test['ImageID'].tolist()
    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            'train_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv',
            'test_csv' : '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv',
            'image_dir': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data'
        }
        X_tr, X_val, y_tr, y_val, X_te, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_3be51793.py ---

--- START FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_e01a36aa.py ---
import os
import sys
import pandas as pd
import numpy as np
import re
from PIL import Image, UnidentifiedImageError
from sklearn.model_selection import train_test_split
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as resnet_preprocess
from tensorflow.keras.preprocessing import image as kimage
from sentence_transformers import SentenceTransformer

def preprocess_data(file_paths: dict):
    train_csv = file_paths.get('train_csv')
    test_csv = file_paths.get('test_csv')
    img_dir = file_paths.get('image_dir')
    if not (train_csv and test_csv and img_dir):
        raise ValueError("Paths for 'train_csv', 'test_csv', and 'image_dir' must be provided")
    
    # Load CSVs
    df_train = pd.read_csv(train_csv)
    df_test = pd.read_csv(test_csv)
    
    # Data cleaning train
    valid_idxs = []
    for idx, img_id in enumerate(df_train['ImageID']):
        img_path = os.path.join(img_dir, f"{img_id}.jpg")
        try:
            with Image.open(img_path) as img:
                img.verify()
            valid_idxs.append(idx)
        except (FileNotFoundError, UnidentifiedImageError):
            continue
    df_train = df_train.loc[valid_idxs].reset_index(drop=True)
    
    # Parse labels
    df_train['label_list'] = df_train['Labels'].fillna('').astype(str).apply(
        lambda x: [int(i) for i in x.split() if i.isdigit()]
    )
    
    # Clean captions
    def clean_text(txt):
        txt = str(txt).lower()
        txt = re.sub(r'[^\w\s]', '', txt)
        return re.sub(r'\s+', ' ', txt).strip()
    df_train['caption_clean'] = df_train['Caption'].fillna('').apply(clean_text)
    df_test['caption_clean'] = df_test['Caption'].fillna('').apply(clean_text)
    
    # Image embedding model
    resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    def get_img_emb(img_id):
        path = os.path.join(img_dir, f"{img_id}.jpg")
        img = kimage.load_img(path, target_size=(224,224))
        arr = kimage.img_to_array(img)
        arr = np.expand_dims(arr, axis=0)
        arr = resnet_preprocess(arr)
        return resnet.predict(arr)[0]
    
    # Compute image embeddings
    X_img_train = np.vstack([get_img_emb(i) for i in df_train['ImageID']])
    X_img_test = np.vstack([get_img_emb(i) for i in df_test['ImageID']])
    
    # Text embedding model
    sbert = SentenceTransformer('all-MiniLM-L6-v2')
    X_txt_train = sbert.encode(df_train['caption_clean'].tolist(), convert_to_numpy=True)
    X_txt_test = sbert.encode(df_test['caption_clean'].tolist(), convert_to_numpy=True)
    
    # Combine features
    X_train_full = np.hstack([X_img_train, X_txt_train])
    X_test = np.hstack([X_img_test, X_txt_test])
    
    # Multi-hot labels (length 316)
    num_classes = 316
    y = np.zeros((len(df_train), num_classes), dtype=int)
    for i, labs in enumerate(df_train['label_list']):
        for l in labs:
            if 1 <= l <= num_classes:
                y[i, l-1] = 1
    
    # Drop rare labels
    label_counts = y.sum(axis=0)
    keep_cols = np.where(label_counts >= 5)[0]
    y = y[:, keep_cols]
    
    # Stratified split
    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    train_idx, val_idx = next(msss.split(X_train_full, y))
    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    return X_train, X_val, y_train, y_val, X_test, df_test['ImageID'].tolist()

if __name__ == "__main__":
    try:
        paths = {
            'train_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv',
            'test_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv',
            'image_dir': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data'
        }
        X_tr, X_val, y_tr, y_val, X_te, test_ids = preprocess_data(paths)
        print("Preprocessing completed.")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_e01a36aa.py ---

--- START FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_a7b8f6fd.py ---
import os
import sys
import re
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
from sklearn.preprocessing import MultiLabelBinarizer
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
from sentence_transformers import SentenceTransformer
import torch
from torchvision import transforms
from torchvision.models import resnet50, ResNet50_Weights

def preprocess_data(file_paths: dict):
    train_csv = file_paths.get('train_csv')
    test_csv = file_paths.get('test_csv')
    img_dir   = file_paths.get('image_dir')
    if not train_csv or not test_csv or not img_dir:
        raise ValueError("file_paths must include 'train_csv', 'test_csv', and 'image_dir'")
    if not os.path.isfile(train_csv) or not os.path.isfile(test_csv):
        raise FileNotFoundError("Train or test CSV path is invalid")
    if not os.path.isdir(img_dir):
        raise FileNotFoundError("Image directory path is invalid")

    df_train = pd.read_csv(train_csv)
    df_test  = pd.read_csv(test_csv)

    def is_valid_image(path):
        try:
            with Image.open(path) as img:
                img.verify()
            return True
        except (FileNotFoundError, UnidentifiedImageError, OSError):
            return False

    df_train['img_path'] = df_train['ImageID'].astype(str).apply(
        lambda x: os.path.join(img_dir, f"{x}.jpg"))
    df_test['img_path']  = df_test['ImageID'].astype(str).apply(
        lambda x: os.path.join(img_dir, f"{x}.jpg"))

    df_train = df_train[df_train['img_path'].apply(is_valid_image)].reset_index(drop=True)
    df_test  = df_test[df_test['img_path'].apply(is_valid_image)].reset_index(drop=True)

    if df_train.empty:
        raise ValueError("No valid training images found")
    # parse labels
    df_train['label_list'] = (
        df_train['Labels'].fillna('')
        .astype(str)
        .apply(lambda s: [int(t) for t in s.split() if t.isdigit()])
    )
    # clean captions
    def clean_text(t):
        s = str(t).lower()
        s = re.sub(r'[^\w\s]', ' ', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    df_train['cap_clean'] = df_train['Caption'].fillna('').apply(clean_text)
    df_test['cap_clean']  = df_test['Caption'].fillna('').apply(clean_text)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    weights = ResNet50_Weights.IMAGENET1K_V2
    model = resnet50(weights=weights)
    model.fc = torch.nn.Identity()
    model = model.to(device).eval()

    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=weights.meta['mean'],
                             std=weights.meta['std'])
    ])

    def get_img_emb(path):
        img = Image.open(path).convert('RGB')
        x = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = model(x)
        return feat.cpu().numpy().reshape(-1)

    img_feats_train = [get_img_emb(p) for p in df_train['img_path']]
    img_feats_test  = [get_img_emb(p) for p in df_test['img_path']]
    feat_dim = img_feats_train[0].shape[0]

    X_img_train = np.vstack(img_feats_train) if img_feats_train else np.empty((0, feat_dim))
    X_img_test  = np.vstack(img_feats_test)  if img_feats_test  else np.empty((0, feat_dim))

    sbert = SentenceTransformer('all-MiniLM-L6-v2')
    X_txt_train = sbert.encode(df_train['cap_clean'].tolist(), convert_to_numpy=True)
    X_txt_test  = sbert.encode(df_test['cap_clean'].tolist(), convert_to_numpy=True)

    X_full_train = np.hstack([X_img_train, X_txt_train])
    X_test       = np.hstack([X_img_test,  X_txt_test])

    mlb = MultiLabelBinarizer(sparse_output=False)
    y_full = mlb.fit_transform(df_train['label_list'])

    # drop rare labels
    freq = y_full.sum(axis=0)
    keep_idx = np.where(freq >= 5)[0]
    if keep_idx.size == 0:
        raise ValueError("No labels with at least 5 occurrences")
    y_full = y_full[:, keep_idx]

    splitter = MultilabelStratifiedShuffleSplit(
        n_splits=1, test_size=0.2, random_state=42)
    train_idx, val_idx = next(splitter.split(X_full_train, y_full))
    X_train = X_full_train[train_idx]
    X_val   = X_full_train[val_idx]
    y_train = y_full[train_idx]
    y_val   = y_full[val_idx]

    test_ids = df_test['ImageID'].tolist()
    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = {
            'train_csv': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv',
            'test_csv':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv',
            'image_dir': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data'
        }
        X_tr, X_val, y_tr, y_val, X_te, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_a7b8f6fd.py ---

--- START FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_7d0944a4.py ---
import os
import sys
import re
import glob
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
import torch
from torchvision import transforms
from torchvision.models import resnet50, ResNet50_Weights
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import MultiLabelBinarizer
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit

def preprocess_data(file_paths):
    # normalize input
    if isinstance(file_paths, (list, tuple)) and len(file_paths) == 3:
        fp = {
            'train_csv': file_paths[0],
            'test_csv' : file_paths[1],
            'image_dir': file_paths[2]
        }
    elif isinstance(file_paths, dict):
        fp = file_paths
    else:
        raise ValueError("file_paths must be a dict or list of [train_csv, test_csv, image_dir]")
    train_csv = fp.get('train_csv')
    test_csv  = fp.get('test_csv')
    img_dir   = fp.get('image_dir')
    # verify paths
    if not train_csv or not os.path.isfile(train_csv):
        raise FileNotFoundError(f"Train CSV not found: {train_csv}")
    if not test_csv or not os.path.isfile(test_csv):
        raise FileNotFoundError(f"Test CSV not found: {test_csv}")
    if not img_dir or not os.path.isdir(img_dir):
        raise FileNotFoundError(f"Image directory not found: {img_dir}")

    # load data
    df_train = pd.read_csv(train_csv)
    df_test  = pd.read_csv(test_csv)

    # map image IDs to full paths
    img_paths = {}
    for path in glob.glob(os.path.join(img_dir, '*')):
        base = os.path.splitext(os.path.basename(path))[0]
        img_paths[base] = path

    def get_img_path(img_id):
        return img_paths.get(str(img_id))

    def is_good_image(p):
        try:
            with Image.open(p) as im:
                im.verify()
            return True
        except Exception:
            return False

    # attach and validate image paths
    df_train['img_path'] = df_train['ImageID'].astype(str).apply(get_img_path)
    df_train = df_train[df_train['img_path'].notnull()].copy()
    df_train = df_train[df_train['img_path'].apply(is_good_image)].reset_index(drop=True)
    df_test['img_path']  = df_test['ImageID'].astype(str).apply(get_img_path)
    df_test  = df_test[df_test['img_path'].notnull()].copy()
    df_test  = df_test[df_test['img_path'].apply(is_good_image)].reset_index(drop=True)
    if df_train.empty:
        raise ValueError("No valid training images found")

    # parse labels
    df_train['label_list'] = df_train['Labels'].fillna('').astype(str) \
        .apply(lambda s: [int(x) for x in s.split() if x.isdigit()])

    # clean captions
    def clean_text(t):
        s = str(t).lower()
        s = re.sub(r'[^\w\s]', ' ', s)
        return re.sub(r'\s+', ' ', s).strip()
    df_train['cap_clean'] = df_train['Caption'].fillna('').apply(clean_text)
    df_test ['cap_clean'] = df_test ['Caption'].fillna('').apply(clean_text)

    # image embedding setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    weights = ResNet50_Weights.IMAGENET1K_V2
    img_model = resnet50(weights=weights)
    img_model.fc = torch.nn.Identity()
    img_model = img_model.to(device).eval()
    img_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=weights.meta['mean'], std=weights.meta['std'])
    ])
    def embed_image(p):
        img = Image.open(p).convert('RGB')
        x = img_transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            out = img_model(x)
        return out.cpu().numpy().reshape(-1)

    X_img_train = np.vstack([embed_image(p) for p in df_train['img_path']])
    X_img_test  = np.vstack([embed_image(p) for p in df_test ['img_path']])

    # text embeddings
    txt_model = SentenceTransformer('all-MiniLM-L6-v2')
    X_txt_train = txt_model.encode(df_train['cap_clean'].tolist(), convert_to_numpy=True)
    X_txt_test  = txt_model.encode(df_test ['cap_clean'].tolist(), convert_to_numpy=True)

    # combine features
    X_full_train = np.hstack([X_img_train, X_txt_train])
    X_test       = np.hstack([X_img_test,  X_txt_test])

    # multi-hot encode labels
    mlb = MultiLabelBinarizer()
    y_full = mlb.fit_transform(df_train['label_list'])
    # drop rare labels
    freq = y_full.sum(axis=0)
    keep = np.where(freq >= 5)[0]
    if keep.size == 0:
        raise ValueError("No labels with at least 5 occurrences")
    y_full = y_full[:, keep]

    # split train/val
    splitter = MultilabelStratifiedShuffleSplit(
        n_splits=1, test_size=0.2, random_state=42
    )
    train_idx, val_idx = next(splitter.split(X_full_train, y_full))
    X_train = X_full_train[train_idx]
    X_val   = X_full_train[val_idx]
    y_train = y_full[train_idx]
    y_val   = y_full[val_idx]

    test_ids = df_test['ImageID'].tolist()
    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = [
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data'
        ]
        X_tr, X_val, y_tr, y_val, X_te, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/temp_exec_7d0944a4.py ---

--- START FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/states/preprocessing_code_response.py ---
import os
import sys
import re
import glob
import pandas as pd
import numpy as np
from PIL import Image, UnidentifiedImageError
import torch
from torchvision import transforms
from torchvision.models import resnet50, ResNet50_Weights
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import MultiLabelBinarizer
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit

def preprocess_data(file_paths):
    # normalize input
    if isinstance(file_paths, (list, tuple)) and len(file_paths) == 3:
        fp = {
            'train_csv': file_paths[0],
            'test_csv' : file_paths[1],
            'image_dir': file_paths[2]
        }
    elif isinstance(file_paths, dict):
        fp = file_paths
    else:
        raise ValueError("file_paths must be a dict or list of [train_csv, test_csv, image_dir]")
    train_csv = fp.get('train_csv')
    test_csv  = fp.get('test_csv')
    img_dir   = fp.get('image_dir')
    # verify paths
    if not train_csv or not os.path.isfile(train_csv):
        raise FileNotFoundError(f"Train CSV not found: {train_csv}")
    if not test_csv or not os.path.isfile(test_csv):
        raise FileNotFoundError(f"Test CSV not found: {test_csv}")
    if not img_dir or not os.path.isdir(img_dir):
        raise FileNotFoundError(f"Image directory not found: {img_dir}")

    # load data
    df_train = pd.read_csv(train_csv)
    df_test  = pd.read_csv(test_csv)

    # map image IDs to full paths
    img_paths = {}
    for path in glob.glob(os.path.join(img_dir, '*')):
        base = os.path.splitext(os.path.basename(path))[0]
        img_paths[base] = path

    def get_img_path(img_id):
        return img_paths.get(str(img_id))

    def is_good_image(p):
        try:
            with Image.open(p) as im:
                im.verify()
            return True
        except Exception:
            return False

    # attach and validate image paths
    df_train['img_path'] = df_train['ImageID'].astype(str).apply(get_img_path)
    df_train = df_train[df_train['img_path'].notnull()].copy()
    df_train = df_train[df_train['img_path'].apply(is_good_image)].reset_index(drop=True)
    df_test['img_path']  = df_test['ImageID'].astype(str).apply(get_img_path)
    df_test  = df_test[df_test['img_path'].notnull()].copy()
    df_test  = df_test[df_test['img_path'].apply(is_good_image)].reset_index(drop=True)
    if df_train.empty:
        raise ValueError("No valid training images found")

    # parse labels
    df_train['label_list'] = df_train['Labels'].fillna('').astype(str) \
        .apply(lambda s: [int(x) for x in s.split() if x.isdigit()])

    # clean captions
    def clean_text(t):
        s = str(t).lower()
        s = re.sub(r'[^\w\s]', ' ', s)
        return re.sub(r'\s+', ' ', s).strip()
    df_train['cap_clean'] = df_train['Caption'].fillna('').apply(clean_text)
    df_test ['cap_clean'] = df_test ['Caption'].fillna('').apply(clean_text)

    # image embedding setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    weights = ResNet50_Weights.IMAGENET1K_V2
    img_model = resnet50(weights=weights)
    img_model.fc = torch.nn.Identity()
    img_model = img_model.to(device).eval()
    img_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=weights.meta['mean'], std=weights.meta['std'])
    ])
    def embed_image(p):
        img = Image.open(p).convert('RGB')
        x = img_transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            out = img_model(x)
        return out.cpu().numpy().reshape(-1)

    X_img_train = np.vstack([embed_image(p) for p in df_train['img_path']])
    X_img_test  = np.vstack([embed_image(p) for p in df_test ['img_path']])

    # text embeddings
    txt_model = SentenceTransformer('all-MiniLM-L6-v2')
    X_txt_train = txt_model.encode(df_train['cap_clean'].tolist(), convert_to_numpy=True)
    X_txt_test  = txt_model.encode(df_test ['cap_clean'].tolist(), convert_to_numpy=True)

    # combine features
    X_full_train = np.hstack([X_img_train, X_txt_train])
    X_test       = np.hstack([X_img_test,  X_txt_test])

    # multi-hot encode labels
    mlb = MultiLabelBinarizer()
    y_full = mlb.fit_transform(df_train['label_list'])
    # drop rare labels
    freq = y_full.sum(axis=0)
    keep = np.where(freq >= 5)[0]
    if keep.size == 0:
        raise ValueError("No labels with at least 5 occurrences")
    y_full = y_full[:, keep]

    # split train/val
    splitter = MultilabelStratifiedShuffleSplit(
        n_splits=1, test_size=0.2, random_state=42
    )
    train_idx, val_idx = next(splitter.split(X_full_train, y_full))
    X_train = X_full_train[train_idx]
    X_val   = X_full_train[val_idx]
    y_train = y_full[train_idx]
    y_val   = y_full[val_idx]

    test_ids = df_test['ImageID'].tolist()
    return X_train, X_val, y_train, y_val, X_test, test_ids

if __name__ == "__main__":
    try:
        file_paths = [
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/train.csv',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/test.csv',
            '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/multi_label_classification/data'
        ]
        X_tr, X_val, y_tr, y_val, X_te, test_ids = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_125538_f5bfb46c/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/states/final_preprocessing_code.py ---
import os
import sys
import pandas as pd
import numpy as np
import string

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sentence_transformers import SentenceTransformer

def preprocess_data(file_paths: dict):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict with keys 'train', 'test', 'sample' pointing to CSV file paths.
    Returns:
        Tuple containing:
            X_tfidf_train (sparse matrix),
            X_tfidf_val (sparse matrix),
            X_embed_train (ndarray),
            X_embed_val (ndarray),
            y_train (ndarray),
            y_val (ndarray),
            X_tfidf_test (sparse matrix),
            X_embed_test (ndarray),
            test_ids (pd.Index)
    """
    # Validate input paths
    required_keys = ['train', 'test', 'sample']
    for key in required_keys:
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' in file_paths dict")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load datasets
    train_df = pd.read_csv(file_paths['train'])
    test_df = pd.read_csv(file_paths['test'])

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Response' not in df.columns:
            raise ValueError(f"'Response' column missing in {name} data")
        if 'Question' not in df.columns:
            raise ValueError(f"'Question' column missing in {name} data")

    # Data cleaning
    train_df = train_df.drop(columns=['Question'])
    test_df  = test_df.drop(columns=['Question'])
    train_df = train_df.dropna(subset=['Response'])
    test_df  = test_df.dropna(subset=['Response'])

    # Lowercase + remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(s: str) -> str:
        return s.lower().translate(translator)
    train_df['clean_resp'] = train_df['Response'].astype(str).map(clean_text)
    test_df['clean_resp']  = test_df['Response'].astype(str).map(clean_text)

    # TF-IDF vectorization (1,2)-grams
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=None)
    X_tfidf_all = tfidf.fit_transform(train_df['clean_resp'])
    X_tfidf_test = tfidf.transform(test_df['clean_resp'])

    # Feature selection: top 5000 by chi2
    y = train_df['target'].values
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_train_sel = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_test_sel  = selector.transform(X_tfidf_test)

    # Sentence embeddings via pretrained model
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    X_embed_all   = embedder.encode(train_df['clean_resp'].tolist(), show_progress_bar=False)
    X_embed_test  = embedder.encode(test_df['clean_resp'].tolist(), show_progress_bar=False)

    # Train/validation split (stratified)
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf_train_sel, X_embed_all, y,
        test_size=0.2, random_state=42, stratify=y
    )

    test_ids = test_df['id'] if 'id' in test_df.columns else test_df.index

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
        X_tfidf_test_sel,
        X_embed_test,
        test_ids
    )

if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }
        outputs = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/states/final_preprocessing_code.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/states/final_executable_code.py ---
#!/usr/bin/env python3

import os
import sys
import string

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix, hstack

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import f1_score

import lightgbm as lgb
from sentence_transformers import SentenceTransformer


def preprocess_data(file_paths: dict):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict with keys 'train', 'test', 'sample' pointing to CSV file paths.
    Returns:
        Tuple containing:
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val,
            X_tfidf_test, X_emb_test,
            test_ids
    """
    # Validate input paths
    required_keys = ['train', 'test', 'sample']
    for key in required_keys:
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' in file_paths dict")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load datasets
    train_df = pd.read_csv(file_paths['train'])
    test_df = pd.read_csv(file_paths['test'])

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Response' not in df.columns:
            raise ValueError(f"'Response' column missing in {name} data")
        if 'Question' not in df.columns:
            raise ValueError(f"'Question' column missing in {name} data")
    if 'target' not in train_df.columns:
        raise ValueError("'target' column missing in train data")

    # Drop unused columns & NaNs
    train_df = train_df.drop(columns=['Question']).dropna(subset=['Response'])
    test_df  = test_df.drop(columns=['Question']).dropna(subset=['Response'])

    # Lowercase + remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(s: str) -> str:
        return s.lower().translate(translator)

    train_df['clean_resp'] = train_df['Response'].astype(str).map(clean_text)
    test_df['clean_resp']  = test_df['Response'].astype(str).map(clean_text)

    # TF-IDF vectorization (1,2)-grams
    tfidf = TfidfVectorizer(ngram_range=(1, 2))
    X_tfidf_all  = tfidf.fit_transform(train_df['clean_resp'])
    X_tfidf_test = tfidf.transform(test_df['clean_resp'])

    # Feature selection: top 5000 by chi2
    y = train_df['target'].values
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_sel_all  = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_sel_test = selector.transform(X_tfidf_test)

    # Sentence embeddings
    embedder     = SentenceTransformer('all-MiniLM-L6-v2')
    X_embed_all  = embedder.encode(train_df['clean_resp'].tolist(),
                                   show_progress_bar=False)
    X_embed_test = embedder.encode(test_df['clean_resp'].tolist(),
                                   show_progress_bar=False)

    # Train/validation split (stratified)
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf_sel_all, X_embed_all, y,
        test_size=0.2, random_state=42, stratify=y
    )

    # Test IDs
    test_ids = test_df['id'] if 'id' in test_df.columns else test_df.index

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
        X_tfidf_sel_test,
        X_embed_test,
        test_ids
    )


def train_and_evaluate(
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
    y_train, y_val,
    X_tfidf_test, X_emb_test
):
    """
    Train a LightGBM classifier on combined TF-IDF and embeddings,
    evaluate on validation set, and return test predictions.
    """
    # Combine sparse TF-IDF and dense embeddings
    X_train = hstack([X_tfidf_tr, csr_matrix(X_emb_tr)])
    X_val   = hstack([X_tfidf_val, csr_matrix(X_emb_val)])
    X_test  = hstack([X_tfidf_test, csr_matrix(X_emb_test)])

    # Determine number of classes
    num_classes = len(np.unique(y_train))

    # Initialize & train model
    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=num_classes,
        boosting_type='gbdt',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)

    # Validation
    val_preds = model.predict(X_val)
    val_f1    = f1_score(y_val, val_preds, average='macro')
    print(f"Validation Macro F1 Score: {val_f1:.4f}")

    # Test predictions
    test_preds = model.predict(X_test)
    return test_preds


if __name__ == "__main__":
    try:
        file_paths = {
            'train':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test':   '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }

        (X_tfidf_tr,
         X_tfidf_val,
         X_emb_tr,
         X_emb_val,
         y_train,
         y_val,
         X_tfidf_test,
         X_emb_test,
         test_ids) = preprocess_data(file_paths)

        preds_test = train_and_evaluate(
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val,
            X_tfidf_test, X_emb_test
        )

        submission = pd.DataFrame({
            'id':     test_ids,
            'target': preds_test
        })

        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
        )
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/states/final_executable_code.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_c3c595ea.py ---
#!/usr/bin/env python3
import os
import sys
import logging
import string

import pandas as pd
import numpy as np
import joblib

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, log_loss
import lightgbm as lgb

from sentence_transformers import SentenceTransformer

# Absolute paths (do not change)
TRAIN_PATH = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv'
TEST_PATH  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv'
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
)

NGRAM_RANGE = (1, 2)
TFIDF_MAX_FEATURES = 5000
EMB_REDUCED_DIM = 128
N_FOLDS = 5
RANDOM_STATE = 42
EARLY_STOPPING_ROUNDS = 50
LGBM_N_ESTIMATORS = 1000
BATCH_SIZE = 64

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)]
    )

def clean_texts(series: pd.Series) -> pd.Series:
    translator = str.maketrans('', '', string.punctuation)
    return series.fillna('').str.lower().str.translate(translator)

def load_and_preprocess():
    # Load
    train = pd.read_csv(TRAIN_PATH)
    test  = pd.read_csv(TEST_PATH)

    # Ensure required columns
    for df, name in [(train, 'train'), (test, 'test')]:
        if 'Question' not in df.columns or 'Response' not in df.columns:
            raise ValueError(f"Missing 'Question' or 'Response' in {name}.csv")

    train['Response'] = train['Response'].fillna('')
    test['Response']  = test['Response'].fillna('')

    # Combine question + response
    train['text'] = train['Question'].fillna('') + " " + train['Response']
    test['text']  = test['Question'].fillna('') + " " + test['Response']

    # Clean text
    train['text'] = clean_texts(train['text'])
    test['text']  = clean_texts(test['text'])

    y = train['target'].values
    test_ids = test['id'].values if 'id' in test.columns else np.arange(len(test))

    return train['text'].tolist(), y, test['text'].tolist(), test_ids

def compute_tfidf(train_texts, test_texts):
    cache_file = "tfidf_vectorizer.joblib"
    if os.path.exists(cache_file):
        vec, X_tr, X_te = joblib.load(cache_file)
        logging.info("Loaded TF-IDF from cache.")
    else:
        vec = TfidfVectorizer(ngram_range=NGRAM_RANGE, max_features=TFIDF_MAX_FEATURES)
        X_tr = vec.fit_transform(train_texts).toarray()
        X_te = vec.transform(test_texts).toarray()
        joblib.dump((vec, X_tr, X_te), cache_file)
        logging.info("Computed and cached TF-IDF.")
    return X_tr, X_te

def compute_embeddings(train_texts, test_texts):
    cache_file = "sentence_embeddings.joblib"
    if os.path.exists(cache_file):
        emb_tr, emb_te = joblib.load(cache_file)
        logging.info("Loaded embeddings from cache.")
    else:
        embedder = SentenceTransformer('all-MiniLM-L6-v2')
        emb_tr = embedder.encode(
            train_texts,
            batch_size=BATCH_SIZE,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        emb_te = embedder.encode(
            test_texts,
            batch_size=BATCH_SIZE,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        joblib.dump((emb_tr, emb_te), cache_file)
        logging.info("Computed and cached embeddings.")
    return emb_tr, emb_te

def reduce_embeddings(emb_tr, emb_te):
    cache_file = "pca_embeddings.joblib"
    if os.path.exists(cache_file):
        pca, emb_tr_r, emb_te_r = joblib.load(cache_file)
        logging.info("Loaded PCA-reduced embeddings from cache.")
    else:
        pca = PCA(n_components=EMB_REDUCED_DIM, random_state=RANDOM_STATE)
        emb_tr_r = pca.fit_transform(emb_tr)
        emb_te_r = pca.transform(emb_te)
        joblib.dump((pca, emb_tr_r, emb_te_r), cache_file)
        logging.info("Computed and cached PCA on embeddings.")
    return emb_tr_r, emb_te_r

def train_and_cross_validate(X, y, X_test):
    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    n_classes = len(np.unique(y))
    oof_preds = np.zeros((len(y), n_classes))
    test_preds = np.zeros((X_test.shape[0], n_classes))
    fold = 0

    for train_idx, val_idx in skf.split(X, y):
        fold += 1
        logging.info(f"Starting fold {fold}/{N_FOLDS}")
        X_tr, X_val = X[train_idx], X[val_idx]
        y_tr, y_val = y[train_idx], y[val_idx]

        model = lgb.LGBMClassifier(
            objective='multiclass',
            num_class=n_classes,
            n_estimators=LGBM_N_ESTIMATORS,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            eval_metric='multi_logloss',
            early_stopping_rounds=EARLY_STOPPING_ROUNDS,
            verbose=50
        )
        val_proba = model.predict_proba(X_val)
        oof_preds[val_idx] = val_proba
        test_preds += model.predict_proba(X_test) / N_FOLDS

        fold_f1 = f1_score(y_val, val_proba.argmax(axis=1), average='macro')
        fold_ll = log_loss(y_val, val_proba)
        logging.info(f"Fold {fold} F1: {fold_f1:.4f}, LogLoss: {fold_ll:.4f}")

    # CV metrics
    cv_f1 = f1_score(y, oof_preds.argmax(axis=1), average='macro')
    cv_ll = log_loss(y, oof_preds)
    logging.info(f"CV Macro F1: {cv_f1:.4f}, CV LogLoss: {cv_ll:.4f}")
    return test_preds

def main():
    setup_logging()
    logging.info("Loading and preprocessing data...")
    train_texts, y, test_texts, test_ids = load_and_preprocess()

    logging.info("Computing TF-IDF features...")
    X_tfidf_tr, X_tfidf_te = compute_tfidf(train_texts, test_texts)

    logging.info("Computing sentence embeddings...")
    emb_tr, emb_te = compute_embeddings(train_texts, test_texts)

    logging.info("Reducing embedding dimensionality with PCA...")
    emb_tr_r, emb_te_r = reduce_embeddings(emb_tr, emb_te)

    logging.info("Stacking features...")
    X_train = np.hstack([X_tfidf_tr, emb_tr_r])
    X_test  = np.hstack([X_tfidf_te, emb_te_r])

    logging.info("Training with Stratified K-Fold CV...")
    preds_test = train_and_cross_validate(X_train, y, X_test)

    # Prepare submission
    logging.info("Preparing submission file...")
    submission = pd.DataFrame({'id': test_ids})
    for cls in range(preds_test.shape[1]):
        submission[f"target_{cls}"] = preds_test[:, cls]
    submission.to_csv(OUTPUT_PATH, index=False)
    logging.info(f"Submission written to {OUTPUT_PATH}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.exception("An error occurred")
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_c3c595ea.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_19b2bb6d.py ---
#!/usr/bin/env python3
# Candidate #3: Improved script with batched embeddings, PCA reduction, early stopping, predict_proba, and logging

import os
import sys
import string
import logging
import joblib

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix, hstack

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, accuracy_score, log_loss

import lightgbm as lgb
from sentence_transformers import SentenceTransformer

# ----------------------------
# Configuration
# ----------------------------
TRAIN_PATH = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv'
TEST_PATH = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv'
SAMPLE_SUB_PATH = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
)

BATCH_SIZE = 64             # for sentence embeddings
PCA_DIM = 128               # reduce embeddings from 768 to 128
CHI2_K = 5000               # top-K TF-IDF features
RANDOM_STATE = 42
TEST_SIZE = 0.2
EARLY_STOPPING_ROUNDS = 50
VERBOSE_EVAL = 20

# ----------------------------
# Logging setup
# ----------------------------
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ----------------------------
# Functions
# ----------------------------
def clean_text_series(series: pd.Series) -> pd.Series:
    """Lowercase and remove punctuation."""
    translator = str.maketrans('', '', string.punctuation)
    return series.astype(str).str.lower().str.translate(translator)

def preprocess_data():
    """Load, clean, vectorize, embed, and split data."""
    # Validate files
    for path in (TRAIN_PATH, TEST_PATH):
        if not os.path.isfile(path):
            logger.error(f"File not found: {path}")
            sys.exit(1)

    # Load
    train_df = pd.read_csv(TRAIN_PATH)
    test_df  = pd.read_csv(TEST_PATH)

    # Drop Question, drop missing Response
    for df in (train_df, test_df):
        if 'Response' not in df.columns or 'Question' not in df.columns:
            logger.error("Missing required columns in data")
            sys.exit(1)
    train_df = train_df.drop(columns=['Question']).dropna(subset=['Response'])
    test_df  = test_df.drop(columns=['Question']).dropna(subset=['Response'])
    test_ids = test_df['id']

    # Clean text
    train_df['clean'] = clean_text_series(train_df['Response'])
    test_df['clean']  = clean_text_series(test_df['Response'])

    # TF-IDF vectorization
    logger.info("Fitting TF-IDF vectorizer...")
    tfidf = TfidfVectorizer(ngram_range=(1,2))
    X_tfidf_all = tfidf.fit_transform(train_df['clean'])
    X_tfidf_test = tfidf.transform(test_df['clean'])

    # Chi2 feature selection
    logger.info(f"Selecting top {CHI2_K} TF-IDF features by chi2...")
    selector = SelectKBest(chi2, k=CHI2_K)
    y_all = train_df['target'].values
    X_tfidf_all = selector.fit_transform(X_tfidf_all, y_all)
    X_tfidf_test = selector.transform(X_tfidf_test)

    # Sentence embeddings (batched)
    logger.info("Computing sentence embeddings in batches...")
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    X_emb_all  = embedder.encode(
        train_df['clean'].tolist(),
        batch_size=BATCH_SIZE,
        convert_to_numpy=True,
        show_progress_bar=True
    )
    X_emb_test = embedder.encode(
        test_df['clean'].tolist(),
        batch_size=BATCH_SIZE,
        convert_to_numpy=True,
        show_progress_bar=True
    )

    # PCA reduction on embeddings
    logger.info(f"Reducing embeddings to {PCA_DIM} dims via PCA...")
    pca = PCA(n_components=PCA_DIM, random_state=RANDOM_STATE)
    X_emb_all  = pca.fit_transform(X_emb_all)
    X_emb_test = pca.transform(X_emb_test)

    # Train/validation split
    logger.info(f"Splitting data train/val (test_size={TEST_SIZE})...")
    splits = train_test_split(
        X_tfidf_all, X_emb_all, y_all,
        test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_all
    )
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = splits

    return (
        X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
        y_train, y_val, X_tfidf_test, X_emb_test, test_ids
    )

def train_and_evaluate(
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
    y_train, y_val, X_tfidf_test, X_emb_test
):
    """Train LightGBM with early stopping, evaluate, and predict probabilities."""
    # Combine features
    logger.info("Combining TF-IDF and embedding features...")
    X_train = hstack([X_tfidf_tr, csr_matrix(X_emb_tr)])
    X_val   = hstack([X_tfidf_val, csr_matrix(X_emb_val)])
    X_test  = hstack([X_tfidf_test, csr_matrix(X_emb_test)])

    num_classes = len(np.unique(y_train))
    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=num_classes,
        boosting_type='gbdt',
        random_state=RANDOM_STATE,
        n_jobs=-1
    )

    # Train with early stopping
    logger.info("Training LightGBM with early stopping...")
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        eval_metric='multi_logloss',
        early_stopping_rounds=EARLY_STOPPING_ROUNDS,
        verbose=VERBOSE_EVAL
    )

    # Validation metrics
    val_probs = model.predict_proba(X_val)
    val_preds = np.argmax(val_probs, axis=1)
    f1 = f1_score(y_val, val_preds, average='macro')
    acc = accuracy_score(y_val, val_preds)
    ll  = log_loss(y_val, val_probs)
    logger.info(f"Validation accuracy: {acc:.4f}, f1_macro: {f1:.4f}, log_loss: {ll:.4f}")

    # Test prediction probabilities
    logger.info("Generating test set probabilities...")
    test_probs = model.predict_proba(X_test)
    return test_probs

def main():
    try:
        (
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val, X_tfidf_test, X_emb_test, test_ids
        ) = preprocess_data()

        test_probs = train_and_evaluate(
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val, X_tfidf_test, X_emb_test
        )

        # Build submission DataFrame
        logger.info("Building submission DataFrame...")
        n_classes = test_probs.shape[1]
        cols = {f"target_{i}": test_probs[:, i] for i in range(n_classes)}
        submission = pd.DataFrame({'id': test_ids})
        for k, v in cols.items():
            submission[k] = v

        # Save
        submission.to_csv(OUTPUT_PATH, index=False)
        logger.info(f"Submission saved to {OUTPUT_PATH}")

    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_19b2bb6d.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_b2bfee36.py ---
#!/usr/bin/env python3
# Candidate #2: Enhanced training with 5-fold CV, early stopping, PCA on embeddings,
# ensemble of TF-IDF+PCA embeddings, batch embedding, caching, and probability output.

import os
import logging
import joblib
import numpy as np
import pandas as pd
from scipy.sparse import hstack, csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, log_loss
import lightgbm as lgb
from sentence_transformers import SentenceTransformer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
logger = logging.getLogger(__name__)

# Hardcoded file paths (must not change)
TRAIN_PATH = (
    "/home/shiinehata/Desktop/iSE/iSE_AutoML/"
    "model_eval/datasets/predict_the_llms/train.csv"
)
TEST_PATH = (
    "/home/shiinehata/Desktop/iSE/iSE_AutoML/"
    "model_eval/datasets/predict_the_llms/test.csv"
)
SAMPLE_SUB_PATH = (
    "/home/shiinehata/Desktop/iSE/iSE_AutoML/"
    "model_eval/datasets/predict_the_llms/sample_submission.csv"
)
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_cv_candidate2/submission.csv"
)

# Cache file names
CACHE_DIR = "./cache_candidate2"
os.makedirs(CACHE_DIR, exist_ok=True)
EMB_TRAIN_CACHE = os.path.join(CACHE_DIR, "train_emb.npy")
EMB_TEST_CACHE = os.path.join(CACHE_DIR, "test_emb.npy")

def load_and_clean():
    logger.info("Loading data")
    train = pd.read_csv(TRAIN_PATH)
    test  = pd.read_csv(TEST_PATH)

    # Drop missing responses
    train.dropna(subset=["Response"], inplace=True)
    test.dropna(subset=["Response"], inplace=True)

    # Clean text: lowercase & remove punctuation
    import string
    translator = str.maketrans("", "", string.punctuation)
    def clean(s):
        return s.lower().translate(translator)

    train["clean_q"] = train["Question"].astype(str).map(clean)
    train["clean_r"] = train["Response"].astype(str).map(clean)
    test["clean_q"]  = test["Question"].astype(str).map(clean)
    test["clean_r"]  = test["Response"].astype(str).map(clean)

    # Combine question and response for richer features
    train["text"] = train["clean_q"] + " " + train["clean_r"]
    test["text"]  = test["clean_q"] + " " + test["clean_r"]

    return train, test

def vectorize_text(train_texts, test_texts):
    logger.info("Vectorizing text with TF-IDF")
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=10000)
    X_tr = tfidf.fit_transform(train_texts)
    X_te = tfidf.transform(test_texts)

    # Select top k correlations
    logger.info("Selecting top 5000 features by chi2")
    selector = SelectKBest(chi2, k=5000)
    y = train["target"].values
    X_tr_sel = selector.fit_transform(X_tr, y)
    X_te_sel = selector.transform(X_te)

    # Persist vectorizer and selector (optional)
    joblib.dump(tfidf, os.path.join(CACHE_DIR, "tfidf.joblib"))
    joblib.dump(selector, os.path.join(CACHE_DIR, "selector.joblib"))

    return X_tr_sel, X_te_sel

def embed_text(train_texts, test_texts):
    # Batch embedding with caching
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    if os.path.exists(EMB_TRAIN_CACHE) and os.path.exists(EMB_TEST_CACHE):
        logger.info("Loading cached embeddings")
        X_emb_tr = np.load(EMB_TRAIN_CACHE)
        X_emb_te = np.load(EMB_TEST_CACHE)
    else:
        logger.info("Generating embeddings (batch_size=64)")
        X_emb_tr = embedder.encode(
            train_texts.tolist(),
            batch_size=64,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        X_emb_te = embedder.encode(
            test_texts.tolist(),
            batch_size=64,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        np.save(EMB_TRAIN_CACHE, X_emb_tr)
        np.save(EMB_TEST_CACHE, X_emb_te)

    # Dimensionality reduction
    logger.info("Reducing embeddings from 768 to 128 via PCA")
    pca = PCA(n_components=128, random_state=42)
    X_pca_tr = pca.fit_transform(X_emb_tr)
    X_pca_te = pca.transform(X_emb_te)
    joblib.dump(pca, os.path.join(CACHE_DIR, "pca.joblib"))

    return X_pca_tr, X_pca_te

def run_cv_and_predict(X_tfidf, X_emb, y, X_tfidf_test, X_emb_test, test_ids):
    logger.info("Starting 5-fold Stratified CV")
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    num_classes = len(np.unique(y))
    test_pred_sum = np.zeros((X_tfidf_test.shape[0], num_classes))
    f1_scores = []
    loglosses = []

    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_tfidf, y), 1):
        logger.info(f"Fold {fold} training")
        X_tr_tfidf = X_tfidf[tr_idx]
        X_va_tfidf = X_tfidf[val_idx]
        X_tr_emb = X_emb[tr_idx]
        X_va_emb = X_emb[val_idx]
        y_tr = y[tr_idx]
        y_va = y[val_idx]

        # Combine features
        X_tr = hstack([X_tr_tfidf, csr_matrix(X_tr_emb)])
        X_va = hstack([X_va_tfidf, csr_matrix(X_va_emb)])
        X_te = hstack([X_tfidf_test, csr_matrix(X_emb_test)])

        # LightGBM dataset
        lgb_train = lgb.Dataset(X_tr, label=y_tr)
        lgb_val   = lgb.Dataset(X_va, label=y_va, reference=lgb_train)

        params = {
            "objective": "multiclass",
            "num_class": num_classes,
            "learning_rate": 0.1,
            "num_leaves": 31,
            "metric": "multi_logloss",
            "verbose": -1,
            "seed": 42
        }
        model = lgb.train(
            params,
            lgb_train,
            num_boost_round=1000,
            valid_sets=[lgb_train, lgb_val],
            early_stopping_rounds=50,
            verbose_eval=100
        )

        # Validation predictions
        val_proba = model.predict(X_va, num_iteration=model.best_iteration)
        val_pred  = np.argmax(val_proba, axis=1)
        f1 = f1_score(y_va, val_pred, average="macro")
        ll = log_loss(y_va, val_proba)
        f1_scores.append(f1)
        loglosses.append(ll)
        logger.info(f"Fold {fold} — f1_macro: {f1:.4f}, log_loss: {ll:.4f}")

        # Test predictions
        test_pred_sum += model.predict(X_te, num_iteration=model.best_iteration)

    # Average metrics & predictions
    avg_f1 = np.mean(f1_scores)
    avg_ll = np.mean(loglosses)
    logger.info(f"CV average f1_macro: {avg_f1:.4f}")
    logger.info(f"CV average log_loss: {avg_ll:.4f}")

    test_pred_avg = test_pred_sum / skf.n_splits
    return test_pred_avg

if __name__ == "__main__":
    try:
        # Load & preprocess
        train, test = load_and_clean()
        y = train["target"].values
        test_ids = test["id"].values

        # Features
        X_tfidf_tr, X_tfidf_te = vectorize_text(train["text"], test["text"])
        X_emb_tr,   X_emb_te   = embed_text(train["text"], test["text"])

        # CV training & predict
        proba = run_cv_and_predict(
            X_tfidf_tr, X_emb_tr, y,
            X_tfidf_te, X_emb_te,
            test_ids
        )

        # Prepare submission
        logger.info("Preparing submission file")
        submission = pd.DataFrame(proba, columns=[
            f"target_{i}" for i in range(proba.shape[1])
        ])
        submission.insert(0, "id", test_ids)
        submission.to_csv(OUTPUT_PATH, index=False)
        logger.info(f"Submission saved to {OUTPUT_PATH}")

    except Exception as e:
        logger.exception("An error occurred")
        raise
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_b2bfee36.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_5e96c93a.py ---
#!/usr/bin/env python3
# Complete Python script with batching, caching embeddings, PCA, TF-IDF + LightGBM ensemble, CV & probabilities

import os
import sys
import logging
import string
import joblib
import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, log_loss
from lightgbm import LGBMClassifier
from sentence_transformers import SentenceTransformer

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# File paths (must not change)
TRAIN_PATH = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv"
TEST_PATH = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv"
SAMPLE_SUB_PATH = "/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv"

# Output path
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
)

# Cache directory (alongside train.csv)
CACHE_DIR = os.path.join(os.path.dirname(TRAIN_PATH), "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

def clean_text(s: str) -> str:
    translator = str.maketrans("", "", string.punctuation)
    return s.lower().translate(translator)

def preprocess_and_cache():
    """
    Load data, clean, extract TF-IDF & embeddings, apply PCA, cache intermediate results.
    Returns:
        X_tfidf_all (sparse), X_tfidf_test (sparse),
        X_emb_all (ndarray), X_emb_test (ndarray),
        y (ndarray), test_ids (Series)
    """
    # Load
    train_df = pd.read_csv(TRAIN_PATH)
    test_df = pd.read_csv(TEST_PATH)
    if train_df.isna().Response.any():
        train_df.Response = train_df.Response.fillna("")
    if test_df.isna().Response.any():
        test_df.Response = test_df.Response.fillna("")

    # Combine Question + Response
    train_df["full_text"] = (train_df.Question.fillna("") + " " + train_df.Response).map(clean_text)
    test_df["full_text"]  = (test_df.Question.fillna("") + " " + test_df.Response).map(clean_text)

    y = train_df.target.values
    test_ids = test_df.id if "id" in test_df.columns else test_df.index

    # TF-IDF + SelectKBest
    tfidf_cache = os.path.join(CACHE_DIR, "tfidf_selector.joblib")
    if os.path.exists(tfidf_cache):
        logger.info("Loading cached TF-IDF vectorizer and selector...")
        tfidf, selector = joblib.load(tfidf_cache)
        X_tfidf_all = tfidf.transform(train_df.full_text)
        X_tfidf_test = tfidf.transform(test_df.full_text)
        X_tfidf_all = selector.transform(X_tfidf_all)
        X_tfidf_test = selector.transform(X_tfidf_test)
    else:
        logger.info("Fitting TF-IDF + SelectKBest...")
        tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.95, min_df=3)
        X_tfidf_all = tfidf.fit_transform(train_df.full_text)
        X_tfidf_test = tfidf.transform(test_df.full_text)
        selector = SelectKBest(chi2, k=5000)
        X_tfidf_all = selector.fit_transform(X_tfidf_all, y)
        X_tfidf_test = selector.transform(X_tfidf_test)
        joblib.dump((tfidf, selector), tfidf_cache)
        logger.info(f"Cached TF-IDF & selector to {tfidf_cache}")

    # Sentence embeddings + PCA
    emb_cache = os.path.join(CACHE_DIR, "embeddings.joblib")
    pca_cache = os.path.join(CACHE_DIR, "pca_128.joblib")
    if os.path.exists(emb_cache) and os.path.exists(pca_cache):
        logger.info("Loading cached embeddings and PCA...")
        data = joblib.load(emb_cache)
        X_emb_all = data["train"]
        X_emb_test = data["test"]
        pca = joblib.load(pca_cache)
        X_emb_all = pca.transform(X_emb_all)
        X_emb_test = pca.transform(X_emb_test)
    else:
        logger.info("Computing sentence embeddings in batches...")
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        X_emb_all = embedder.encode(
            train_df.full_text.tolist(),
            batch_size=64,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        X_emb_test = embedder.encode(
            test_df.full_text.tolist(),
            batch_size=64,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        logger.info("Applying PCA to reduce embeddings to 128 dims...")
        pca = PCA(n_components=128, random_state=42)
        X_emb_all = pca.fit_transform(X_emb_all)
        X_emb_test = pca.transform(X_emb_test)
        joblib.dump(pca, pca_cache)
        joblib.dump({"train": X_emb_all, "test": X_emb_test}, emb_cache)
        logger.info(f"Cached embeddings to {emb_cache} and PCA to {pca_cache}")

    return X_tfidf_all, X_tfidf_test, X_emb_all, X_emb_test, y, test_ids

def train_cv_and_predict(
    X_tfidf, X_tfidf_test, X_emb, X_emb_test, y, n_splits=5
):
    """
    Perform StratifiedKFold CV, train separate LGBM on TF-IDF and embeddings,
    average probabilities, and return test_proba.
    """
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    n_classes = len(np.unique(y))
    test_pred_proba = np.zeros((X_tfidf_test.shape[0], n_classes), dtype=float)

    fold = 0
    for train_idx, val_idx in skf.split(X_tfidf, y):
        fold += 1
        logger.info(f"Starting fold {fold}/{n_splits}...")
        X_tr_t = X_tfidf[train_idx]; X_val_t = X_tfidf[val_idx]
        X_tr_e = X_emb[train_idx];  X_val_e = X_emb[val_idx]
        y_tr = y[train_idx];        y_val = y[val_idx]

        # TF-IDF model
        model_t = LGBMClassifier(
            objective="multiclass",
            num_class=n_classes,
            learning_rate=0.1,
            n_estimators=1000,
            random_state=42,
            n_jobs=-1
        )
        model_t.fit(
            X_tr_t, y_tr,
            eval_set=[(X_val_t, y_val)],
            eval_metric="multi_logloss",
            early_stopping_rounds=50,
            verbose=False
        )
        proba_t_val = model_t.predict_proba(X_val_t)
        # Embedding model
        model_e = LGBMClassifier(
            objective="multiclass",
            num_class=n_classes,
            learning_rate=0.1,
            n_estimators=1000,
            random_state=42,
            n_jobs=-1
        )
        model_e.fit(
            X_tr_e, y_tr,
            eval_set=[(X_val_e, y_val)],
            eval_metric="multi_logloss",
            early_stopping_rounds=50,
            verbose=False
        )
        proba_e_val = model_e.predict_proba(X_val_e)

        # Ensemble validation
        proba_val = 0.5 * (proba_t_val + proba_e_val)
        val_preds = np.argmax(proba_val, axis=1)
        f1 = f1_score(y_val, val_preds, average="macro")
        ll = log_loss(y_val, proba_val)
        logger.info(f"Fold {fold} - f1_macro: {f1:.4f}, log_loss: {ll:.4f}")

        # Test predictions
        proba_t_test = model_t.predict_proba(X_tfidf_test)
        proba_e_test = model_e.predict_proba(X_emb_test)
        test_pred_proba += 0.5 * (proba_t_test + proba_e_test)

    # Average over folds
    test_pred_proba /= n_splits
    return test_pred_proba

def main():
    try:
        # Preprocess & load/cached features
        X_tfidf_all, X_tfidf_test, X_emb_all, X_emb_test, y, test_ids = preprocess_and_cache()

        # Train CV & predict
        logger.info("Training with cross-validation and predicting...")
        test_proba = train_cv_and_predict(
            X_tfidf_all, X_tfidf_test, X_emb_all, X_emb_test, y, n_splits=5
        )

        # Build submission
        classes = [f"target_{i}" for i in range(test_proba.shape[1])]
        submission = pd.DataFrame(test_proba, columns=classes)
        submission.insert(0, "id", test_ids.values)
        submission.to_csv(OUTPUT_PATH, index=False)
        logger.info(f"Submission saved to {OUTPUT_PATH}")

    except Exception as exc:
        logger.exception("An error occurred during processing.")
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_5e96c93a.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_061eab52.py ---
import os
import sys
import pandas as pd
import numpy as np
import string

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sentence_transformers import SentenceTransformer

def preprocess_data(file_paths: dict):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict with keys 'train', 'test', 'sample' pointing to CSV file paths.
    Returns:
        Tuple containing:
            X_tfidf_train (sparse matrix),
            X_tfidf_val (sparse matrix),
            X_embed_train (ndarray),
            X_embed_val (ndarray),
            y_train (ndarray),
            y_val (ndarray),
            X_tfidf_test (sparse matrix),
            X_embed_test (ndarray),
            test_ids (pd.Index)
    """
    # Validate input paths
    required_keys = ['train', 'test', 'sample']
    for key in required_keys:
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' in file_paths dict")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load datasets
    train_df = pd.read_csv(file_paths['train'])
    test_df = pd.read_csv(file_paths['test'])

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Response' not in df.columns:
            raise ValueError(f"'Response' column missing in {name} data")
        if 'Question' not in df.columns:
            raise ValueError(f"'Question' column missing in {name} data")

    # Data cleaning
    train_df = train_df.drop(columns=['Question'])
    test_df  = test_df.drop(columns=['Question'])
    train_df = train_df.dropna(subset=['Response'])
    test_df  = test_df.dropna(subset=['Response'])

    # Lowercase + remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(s: str) -> str:
        return s.lower().translate(translator)
    train_df['clean_resp'] = train_df['Response'].astype(str).map(clean_text)
    test_df['clean_resp']  = test_df['Response'].astype(str).map(clean_text)

    # TF-IDF vectorization (1,2)-grams
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=None)
    X_tfidf_all = tfidf.fit_transform(train_df['clean_resp'])
    X_tfidf_test = tfidf.transform(test_df['clean_resp'])

    # Feature selection: top 5000 by chi2
    y = train_df['target'].values
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_train_sel = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_test_sel  = selector.transform(X_tfidf_test)

    # Sentence embeddings via pretrained model
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    X_embed_all   = embedder.encode(train_df['clean_resp'].tolist(), show_progress_bar=False)
    X_embed_test  = embedder.encode(test_df['clean_resp'].tolist(), show_progress_bar=False)

    # Train/validation split (stratified)
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf_train_sel, X_embed_all, y,
        test_size=0.2, random_state=42, stratify=y
    )

    test_ids = test_df['id'] if 'id' in test_df.columns else test_df.index

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
        X_tfidf_test_sel,
        X_embed_test,
        test_ids
    )

if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }
        outputs = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_061eab52.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_6ee30c33.py ---
#!/usr/bin/env python3
# candidate_3.py
import os
import logging
import pandas as pd
import numpy as np
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, log_loss
from lightgbm import LGBMClassifier
from sentence_transformers import SentenceTransformer

# Absolute data paths (must not change)
TRAIN_PATH = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv'
TEST_PATH  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv'
SAMPLE_PATH= '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

def clean_text(text: str) -> str:
    import string
    translator = str.maketrans('', '', string.punctuation)
    return text.lower().translate(translator)

def load_and_preprocess():
    # Load
    train = pd.read_csv(TRAIN_PATH)
    test  = pd.read_csv(TEST_PATH)

    # Fill missing responses
    train['Response'] = train['Response'].fillna('')
    test['Response']  = test['Response'].fillna('')

    # Combine Question + Response
    train['text'] = (train['Question'].fillna('') + " " + train['Response']).map(clean_text)
    test['text']  = (test['Question'].fillna('') + " " + test['Response']).map(clean_text)

    y = train['target'].values
    ids = test['id'].values

    # TF-IDF
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=10000)
    X_tfidf_train = tfidf.fit_transform(train['text'])
    X_tfidf_test  = tfidf.transform(test['text'])
    logging.info(f"TF-IDF train shape: {X_tfidf_train.shape}, test shape: {X_tfidf_test.shape}")

    # Embeddings with batching & caching
    EMB_DIM = 768
    CACHE_TRAIN = 'train_emb.npy'
    CACHE_TEST  = 'test_emb.npy'
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    if os.path.exists(CACHE_TRAIN):
        X_emb_train = np.load(CACHE_TRAIN)
        logging.info("Loaded cached train embeddings")
    else:
        X_emb_train = embedder.encode(
            train['text'].tolist(),
            batch_size=64,
            convert_to_numpy=True,
            show_progress_bar=True
        )
        np.save(CACHE_TRAIN, X_emb_train)
        logging.info("Computed and cached train embeddings")
    if os.path.exists(CACHE_TEST):
        X_emb_test = np.load(CACHE_TEST)
        logging.info("Loaded cached test embeddings")
    else:
        X_emb_test = embedder.encode(
            test['text'].tolist(),
            batch_size=64,
            convert_to_numpy=True,
            show_progress_bar=True
        )
        np.save(CACHE_TEST, X_emb_test)
        logging.info("Computed and cached test embeddings")

    # Dimensionality reduction on embeddings
    PCA_DIM = 128
    pca = PCA(n_components=PCA_DIM, random_state=42)
    X_emb_train = pca.fit_transform(X_emb_train)
    X_emb_test  = pca.transform(X_emb_test)
    logging.info(f"PCA-reduced embeddings to {PCA_DIM} dims")

    return X_tfidf_train, X_tfidf_test, X_emb_train, X_emb_test, y, ids

def train_ensemble_cv(X_tfidf, X_emb, y, X_tfidf_test, X_emb_test, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    n_classes = len(np.unique(y))
    # accumulate test probabilities
    test_proba_sum = np.zeros((X_tfidf_test.shape[0], n_classes))
    fold = 0
    cv_f1 = []
    cv_ll = []

    for train_idx, val_idx in skf.split(X_tfidf, y):
        fold += 1
        logging.info(f"Starting fold {fold}")
        X_tfidf_tr, X_tfidf_val = X_tfidf[train_idx], X_tfidf[val_idx]
        X_emb_tr,   X_emb_val   = X_emb[train_idx],   X_emb[val_idx]
        y_tr,       y_val       = y[train_idx],       y[val_idx]

        # Model on TF-IDF
        m_tfidf = LGBMClassifier(
            objective='multiclass',
            num_class=n_classes,
            boosting_type='gbdt',
            random_state=42,
            n_jobs=-1,
            learning_rate=0.1,
            n_estimators=1000
        )
        m_tfidf.fit(
            X_tfidf_tr, y_tr,
            eval_set=[(X_tfidf_val, y_val)],
            early_stopping_rounds=50,
            verbose=False
        )

        # Model on Embeddings
        m_emb = LGBMClassifier(
            objective='multiclass',
            num_class=n_classes,
            boosting_type='gbdt',
            random_state=42,
            n_jobs=-1,
            learning_rate=0.1,
            n_estimators=1000
        )
        m_emb.fit(
            X_emb_tr, y_tr,
            eval_set=[(X_emb_val, y_val)],
            early_stopping_rounds=50,
            verbose=False
        )

        # Validation predictions (probabilities)
        proba_tfidf_val = m_tfidf.predict_proba(X_tfidf_val)
        proba_emb_val   = m_emb.predict_proba(X_emb_val)
        proba_val = 0.5 * (proba_tfidf_val + proba_emb_val)

        # Metrics
        val_preds = np.argmax(proba_val, axis=1)
        f1 = f1_score(y_val, val_preds, average='macro')
        ll = log_loss(y_val, proba_val)
        cv_f1.append(f1)
        cv_ll.append(ll)
        logging.info(f"Fold {fold} - F1_macro: {f1:.4f}, log_loss: {ll:.4f}")

        # Test predictions
        proba_tfidf_test = m_tfidf.predict_proba(X_tfidf_test)
        proba_emb_test   = m_emb.predict_proba(X_emb_test)
        proba_test = 0.5 * (proba_tfidf_test + proba_emb_test)
        test_proba_sum += proba_test

    # Average test probabilities
    test_proba_avg = test_proba_sum / n_splits
    logging.info(f"CV F1_macro mean/std: {np.mean(cv_f1):.4f} ± {np.std(cv_f1):.4f}")
    logging.info(f"CV log_loss mean/std: {np.mean(cv_ll):.4f} ± {np.std(cv_ll):.4f}")
    return test_proba_avg

def make_submission(proba, ids):
    n_classes = proba.shape[1]
    cols = {f'target_{i}': proba[:, i] for i in range(n_classes)}
    sub = pd.DataFrame({'id': ids, **cols})
    sub.to_csv(OUTPUT_PATH, index=False)
    logging.info(f"Submission saved to {OUTPUT_PATH}")

if __name__ == "__main__":
    try:
        X_tfidf_train, X_tfidf_test, X_emb_train, X_emb_test, y, ids = load_and_preprocess()
        test_proba = train_ensemble_cv(
            X_tfidf_train, X_emb_train, y,
            X_tfidf_test, X_emb_test,
            n_splits=5
        )
        make_submission(test_proba, ids)
    except Exception:
        logging.exception("An error occurred during execution")
        raise
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_6ee30c33.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_2aba2d31.py ---
#!/usr/bin/env python3
import os
import sys
import logging

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, log_loss

import lightgbm as lgb
from sentence_transformers import SentenceTransformer

# Absolute paths (must be preserved)
TRAIN_PATH  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv'
TEST_PATH   = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv'
SAMPLE_PATH = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
)

# Configuration
TFIDF_NGRAM_RANGE    = (1, 2)
TFIDF_K_FEATURES     = 5000
PCA_EMB_DIMS         = 128
EMB_BATCH_SIZE       = 64
VALIDATION_SIZE      = 0.2
RANDOM_STATE         = 42
EARLY_STOPPING_ROUNDS = 50
NUM_BOOST_ROUND      = 1000

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def load_and_clean():
    logging.info("Loading data")
    train = pd.read_csv(TRAIN_PATH)
    test  = pd.read_csv(TEST_PATH)

    # Validate columns
    for df, name in [(train, 'train'), (test, 'test')]:
        for col in ['Question', 'Response']:
            if col not in df.columns:
                raise ValueError(f"Missing column {col} in {name} data")

    # Fill missing responses with empty string
    train['Response'] = train['Response'].fillna("")
    test['Response']  = test['Response'].fillna("")

    # Combine Question + Response
    train['text'] = train['Question'].astype(str) + " " + train['Response'].astype(str)
    test['text']  = test['Question'].astype(str)  + " " + test['Response'].astype(str)

    return train, test

def preprocess_text(train, test):
    logging.info("Cleaning text (lowercase, remove punctuation)")
    import string
    translator = str.maketrans('', '', string.punctuation)
    def clean(s): return s.lower().translate(translator)

    train_text = train['text'].map(clean).tolist()
    test_text  = test['text'].map(clean).tolist()

    logging.info("Vectorizing with TF-IDF")
    tfidf = TfidfVectorizer(ngram_range=TFIDF_NGRAM_RANGE)
    X_tfidf_all = tfidf.fit_transform(train_text)
    X_tfidf_test = tfidf.transform(test_text)

    logging.info(f"Selecting top {TFIDF_K_FEATURES} TF-IDF features by chi2")
    selector = SelectKBest(chi2, k=min(TFIDF_K_FEATURES, X_tfidf_all.shape[1]))
    y = train['target'].values
    X_tfidf_sel = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_test_sel = selector.transform(X_tfidf_test)

    return X_tfidf_sel, X_tfidf_test_sel, tfidf, selector

def embed_and_reduce(train, test):
    logging.info("Encoding sentence embeddings in batches")
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    emb_train = embedder.encode(
        train['text'].tolist(),
        batch_size=EMB_BATCH_SIZE,
        show_progress_bar=True,
        convert_to_numpy=True
    )
    emb_test = embedder.encode(
        test['text'].tolist(),
        batch_size=EMB_BATCH_SIZE,
        show_progress_bar=True,
        convert_to_numpy=True
    )

    logging.info(f"Reducing embeddings to {PCA_EMB_DIMS} dims via PCA")
    pca = PCA(n_components=min(PCA_EMB_DIMS, emb_train.shape[1]), random_state=RANDOM_STATE)
    emb_train_red = pca.fit_transform(emb_train)
    emb_test_red  = pca.transform(emb_test)

    return emb_train_red, emb_test_red, pca

def train_lgb(X_tr, y_tr, X_val, y_val, feature_name):
    logging.info(f"Training LightGBM on {feature_name}")
    clf = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=len(np.unique(y_tr)),
        boosting_type='gbdt',
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    clf.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        eval_metric='multi_logloss',
        early_stopping_rounds=EARLY_STOPPING_ROUNDS,
        verbose=False
    )
    return clf

def main():
    try:
        setup_logging()
        train_df, test_df = load_and_clean()

        # Preprocess text features
        X_tfidf, X_tfidf_test, tfidf_vec, tfidf_sel = preprocess_text(train_df, test_df)
        X_emb, X_emb_test, pca = embed_and_reduce(train_df, test_df)

        y = train_df['target'].values
        # Hold-out split
        logging.info("Splitting data into train/validation")
        (X_tfidf_tr, X_tfidf_val,
         X_emb_tr,   X_emb_val,
         y_tr,       y_val) = train_test_split(
            X_tfidf, X_emb, y,
            test_size=VALIDATION_SIZE,
            random_state=RANDOM_STATE,
            stratify=y
        )

        # Train separate models
        lgb_tfidf = train_lgb(X_tfidf_tr, y_tr, X_tfidf_val, y_val, "TF-IDF")
        lgb_emb   = train_lgb(X_emb_tr,   y_tr, X_emb_val,   y_val, "Embeddings")

        # Validation metrics
        logging.info("Evaluating on validation set")
        proba_val_tfidf = lgb_tfidf.predict_proba(X_tfidf_val)
        proba_val_emb   = lgb_emb.predict_proba(X_emb_val)
        proba_val = 0.5 * proba_val_tfidf + 0.5 * proba_val_emb
        val_preds = np.argmax(proba_val, axis=1)
        val_f1 = f1_score(y_val, val_preds, average='macro')
        val_ll = log_loss(y_val, proba_val)
        logging.info(f"Validation f1_macro: {val_f1:.4f}, log_loss: {val_ll:.4f}")

        # Predict on test set
        logging.info("Predicting on test set")
        proba_test_tfidf = lgb_tfidf.predict_proba(X_tfidf_test)
        proba_test_emb   = lgb_emb.predict_proba(X_emb_test)
        proba_test = 0.5 * proba_test_tfidf + 0.5 * proba_test_emb

        # Prepare submission
        logging.info("Preparing submission")
        submission = pd.read_csv(SAMPLE_PATH)
        submission[['target_0','target_1','target_2',
                    'target_3','target_4','target_5','target_6']] = proba_test
        submission.to_csv(OUTPUT_PATH, index=False)
        logging.info(f"Submission saved to {OUTPUT_PATH}")

    except Exception as e:
        logging.exception("An error occurred")
        sys.exit(1)

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_2aba2d31.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_e6ce54f4.py ---
#!/usr/bin/env python3

import os
import sys
import string

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix, hstack

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import f1_score

import lightgbm as lgb
from sentence_transformers import SentenceTransformer


def preprocess_data(file_paths: dict):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict with keys 'train', 'test', 'sample' pointing to CSV file paths.
    Returns:
        Tuple containing:
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val,
            X_tfidf_test, X_emb_test,
            test_ids
    """
    # Validate input paths
    required_keys = ['train', 'test', 'sample']
    for key in required_keys:
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' in file_paths dict")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load datasets
    train_df = pd.read_csv(file_paths['train'])
    test_df = pd.read_csv(file_paths['test'])

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Response' not in df.columns:
            raise ValueError(f"'Response' column missing in {name} data")
        if 'Question' not in df.columns:
            raise ValueError(f"'Question' column missing in {name} data")
    if 'target' not in train_df.columns:
        raise ValueError("'target' column missing in train data")

    # Drop unused columns & NaNs
    train_df = train_df.drop(columns=['Question']).dropna(subset=['Response'])
    test_df  = test_df.drop(columns=['Question']).dropna(subset=['Response'])

    # Lowercase + remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(s: str) -> str:
        return s.lower().translate(translator)

    train_df['clean_resp'] = train_df['Response'].astype(str).map(clean_text)
    test_df['clean_resp']  = test_df['Response'].astype(str).map(clean_text)

    # TF-IDF vectorization (1,2)-grams
    tfidf = TfidfVectorizer(ngram_range=(1, 2))
    X_tfidf_all  = tfidf.fit_transform(train_df['clean_resp'])
    X_tfidf_test = tfidf.transform(test_df['clean_resp'])

    # Feature selection: top 5000 by chi2
    y = train_df['target'].values
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_sel_all  = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_sel_test = selector.transform(X_tfidf_test)

    # Sentence embeddings
    embedder     = SentenceTransformer('all-MiniLM-L6-v2')
    X_embed_all  = embedder.encode(train_df['clean_resp'].tolist(),
                                   show_progress_bar=False)
    X_embed_test = embedder.encode(test_df['clean_resp'].tolist(),
                                   show_progress_bar=False)

    # Train/validation split (stratified)
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf_sel_all, X_embed_all, y,
        test_size=0.2, random_state=42, stratify=y
    )

    # Test IDs
    test_ids = test_df['id'] if 'id' in test_df.columns else test_df.index

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
        X_tfidf_sel_test,
        X_embed_test,
        test_ids
    )


def train_and_evaluate(
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
    y_train, y_val,
    X_tfidf_test, X_emb_test
):
    """
    Train a LightGBM classifier on combined TF-IDF and embeddings,
    evaluate on validation set, and return test predictions.
    """
    # Combine sparse TF-IDF and dense embeddings
    X_train = hstack([X_tfidf_tr, csr_matrix(X_emb_tr)])
    X_val   = hstack([X_tfidf_val, csr_matrix(X_emb_val)])
    X_test  = hstack([X_tfidf_test, csr_matrix(X_emb_test)])

    # Determine number of classes
    num_classes = len(np.unique(y_train))

    # Initialize & train model
    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=num_classes,
        boosting_type='gbdt',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)

    # Validation
    val_preds = model.predict(X_val)
    val_f1    = f1_score(y_val, val_preds, average='macro')
    print(f"Validation Macro F1 Score: {val_f1:.4f}")

    # Test predictions
    test_preds = model.predict(X_test)
    return test_preds


if __name__ == "__main__":
    try:
        file_paths = {
            'train':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test':   '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }

        (X_tfidf_tr,
         X_tfidf_val,
         X_emb_tr,
         X_emb_val,
         y_train,
         y_val,
         X_tfidf_test,
         X_emb_test,
         test_ids) = preprocess_data(file_paths)

        preds_test = train_and_evaluate(
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val,
            X_tfidf_test, X_emb_test
        )

        submission = pd.DataFrame({
            'id':     test_ids,
            'target': preds_test
        })

        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
        )
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_e6ce54f4.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_6b4db79a.py ---
#!/usr/bin/env python3
import os
import logging
import string
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, log_loss
import lightgbm as lgb
from sentence_transformers import SentenceTransformer

def main():
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # File paths (must remain as given)
    train_path = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv'
    test_path  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv'
    sample_path = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'

    # Load data
    logging.info("Loading datasets...")
    train_df = pd.read_csv(train_path)
    test_df  = pd.read_csv(test_path)

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Question' not in df.columns or 'Response' not in df.columns:
            logging.error(f"'{name}' file must contain 'Question' and 'Response' columns")
            return
    if 'target' not in train_df.columns:
        logging.error("'train' file must contain 'target' column")
        return

    # Fill missing responses with empty strings
    train_df['Response'].fillna('', inplace=True)
    test_df['Response'].fillna('', inplace=True)

    # Clean and combine Question + Response
    logging.info("Cleaning text (lowercasing, removing punctuation)...")
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(q, r):
        txt = f"{q} {r}".lower().translate(translator)
        return txt

    train_df['text'] = train_df.apply(
        lambda row: clean_text(row['Question'], row['Response']), axis=1
    )
    test_df['text'] = test_df.apply(
        lambda row: clean_text(row['Question'], row['Response']), axis=1
    )

    # TF-IDF vectorization
    logging.info("Vectorizing with TF-IDF...")
    tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9)
    X_tfidf = tfidf.fit_transform(train_df['text'])
    X_tfidf_test = tfidf.transform(test_df['text'])
    y = train_df['target'].values

    # Chi2 feature selection to top 5000 features (or fewer if vocabulary smaller)
    k = min(5000, X_tfidf.shape[1])
    logging.info(f"Selecting top {k} TF-IDF features by chi2...")
    selector = SelectKBest(chi2, k=k)
    X_tfidf = selector.fit_transform(X_tfidf, y)
    X_tfidf_test = selector.transform(X_tfidf_test)

    # Sentence embeddings in batches
    logging.info("Computing sentence embeddings (batch size=64)...")
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    X_emb = embedder.encode(
        train_df['text'].tolist(),
        batch_size=64,
        show_progress_bar=True,
        convert_to_numpy=True
    )
    X_emb_test = embedder.encode(
        test_df['text'].tolist(),
        batch_size=64,
        show_progress_bar=True,
        convert_to_numpy=True
    )

    # Dimensionality reduction of embeddings via PCA
    logging.info("Reducing embedding dimensionality with PCA (n_components=128)...")
    pca = PCA(n_components=128, random_state=42)
    X_emb = pca.fit_transform(X_emb)
    X_emb_test = pca.transform(X_emb_test)

    # Stratified train/validation split
    logging.info("Splitting data into train/validation sets...")
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf, X_emb, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )

    num_classes = len(np.unique(y_train))
    logging.info(f"Number of classes: {num_classes}")

    # Initialize LightGBM models for TF-IDF and embeddings
    clf_tfidf = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=num_classes,
        random_state=42,
        n_jobs=-1
    )
    clf_emb = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=num_classes,
        random_state=42,
        n_jobs=-1
    )

    # Train with early stopping
    logging.info("Training TF-IDF based classifier with early stopping...")
    clf_tfidf.fit(
        X_tfidf_tr, y_train,
        eval_set=[(X_tfidf_val, y_val)],
        eval_metric='multi_logloss',
        early_stopping_rounds=50,
        verbose=False
    )

    logging.info("Training embedding based classifier with early stopping...")
    clf_emb.fit(
        X_emb_tr, y_train,
        eval_set=[(X_emb_val, y_val)],
        eval_metric='multi_logloss',
        early_stopping_rounds=50,
        verbose=False
    )

    # Validation predictions and metrics
    logging.info("Evaluating on validation set...")
    proba_tfidf_val = clf_tfidf.predict_proba(X_tfidf_val)
    proba_emb_val   = clf_emb.predict_proba(X_emb_val)
    proba_val = (proba_tfidf_val + proba_emb_val) / 2.0
    preds_val = np.argmax(proba_val, axis=1)

    val_f1 = f1_score(y_val, preds_val, average='macro')
    val_ll = log_loss(y_val, proba_val)
    logging.info(f"Validation Macro F1 Score: {val_f1:.4f}")
    logging.info(f"Validation Log Loss: {val_ll:.4f}")

    # Test set predictions (probabilities)
    logging.info("Predicting probabilities on test set...")
    proba_tfidf_test = clf_tfidf.predict_proba(X_tfidf_test)
    proba_emb_test   = clf_emb.predict_proba(X_emb_test)
    proba_test = (proba_tfidf_test + proba_emb_test) / 2.0

    # Build submission DataFrame
    logging.info("Building submission DataFrame...")
    submission = pd.DataFrame(
        proba_test,
        columns=[f"target_{i}" for i in range(num_classes)]
    )
    submission.insert(0, 'id', test_df['id'])

    # Output path
    output_path = (
        "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
        "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
    )
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # Save submission
    submission.to_csv(output_path, index=False)
    logging.info(f"Submission saved to {output_path}")

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_6b4db79a.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_af02bd75.py ---
#!/usr/bin/env python3

import os
import logging
import joblib

import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import f1_score, log_loss, accuracy_score

import lightgbm as lgb
from sentence_transformers import SentenceTransformer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

def load_and_clean():
    # Absolute paths to data
    train_path = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv'
    test_path  = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv'

    logging.info("Loading datasets")
    train = pd.read_csv(train_path)
    test  = pd.read_csv(test_path)

    # Drop missing responses, fill with empty string if any
    train = train.dropna(subset=['Response']).reset_index(drop=True)
    test  = test.dropna(subset=['Response']).reset_index(drop=True)
    train['Question'] = train['Question'].fillna('')
    test['Question']  = test['Question'].fillna('')

    # Combine question and response
    train['text'] = train['Question'].astype(str) + ' ' + train['Response'].astype(str)
    test['text']  = test['Question'].astype(str)  + ' ' + test['Response'].astype(str)

    return train, test

def clean_texts(texts):
    import string
    translator = str.maketrans('', '', string.punctuation)
    return [t.lower().translate(translator) for t in texts]

def compute_embeddings(texts, cache_path, model_name='all-MiniLM-L6-v2', batch_size=64):
    if os.path.exists(cache_path):
        logging.info(f"Loading embeddings from cache: {cache_path}")
        return joblib.load(cache_path)
    logging.info(f"Computing embeddings, saving to {cache_path}")
    embedder = SentenceTransformer(model_name)
    emb = embedder.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=True,
        convert_to_numpy=True
    )
    joblib.dump(emb, cache_path)
    return emb

def main():
    # Load and clean data
    train, test = load_and_clean()
    y = train['target'].values
    test_ids = test.get('id', pd.Series(range(len(test))))

    # Cleaned text
    train_texts = clean_texts(train['text'].tolist())
    test_texts  = clean_texts(test['text'].tolist())

    # TF-IDF vectorization with (1,2)-grams, limit to top 5000 features
    logging.info("Vectorizing text with TF-IDF")
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=5000)
    X_tfidf_train = tfidf.fit_transform(train_texts)
    X_tfidf_test  = tfidf.transform(test_texts)

    # Select top features by chi2
    logging.info("Selecting top 5000 TF-IDF features by chi2")
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_train = selector.fit_transform(X_tfidf_train, y).toarray()
    X_tfidf_test  = selector.transform(X_tfidf_test).toarray()

    # Embeddings for Response (only)
    cache_dir = '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms'
    train_emb_cache = os.path.join(cache_dir, 'train_resp_emb.pkl')
    test_emb_cache  = os.path.join(cache_dir, 'test_resp_emb.pkl')

    logging.info("Computing sentence embeddings for 'Response'")
    X_emb_train = compute_embeddings(
        train['Response'].astype(str).tolist(),
        train_emb_cache
    )
    X_emb_test  = compute_embeddings(
        test['Response'].astype(str).tolist(),
        test_emb_cache
    )

    # Reduce embedding dimension to 128 via PCA
    logging.info("Applying PCA to reduce embeddings to 128 dims")
    pca = PCA(n_components=128, random_state=42)
    X_emb_train = pca.fit_transform(X_emb_train)
    X_emb_test  = pca.transform(X_emb_test)

    # Prepare for Stratified K-Fold
    n_splits = 5
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    n_classes = len(np.unique(y))
    oof_preds = np.zeros((train.shape[0], n_classes))
    test_preds = np.zeros((test.shape[0], n_classes))

    # LightGBM parameters
    lgb_params = {
        'objective': 'multiclass',
        'num_class': n_classes,
        'learning_rate': 0.05,
        'num_leaves': 31,
        'metric': 'multi_logloss',
        'verbosity': -1,
        'seed': 42
    }

    # Cross-validation
    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_tfidf_train, y), 1):
        logging.info(f"Starting fold {fold}/{n_splits}")
        X_tr = np.hstack([X_tfidf_train[tr_idx], X_emb_train[tr_idx]])
        X_val = np.hstack([X_tfidf_train[val_idx], X_emb_train[val_idx]])
        y_tr, y_val = y[tr_idx], y[val_idx]

        dtrain = lgb.Dataset(X_tr, label=y_tr)
        dval   = lgb.Dataset(X_val, label=y_val, reference=dtrain)

        model = lgb.train(
            lgb_params,
            dtrain,
            num_boost_round=1000,
            valid_sets=[dtrain, dval],
            valid_names=['train','valid'],
            early_stopping_rounds=50,
            verbose_eval=100
        )

        # Predict validation
        oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)
        # Predict test
        test_preds += model.predict(
            np.hstack([X_tfidf_test, X_emb_test]),
            num_iteration=model.best_iteration
        ) / n_splits

    # Evaluation on out-of-fold
    oof_labels = np.argmax(oof_preds, axis=1)
    acc = accuracy_score(y, oof_labels)
    f1 = f1_score(y, oof_labels, average='macro')
    ll = log_loss(y, oof_preds)
    logging.info(f"OOF Accuracy: {acc:.4f}, F1_macro: {f1:.4f}, LogLoss: {ll:.4f}")

    # Prepare submission
    logging.info("Preparing submission file")
    submission = pd.DataFrame(test_preds, columns=[f"target_{i}" for i in range(n_classes)])
    submission.insert(0, 'id', test_ids.values)
    output_path = (
        "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
        "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
    )
    submission.to_csv(output_path, index=False)
    logging.info(f"Submission saved to {output_path}")

if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_af02bd75.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_5b5ead59.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved LLM response classifier with TF-IDF + PCA-reduced embeddings,
early stopping, predict_proba for submission, and structured logging.
"""

import os
import string
import logging
import joblib

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, log_loss
from sentence_transformers import SentenceTransformer
import lightgbm as lgb

# ----------------------------
# Configuration
# ----------------------------
TRAIN_PATH = (
    "/home/shiinehata/Desktop/iSE/iSE_AutoML/"
    "model_eval/datasets/predict_the_llms/train.csv"
)
TEST_PATH = (
    "/home/shiinehata/Desktop/iSE/iSE_AutoML/"
    "model_eval/datasets/predict_the_llms/test.csv"
)
SAMPLE_SUB_PATH = (
    "/home/shiinehata/Desktop/iSE/iSE_AutoML/"
    "model_eval/datasets/predict_the_llms/sample_submission.csv"
)
OUTPUT_PATH = (
    "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
    "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
)

RANDOM_STATE = 42
TEST_SIZE = 0.2

NGRAM_RANGE = (1, 2)
TFIDF_MAX_FEATURES = 20000
CHI2_K = 5000

EMBED_MODEL_NAME = "all-MiniLM-L6-v2"
EMBED_BATCH_SIZE = 64
PCA_N_COMPONENTS = 128

LGB_PARAMS = {
    "objective": "multiclass",
    "num_class": 7,
    "boosting_type": "gbdt",
    "learning_rate": 0.1,
    "num_leaves": 31,
    "max_depth": -1,
    "random_state": RANDOM_STATE,
    "n_jobs": -1,
    "verbosity": -1,
}
NUM_BOOST_ROUNDS = 1000
EARLY_STOPPING_ROUNDS = 50

# ----------------------------
# Setup logging
# ----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def clean_text(texts):
    """Lowercase and remove punctuation."""
    translator = str.maketrans("", "", string.punctuation)
    return [t.lower().translate(translator) for t in texts]


def batch_embed(texts, model, batch_size=EMBED_BATCH_SIZE):
    """Compute embeddings in batches to reduce peak memory usage."""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i : i + batch_size]
        emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)
        embeddings.append(emb)
    return np.vstack(embeddings)


def load_and_preprocess():
    """Load data, preprocess text, compute TF-IDF, chi2 selection, embeddings + PCA."""
    logger.info("Loading data")
    train_df = pd.read_csv(TRAIN_PATH)
    test_df = pd.read_csv(TEST_PATH)
    sub_df = pd.read_csv(SAMPLE_SUB_PATH)

    # Drop missing responses
    train_df = train_df.dropna(subset=["Response"])
    test_df = test_df.dropna(subset=["Response"])

    # Combine Question + Response for richer features
    logger.info("Combining question and response")
    train_texts = (
        train_df["Question"].astype(str) + " " + train_df["Response"].astype(str)
    ).tolist()
    test_texts = (
        test_df["Question"].astype(str) + " " + test_df["Response"].astype(str)
    ).tolist()

    # Clean text
    logger.info("Cleaning text")
    train_clean = clean_text(train_texts)
    test_clean = clean_text(test_texts)

    # TF-IDF vectorization
    logger.info("Fitting TF-IDF")
    tfidf = TfidfVectorizer(
        max_features=TFIDF_MAX_FEATURES, ngram_range=NGRAM_RANGE
    )
    X_tfidf_train_all = tfidf.fit_transform(train_clean)
    X_tfidf_test = tfidf.transform(test_clean)

    # Chi2 feature selection
    logger.info(f"Selecting top {CHI2_K} TF-IDF features by chi2")
    y_all = train_df["target"].values
    selector = SelectKBest(chi2, k=CHI2_K)
    X_tfidf_train_sel = selector.fit_transform(X_tfidf_train_all, y_all)
    X_tfidf_test_sel = selector.transform(X_tfidf_test)

    # Sentence embeddings + PCA
    logger.info(f"Loading embedding model: {EMBED_MODEL_NAME}")
    embedder = SentenceTransformer(EMBED_MODEL_NAME)
    logger.info("Computing embeddings for training set")
    X_emb_train_all = batch_embed(train_clean, embedder)
    logger.info("Computing embeddings for test set")
    X_emb_test_all = batch_embed(test_clean, embedder)

    logger.info(f"Reducing embeddings to {PCA_N_COMPONENTS} dimensions with PCA")
    pca = PCA(n_components=PCA_N_COMPONENTS, random_state=RANDOM_STATE)
    X_emb_train = pca.fit_transform(X_emb_train_all)
    X_emb_test = pca.transform(X_emb_test_all)

    # Train/validation split
    logger.info(f"Splitting train/val with test_size={TEST_SIZE}")
    (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
    ) = train_test_split(
        X_tfidf_train_sel,
        X_emb_train,
        y_all,
        test_size=TEST_SIZE,
        stratify=y_all,
        random_state=RANDOM_STATE,
    )

    # Save objects for reproducibility
    joblib.dump(tfidf, "tfidf_vectorizer.joblib")
    joblib.dump(selector, "chi2_selector.joblib")
    joblib.dump(pca, "pca_reducer.joblib")

    test_ids = test_df["id"] if "id" in test_df.columns else test_df.index.values

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_tfidf_test_sel,
        X_emb_tr,
        X_emb_val,
        X_emb_test,
        y_train,
        y_val,
        test_ids,
    )


def train_and_evaluate_model(
    X_tfidf_tr,
    X_tfidf_val,
    X_tfidf_test,
    X_emb_tr,
    X_emb_val,
    X_emb_test,
    y_train,
    y_val,
):
    """Train LightGBM with TF-IDF + embeddings, early stopping, and evaluate."""
    logger.info("Stacking TF-IDF and embedding features")
    from scipy.sparse import hstack, csr_matrix

    X_train = hstack([X_tfidf_tr, csr_matrix(X_emb_tr)])
    X_val = hstack([X_tfidf_val, csr_matrix(X_emb_val)])
    X_test = hstack([X_tfidf_test, csr_matrix(X_emb_test)])

    # Prepare datasets for LightGBM
    lgb_train = lgb.Dataset(X_train, label=y_train)
    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)

    logger.info("Training LightGBM with early stopping")
    evals_result = {}
    model = lgb.train(
        LGB_PARAMS,
        lgb_train,
        num_boost_round=NUM_BOOST_ROUNDS,
        valid_sets=[lgb_train, lgb_val],
        valid_names=["train", "valid"],
        early_stopping_rounds=EARLY_STOPPING_ROUNDS,
        evals_result=evals_result,
        verbose_eval=100,
    )

    # Validation metrics
    logger.info("Evaluating on validation set")
    val_proba = model.predict(X_val, num_iteration=model.best_iteration)
    val_preds = np.argmax(val_proba, axis=1)
    f1 = f1_score(y_val, val_preds, average="macro")
    ll = log_loss(y_val, val_proba)
    logger.info(f"Validation F1 (macro): {f1:.4f}")
    logger.info(f"Validation Log Loss: {ll:.4f}")

    # Test predictions
    logger.info("Predicting probabilities on test set")
    proba_test = model.predict(X_test, num_iteration=model.best_iteration)

    # Save model
    joblib.dump(model, "lgbm_model.joblib")
    return proba_test


def main():
    try:
        (
            X_tfidf_tr,
            X_tfidf_val,
            X_tfidf_test,
            X_emb_tr,
            X_emb_val,
            X_emb_test,
            y_train,
            y_val,
            test_ids,
        ) = load_and_preprocess()

        proba_test = train_and_evaluate_model(
            X_tfidf_tr,
            X_tfidf_val,
            X_tfidf_test,
            X_emb_tr,
            X_emb_val,
            X_emb_test,
            y_train,
            y_val,
        )

        # Prepare submission
        logger.info("Preparing submission DataFrame")
        cols = [f"target_{i}" for i in range(proba_test.shape[1])]
        submission = pd.DataFrame(proba_test, columns=cols)
        submission.insert(0, "id", test_ids)

        logger.info(f"Saving submission to {OUTPUT_PATH}")
        submission.to_csv(OUTPUT_PATH, index=False)
        logger.info("Done")
    except Exception as e:
        logger.exception("An error occurred")
        raise


if __name__ == "__main__":
    main()
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/temp_exec_5b5ead59.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/states/final_assembled_code.py ---
#!/usr/bin/env python3

import os
import sys
import string

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix, hstack

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import f1_score

import lightgbm as lgb
from sentence_transformers import SentenceTransformer


def preprocess_data(file_paths: dict):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict with keys 'train', 'test', 'sample' pointing to CSV file paths.
    Returns:
        Tuple containing:
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val,
            X_tfidf_test, X_emb_test,
            test_ids
    """
    # Validate input paths
    required_keys = ['train', 'test', 'sample']
    for key in required_keys:
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' in file_paths dict")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load datasets
    train_df = pd.read_csv(file_paths['train'])
    test_df = pd.read_csv(file_paths['test'])

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Response' not in df.columns:
            raise ValueError(f"'Response' column missing in {name} data")
        if 'Question' not in df.columns:
            raise ValueError(f"'Question' column missing in {name} data")
    if 'target' not in train_df.columns:
        raise ValueError("'target' column missing in train data")

    # Drop unused columns & NaNs
    train_df = train_df.drop(columns=['Question']).dropna(subset=['Response'])
    test_df  = test_df.drop(columns=['Question']).dropna(subset=['Response'])

    # Lowercase + remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(s: str) -> str:
        return s.lower().translate(translator)

    train_df['clean_resp'] = train_df['Response'].astype(str).map(clean_text)
    test_df['clean_resp']  = test_df['Response'].astype(str).map(clean_text)

    # TF-IDF vectorization (1,2)-grams
    tfidf = TfidfVectorizer(ngram_range=(1, 2))
    X_tfidf_all  = tfidf.fit_transform(train_df['clean_resp'])
    X_tfidf_test = tfidf.transform(test_df['clean_resp'])

    # Feature selection: top 5000 by chi2
    y = train_df['target'].values
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_sel_all  = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_sel_test = selector.transform(X_tfidf_test)

    # Sentence embeddings
    embedder     = SentenceTransformer('all-MiniLM-L6-v2')
    X_embed_all  = embedder.encode(train_df['clean_resp'].tolist(),
                                   show_progress_bar=False)
    X_embed_test = embedder.encode(test_df['clean_resp'].tolist(),
                                   show_progress_bar=False)

    # Train/validation split (stratified)
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf_sel_all, X_embed_all, y,
        test_size=0.2, random_state=42, stratify=y
    )

    # Test IDs
    test_ids = test_df['id'] if 'id' in test_df.columns else test_df.index

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
        X_tfidf_sel_test,
        X_embed_test,
        test_ids
    )


def train_and_evaluate(
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
    y_train, y_val,
    X_tfidf_test, X_emb_test
):
    """
    Train a LightGBM classifier on combined TF-IDF and embeddings,
    evaluate on validation set, and return test predictions.
    """
    # Combine sparse TF-IDF and dense embeddings
    X_train = hstack([X_tfidf_tr, csr_matrix(X_emb_tr)])
    X_val   = hstack([X_tfidf_val, csr_matrix(X_emb_val)])
    X_test  = hstack([X_tfidf_test, csr_matrix(X_emb_test)])

    # Determine number of classes
    num_classes = len(np.unique(y_train))

    # Initialize & train model
    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=num_classes,
        boosting_type='gbdt',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)

    # Validation
    val_preds = model.predict(X_val)
    val_f1    = f1_score(y_val, val_preds, average='macro')
    print(f"Validation Macro F1 Score: {val_f1:.4f}")

    # Test predictions
    test_preds = model.predict(X_test)
    return test_preds


if __name__ == "__main__":
    try:
        file_paths = {
            'train':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test':   '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }

        (X_tfidf_tr,
         X_tfidf_val,
         X_emb_tr,
         X_emb_val,
         y_train,
         y_val,
         X_tfidf_test,
         X_emb_test,
         test_ids) = preprocess_data(file_paths)

        preds_test = train_and_evaluate(
            X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val,
            y_train, y_val,
            X_tfidf_test, X_emb_test
        )

        submission = pd.DataFrame({
            'id':     test_ids,
            'target': preds_test
        })

        output_path = (
            "/home/shiinehata/Desktop/UET/iSE/iSE_AutoML/"
            "RREFACTORED/runs/run_20250807_124631_3fc49b38/submission.csv"
        )
        submission.to_csv(output_path, index=False)
        print(f"Submission saved to {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/states/final_assembled_code.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/states/preprocessing_code_response.py ---
import os
import sys
import pandas as pd
import numpy as np
import string

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from sentence_transformers import SentenceTransformer

def preprocess_data(file_paths: dict):
    """
    Preprocess data according to guidelines.
    Args:
        file_paths: dict with keys 'train', 'test', 'sample' pointing to CSV file paths.
    Returns:
        Tuple containing:
            X_tfidf_train (sparse matrix),
            X_tfidf_val (sparse matrix),
            X_embed_train (ndarray),
            X_embed_val (ndarray),
            y_train (ndarray),
            y_val (ndarray),
            X_tfidf_test (sparse matrix),
            X_embed_test (ndarray),
            test_ids (pd.Index)
    """
    # Validate input paths
    required_keys = ['train', 'test', 'sample']
    for key in required_keys:
        if key not in file_paths:
            raise KeyError(f"Missing '{key}' in file_paths dict")
        if not os.path.isfile(file_paths[key]):
            raise FileNotFoundError(f"File not found: {file_paths[key]}")

    # Load datasets
    train_df = pd.read_csv(file_paths['train'])
    test_df = pd.read_csv(file_paths['test'])

    # Basic validation
    for df, name in [(train_df, 'train'), (test_df, 'test')]:
        if 'Response' not in df.columns:
            raise ValueError(f"'Response' column missing in {name} data")
        if 'Question' not in df.columns:
            raise ValueError(f"'Question' column missing in {name} data")

    # Data cleaning
    train_df = train_df.drop(columns=['Question'])
    test_df  = test_df.drop(columns=['Question'])
    train_df = train_df.dropna(subset=['Response'])
    test_df  = test_df.dropna(subset=['Response'])

    # Lowercase + remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    def clean_text(s: str) -> str:
        return s.lower().translate(translator)
    train_df['clean_resp'] = train_df['Response'].astype(str).map(clean_text)
    test_df['clean_resp']  = test_df['Response'].astype(str).map(clean_text)

    # TF-IDF vectorization (1,2)-grams
    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=None)
    X_tfidf_all = tfidf.fit_transform(train_df['clean_resp'])
    X_tfidf_test = tfidf.transform(test_df['clean_resp'])

    # Feature selection: top 5000 by chi2
    y = train_df['target'].values
    selector = SelectKBest(chi2, k=5000)
    X_tfidf_train_sel = selector.fit_transform(X_tfidf_all, y)
    X_tfidf_test_sel  = selector.transform(X_tfidf_test)

    # Sentence embeddings via pretrained model
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    X_embed_all   = embedder.encode(train_df['clean_resp'].tolist(), show_progress_bar=False)
    X_embed_test  = embedder.encode(test_df['clean_resp'].tolist(), show_progress_bar=False)

    # Train/validation split (stratified)
    X_tfidf_tr, X_tfidf_val, X_emb_tr, X_emb_val, y_train, y_val = train_test_split(
        X_tfidf_train_sel, X_embed_all, y,
        test_size=0.2, random_state=42, stratify=y
    )

    test_ids = test_df['id'] if 'id' in test_df.columns else test_df.index

    return (
        X_tfidf_tr,
        X_tfidf_val,
        X_emb_tr,
        X_emb_val,
        y_train,
        y_val,
        X_tfidf_test_sel,
        X_embed_test,
        test_ids
    )

if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }
        outputs = preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/states/preprocessing_code_response.py ---

--- START FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/states/modeling_code_response.py ---
import sys
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import f1_score

# Assume preprocess_data is available in the execution environment
# from preprocessing_module import preprocess_data

def train_and_predict(X_train, y_train, X_test):
    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=7,
        boosting_type='gbdt',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return preds

if __name__ == "__main__":
    try:
        file_paths = {
            'train': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/train.csv',
            'test':  '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/test.csv',
            'sample': '/home/shiinehata/Desktop/iSE/iSE_AutoML/model_eval/datasets/predict_the_llms/sample_submission.csv'
        }

        (X_tfidf_tr,
         X_tfidf_val,
         X_emb_tr,
         X_emb_val,
         y_train,
         y_val,
         X_tfidf_test,
         X_emb_test,
         test_ids) = preprocess_data(file_paths)

        preds = train_and_predict(X_tfidf_tr, y_train, X_tfidf_test)

        submission = pd.DataFrame({
            'id': test_ids,
            'target': preds
        })
        submission.to_csv("submission.csv", index=False)
        print("Model training and submission generation completed successfully.")

    except Exception as e:
        print(f"An error occurred during modeling: {e}", file=sys.stderr)
        sys.exit(1)
--- END FILE: runs/run_20250807_124631_3fc49b38/generation_iter_0/states/modeling_code_response.py ---

--- START FILE: src/iML/main_runner.py ---
import uuid
from datetime import datetime
from pathlib import Path
import logging

from omegaconf import OmegaConf

logger = logging.getLogger(__name__)

from .core.manager import Manager
from .utils.rich_logging import configure_logging

def run_automl_pipeline(input_data_folder: str, output_folder: str = None, config: str = "configs/default.yaml"):
    """
    Main function to set up the environment and run the entire pipeline.
    """
    # 1. Create the output directory if one is not provided
    if output_folder is None:
        project_root = Path(__file__).parent.parent.parent # Points to the project root
        working_dir = project_root / "runs"
        current_datetime = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_uuid = uuid.uuid4().hex[:8]
        folder_name = f"run_{current_datetime}_{random_uuid}"
        output_path = working_dir / folder_name

    # Ensure output_path is a Path object and the directory exists
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 2. Load the configuration file
    config = OmegaConf.load(config)

    # 3. Configure the logging system to save to the output directory
    configure_logging(output_dir=output_dir, verbosity=config.verbosity)
    
    logger.info(f"Project running. Output will be saved to: {output_dir.resolve()}")
    logger.info(f"Loaded configuration from: {config}")

    # 4. Initialize the Manager with the prepared settings
    manager = Manager(
        input_data_folder=input_data_folder,
        output_folder=str(output_dir),  # Pass as string for consistency
        config=config,
    )

    # 5. Start the pipeline run
    manager.run_pipeline()

    manager.report_token_usage()
    logger.brief(f"output saved in {output_dir}.")
    manager.cleanup()
--- END FILE: src/iML/main_runner.py ---

--- START FILE: src/iML/__init__.py ---

--- END FILE: src/iML/__init__.py ---

--- START FILE: src/iML/llm/bedrock_chat.py ---
import logging
import os
from typing import Any, Dict, List

import boto3
from langchain_aws import ChatBedrock

from .base_chat import BaseAssistantChat

logger = logging.getLogger(__name__)


class AssistantChatBedrock(ChatBedrock, BaseAssistantChat):
    """Bedrock chat model with LangGraph support."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.initialize_conversation(self)

    def describe(self) -> Dict[str, Any]:
        base_desc = super().describe()
        return {**base_desc, "model": self.model_id}


def get_bedrock_models() -> List[str]:
    if "AWS_DEFAULT_REGION" not in os.environ:
        os.environ["AWS_DEFAULT_REGION"] = "us-west-2"
        logger.info("AWS_DEFAULT_REGION not found is os.environ. Set to default value us-west-2.")

    try:
        bedrock = boto3.client("bedrock")
        response = bedrock.list_foundation_models()
        return [model["modelId"] for model in response["modelSummaries"]]
    except Exception as e:
        logger.error(f"Error fetching Bedrock models: {e}")
        return []


def create_bedrock_chat(config, session_name: str) -> AssistantChatBedrock:
    """Create a Bedrock chat model instance."""
    model = config.model

    logger.info(f"Using Bedrock model: {model} for session: {session_name}")
    if "AWS_DEFAULT_REGION" not in os.environ:
        raise ValueError("AWS_DEFAULT_REGION key not found in environment")

    return AssistantChatBedrock(
        model_id=model,
        model_kwargs={
            "temperature": config.temperature,
            "max_tokens": config.max_tokens,
        },
        region_name=os.environ["AWS_DEFAULT_REGION"],
        verbose=config.verbose,
        session_name=session_name,
    )

--- END FILE: src/iML/llm/bedrock_chat.py ---

--- START FILE: src/iML/llm/azure_openai_chat.py ---
import logging
import os
from typing import Any, Dict, List

from langchain_openai import AzureChatOpenAI
from openai import AzureOpenAI

from .base_chat import BaseAssistantChat

logger = logging.getLogger(__name__)


class AssistantAzureChatOpenAI(AzureChatOpenAI, BaseAssistantChat):
    """Azure OpenAI chat model with LangGraph support."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.initialize_conversation(self)

    def describe(self) -> Dict[str, Any]:
        base_desc = super().describe()
        return {**base_desc, "model": self.model_name}


def get_azure_models() -> List[str]:
    try:
        client = AzureOpenAI()
        models = client.models.list()
        return [model.id for model in models if model.id.startswith(("gpt-3.5", "gpt-4", "o1", "o3"))]
    except Exception as e:
        print(f"Error fetching Azure models: {e}")
        return []


def create_azure_openai_chat(config, session_name: str) -> AssistantAzureChatOpenAI:
    """Create an Azure OpenAI chat model instance."""
    model = config.model

    logger.info(f"Using Azure OpenAI model: {model} for session: {session_name}")
    if "AZURE_OPENAI_API_KEY" not in os.environ:
        raise ValueError("Azure OpenAI API key not found in environment")
    if "OPENAI_API_VERSION" not in os.environ:
        raise Exception("Azure API env variable OPENAI_API_VERSION not set")
    if "AZURE_OPENAI_ENDPOINT" not in os.environ:
        raise Exception("Azure API env variable AZURE_OPENAI_ENDPOINT not set")

    kwargs = {
        "model_name": model,
        "openai_api_key": os.environ["AZURE_OPENAI_API_KEY"],
        "api_version": os.environ["OPENAI_API_VERSION"],
        "azure_endpoint": os.environ["AZURE_OPENAI_ENDPOINT"],
        "session_name": session_name,
        "max_tokens": config.max_tokens,
    }

    if hasattr(config, "temperature"):
        kwargs["temperature"] = config.temperature

    if hasattr(config, "verbose"):
        kwargs["verbose"] = config.verbose

    return AssistantAzureChatOpenAI(**kwargs)

--- END FILE: src/iML/llm/azure_openai_chat.py ---

--- START FILE: src/iML/llm/openai_chat.py ---
import logging
import os
from typing import Any, Dict, List

from langchain_openai import ChatOpenAI
from openai import OpenAI

from .base_chat import BaseAssistantChat

logger = logging.getLogger(__name__)


class AssistantChatOpenAI(ChatOpenAI, BaseAssistantChat):
    """OpenAI chat model with LangGraph support."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.initialize_conversation(self)

    def describe(self) -> Dict[str, Any]:
        base_desc = super().describe()
        return {**base_desc, "model": self.model_name, "proxy": self.openai_proxy}


def get_openai_models() -> List[str]:
    try:
        client = OpenAI()
        models = client.models.list()
        return [model.id for model in models]
    except Exception as e:
        logger.error(f"Error fetching OpenAI models: {e}")
        return []


def create_openai_chat(config, session_name: str) -> AssistantChatOpenAI:
    """Create an OpenAI chat model instance."""
    model = config.model

    if "OPENAI_API_KEY" not in os.environ:
        raise ValueError("OpenAI API key not found in environment")

    logger.info(f"Using OpenAI model: {model} for session: {session_name}")
    kwargs = {
        "model_name": model,
        "openai_api_key": os.environ["OPENAI_API_KEY"],
        "session_name": session_name,
        "max_tokens": config.max_tokens,
    }

    if hasattr(config, "temperature"):
        kwargs["temperature"] = config.temperature

    if hasattr(config, "verbose"):
        kwargs["verbose"] = config.verbose

    if hasattr(config, "proxy_url"):
        kwargs["openai_api_base"] = config.proxy_url

    return AssistantChatOpenAI(**kwargs)

--- END FILE: src/iML/llm/openai_chat.py ---

--- START FILE: src/iML/llm/llm_factory.py ---
import logging
import os
from typing import Any, Dict, Optional, Union

from omegaconf import DictConfig

from .anthropic_chat import AssistantChatAnthropic, create_anthropic_chat, get_anthropic_models
from .azure_openai_chat import AssistantAzureChatOpenAI, create_azure_openai_chat, get_azure_models
from .base_chat import GlobalTokenTracker
from .bedrock_chat import AssistantChatBedrock, create_bedrock_chat, get_bedrock_models
from .openai_chat import AssistantChatOpenAI, create_openai_chat, get_openai_models

logger = logging.getLogger(__name__)


class ChatLLMFactory:
    """Factory class for creating chat models with LangGraph support."""

    @staticmethod
    def get_total_token_usage(save_path: Optional[str] = None) -> Dict[str, Any]:
        """Get total token usage across all conversations and sessions."""
        return GlobalTokenTracker().get_total_usage(save_path)

    @classmethod
    def get_valid_models(cls, provider):
        if provider == "azure":
            return get_azure_models()
        elif provider == "openai":
            return get_openai_models()
        elif provider == "bedrock":
            return get_bedrock_models()
        elif provider == "anthropic":
            return get_anthropic_models()
        else:
            raise ValueError(f"Unsupported provider: {provider}")

    @classmethod
    def get_valid_providers(cls):
        return ["azure", "openai", "bedrock", "anthropic"]

    @classmethod
    def get_chat_model(
        cls, config: DictConfig, session_name: str
    ) -> Union[AssistantChatOpenAI, AssistantAzureChatOpenAI, AssistantChatBedrock, AssistantChatAnthropic]:
        """Get a configured chat model instance using LangGraph patterns."""
        provider = config.provider
        model = config.model

        valid_providers = cls.get_valid_providers()
        if provider not in valid_providers:
            raise ValueError(f"Invalid provider: {provider}. Must be one of {valid_providers}")

        valid_models = cls.get_valid_models(provider)
        if model not in valid_models:
            if model[3:] not in valid_models:  # TODO: better logic for cross region inference
                raise ValueError(
                    f"Invalid model: {model} for provider {provider}. All valid models are {valid_models}. If you are using Bedrock, please check if the requested model is available in the provided AWS_DEFAULT_REGION: {os.environ.get('AWS_DEFAULT_REGION')}"
                )

        if provider == "openai":
            return create_openai_chat(config, session_name)
        elif provider == "azure":
            return create_azure_openai_chat(config, session_name)
        elif provider == "anthropic":
            return create_anthropic_chat(config, session_name)
        elif provider == "bedrock":
            return create_bedrock_chat(config, session_name)
        else:
            raise ValueError(f"Unsupported provider: {provider}")

--- END FILE: src/iML/llm/llm_factory.py ---

--- START FILE: src/iML/llm/__init__.py ---
from .llm_factory import ChatLLMFactory

--- END FILE: src/iML/llm/__init__.py ---

--- START FILE: src/iML/llm/base_chat.py ---
import json
import logging
import os
import uuid
from typing import Any, Dict, List, Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from pydantic import BaseModel, ConfigDict, Field
from tenacity import retry, stop_after_attempt, wait_exponential

logger = logging.getLogger(__name__)


def log_retry_attempt(retry_state):
    """Custom callback to log each retry attempt"""
    if retry_state.outcome.failed:
        exception = retry_state.outcome.exception()
        attempt_number = retry_state.attempt_number
        logger.error(f"Attempt {attempt_number} failed: {type(exception).__name__}: {exception}")


class GlobalTokenTracker:
    """Singleton class to track token usage across all conversations."""

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(GlobalTokenTracker, cls).__new__(cls)
            cls._instance.total_input_tokens = 0
            cls._instance.total_output_tokens = 0
            cls._instance.conversations = {}  # Track per-conversation usage
            cls._instance.sessions = {}  # Track per-session usage
        return cls._instance

    def add_tokens(
        self,
        conversation_id: str,
        session_name: str,
        input_tokens: int,
        output_tokens: int,
    ):
        """Add token counts for a specific conversation and session."""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens

        # Track conversation-level usage
        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = {
                "input_tokens": 0,
                "output_tokens": 0,
            }

        self.conversations[conversation_id]["input_tokens"] += input_tokens
        self.conversations[conversation_id]["output_tokens"] += output_tokens

        # Track session-level usage
        if session_name not in self.sessions:
            self.sessions[session_name] = {"input_tokens": 0, "output_tokens": 0}

        self.sessions[session_name]["input_tokens"] += input_tokens
        self.sessions[session_name]["output_tokens"] += output_tokens

    def get_conversation_usage(self, conversation_id: str) -> Dict[str, Any]:
        """Get token usage for a specific conversation."""
        if conversation_id not in self.conversations:
            return {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
            }

        conv_usage = self.conversations[conversation_id]
        return {
            "input_tokens": conv_usage["input_tokens"],
            "output_tokens": conv_usage["output_tokens"],
            "total_conversation_tokens": conv_usage["input_tokens"] + conv_usage["output_tokens"],
        }

    def get_total_usage(self, save_path: Optional[str] = None) -> Dict[str, Any]:
        """Get total token usage across all conversations and sessions."""
        usage_data = {
            "total": {
                "total_input_tokens": self.total_input_tokens,
                "total_output_tokens": self.total_output_tokens,
                "total_tokens": self.total_input_tokens + self.total_output_tokens,
            },
            "conversations": {},
            "sessions": {},
        }

        # Add conversation-level usage
        for conv_id, conv_usage in self.conversations.items():
            usage_data["conversations"][conv_id] = {
                "input_tokens": conv_usage["input_tokens"],
                "output_tokens": conv_usage["output_tokens"],
                "total_tokens": conv_usage["input_tokens"] + conv_usage["output_tokens"],
            }

        # Add session-level usage
        for session_name, session_usage in self.sessions.items():
            usage_data["sessions"][session_name] = {
                "input_tokens": session_usage["input_tokens"],
                "output_tokens": session_usage["output_tokens"],
                "total_tokens": session_usage["input_tokens"] + session_usage["output_tokens"],
            }

        # Save to file if path is provided
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, "w") as f:
                json.dump(usage_data, f, indent=2)

        return usage_data


class BaseAssistantChat(BaseModel):
    """Base class for assistant chat models with LangGraph support."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    history_: List[Dict[str, Any]] = Field(default_factory=list)
    input_tokens_: int = Field(default=0)
    output_tokens_: int = Field(default=0)
    graph: Optional[Any] = Field(default=None, exclude=True)
    app: Optional[Any] = Field(default=None, exclude=True)
    memory: Optional[Any] = Field(default=None, exclude=True)
    token_tracker: GlobalTokenTracker = Field(default_factory=GlobalTokenTracker)
    conversation_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    session_name: str = Field(default="default_session")

    def initialize_conversation(
        self,
        llm: Any,
        system_prompt: str = "You are a technical assistant that excels at working on data science tasks.",
    ) -> None:
        """Initialize conversation using LangGraph."""
        prompt_template = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=system_prompt),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )

        graph = StateGraph(state_schema=MessagesState)

        def call_model(state: MessagesState):
            prompt_messages = prompt_template.invoke(state)
            response = llm.invoke(prompt_messages)
            return {"messages": [response]}

        graph.add_edge(START, "model")
        graph.add_node("model", call_model)

        memory = MemorySaver()
        app = graph.compile(checkpointer=memory)

        self.graph = graph
        self.app = app
        self.memory = memory

    def describe(self) -> Dict[str, Any]:
        """Get model description and conversation history."""
        conversation_usage = self.token_tracker.get_conversation_usage(self.conversation_id)
        total_usage = self.token_tracker.get_total_usage()

        return {
            "history": self.history_,
            "conversation_tokens": conversation_usage,
            "total_tokens_across_all_conversations": total_usage,
            "session_name": self.session_name,
        }

    @retry(stop=stop_after_attempt(6), wait=wait_exponential(multiplier=32, min=32, max=128), after=log_retry_attempt)
    def assistant_chat(self, message: str) -> str:
        """Send a message and get response using LangGraph."""
        if not self.app:
            raise RuntimeError("Conversation not initialized. Call initialize_conversation first.")

        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        input_messages = [HumanMessage(content=message)]
        response = self.app.invoke({"messages": input_messages}, config)

        ai_message = response["messages"][-1]
        input_tokens = output_tokens = 0

        if hasattr(ai_message, "usage_metadata"):
            usage = ai_message.usage_metadata
            input_tokens = usage.get("input_tokens", 0)
            output_tokens = usage.get("output_tokens", 0)

            # Update both instance and global tracking
            self.input_tokens_ += input_tokens
            self.output_tokens_ += output_tokens
            self.token_tracker.add_tokens(self.conversation_id, self.session_name, input_tokens, output_tokens)

        self.history_.append(
            {
                "input": message,
                "output": ai_message.content,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
            }
        )

        return ai_message.content

    async def astream(self, message: str):
        """Stream responses using LangGraph."""
        if not self.app:
            raise RuntimeError("Conversation not initialized. Call initialize_conversation first.")

        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        input_messages = [HumanMessage(content=message)]

        async for chunk, metadata in self.app.stream({"messages": input_messages}, config, stream_mode="messages"):
            if isinstance(chunk, AIMessage):
                yield chunk.content

--- END FILE: src/iML/llm/base_chat.py ---

--- START FILE: src/iML/llm/anthropic_chat.py ---
import logging
import os
from typing import Any, Dict, List

from anthropic import Anthropic
from langchain_anthropic import ChatAnthropic

from .base_chat import BaseAssistantChat

logger = logging.getLogger(__name__)


class AssistantChatAnthropic(ChatAnthropic, BaseAssistantChat):
    """Anthropic chat model with LangGraph support."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.initialize_conversation(self)

    def describe(self) -> Dict[str, Any]:
        base_desc = super().describe()
        return {**base_desc, "model": self.model}


def get_anthropic_models() -> List[str]:
    """Get available Anthropic models."""
    try:
        client = Anthropic()
        # List available models
        models = client.models.list()
        print(models)
        return [model.id for model in models.data]
    except Exception as e:
        logger.warning(f"Failed to fetch Anthropic models: {e}")
        # Fallback to hardcoded list
        return []


def create_anthropic_chat(config, session_name: str) -> AssistantChatAnthropic:
    """Create an Anthropic chat model instance."""
    model = config.model

    if "ANTHROPIC_API_KEY" not in os.environ:
        raise ValueError("Anthropic API key not found in environment")

    logger.info(f"Using Anthropic model: {model} for session: {session_name}")
    kwargs = {
        "model": model,
        "anthropic_api_key": os.environ["ANTHROPIC_API_KEY"],
        "session_name": session_name,
        "max_tokens": config.max_tokens,
    }

    if hasattr(config, "temperature"):
        kwargs["temperature"] = config.temperature

    if hasattr(config, "verbose"):
        kwargs["verbose"] = config.verbose

    # Support for additional Anthropic-specific features
    if hasattr(config, "thinking") and hasattr(config.thinking, "enabled"):
        kwargs["thinking"] = config.thinking

    return AssistantChatAnthropic(**kwargs)

--- END FILE: src/iML/llm/anthropic_chat.py ---

--- START FILE: src/iML/core/manager.py ---
import logging
import os
import uuid
import subprocess
from pathlib import Path
from typing import List

from ..agents import (
    DescriptionAnalyzerAgent,
    ProfilingAgent,
    GuidelineAgent,
    PreprocessingCoderAgent,
    ModelingCoderAgent,
    AssemblerAgent,
    FeedbackAgent,
    CandidateGeneratorAgent,
    CandidateSelectorAgent,
)
from ..llm import ChatLLMFactory

# Basic configuration
logging.basicConfig(level=logging.INFO)

# Create a logger
logger = logging.getLogger(__name__)


class Manager:
    def __init__(
        self,
        input_data_folder: str,
        output_folder: str,
        config: str,
    ):
        """Initialize Manager with required paths and config from YAML file.

        Args:
            input_data_folder: Path to input data directory
            output_folder: Path to output directory
            config_path: Path to YAML configuration file
        """
        self.time_step = -1

        # Store required paths
        self.input_data_folder = input_data_folder
        self.output_folder = output_folder

        # Validate paths
        for path, name in [(input_data_folder, "input_data_folder")]:
            if not Path(path).exists():
                raise FileNotFoundError(f"{name} not found: {path}")

        # Create output folder if it doesn't exist
        Path(output_folder).mkdir(parents=True, exist_ok=True)

        self.config = config

        self.description_analyzer_agent = DescriptionAnalyzerAgent(
            config=config,
            manager=self,
            llm_config=self.config.description_analyzer,
        )
        self.profiling_agent = ProfilingAgent(
            config=config,
            manager=self,
        )
        self.guideline_agent = GuidelineAgent(
            config=config,
            manager=self,
            llm_config=self.config.guideline_generator,
        )
        self.preprocessing_coder_agent = PreprocessingCoderAgent(
            config=config,
            manager=self,
            llm_config=self.config.preprocessing_coder,
        )
        self.modeling_coder_agent = ModelingCoderAgent(
            config=config,
            manager=self,
            llm_config=self.config.modeling_coder,
        )
        self.assembler_agent = AssemblerAgent(
            config=config,
            manager=self,
            llm_config=self.config.assembler,
        )
        self.feedback_agent = FeedbackAgent(
            config=config,
            manager=self,
            llm_config=self.config.feedback,
        )
        self.candidate_generator_agent = CandidateGeneratorAgent(
            config=config,
            manager=self,
            llm_config=self.config.candidate_generator,
        )
        self.candidate_selector_agent = CandidateSelectorAgent(
            config=config,
            manager=self,
            llm_config=self.config.candidate_selector,
        )

        # Initialize prompts
        self.generate_initial_prompts()

        self.user_inputs: List[str] = []
        self.error_messages: List[str] = []
        self.error_prompts: List[str] = []
        self.python_codes: List[str] = []
        self.python_file_paths: List[str] = []
        self.bash_scripts: List[str] = []
        self.tutorial_retrievals: List[str] = []
        self.tutorial_prompts: List[str] = []

    def run_pipeline(self):
        """Run the entire pipeline from description analysis to code generation."""
        self.time_step = 0

        # Step 1: Run description analysis agent
        analysis_result = self.description_analyzer_agent()
        if "error" in analysis_result:
            logger.error(f"Description analysis failed: {analysis_result['error']}")
            return
        logger.info(f"Analysis result: {analysis_result}")

        self.description_analysis = analysis_result

        # Step 2: Run profiling agent
        profiling_result = self.profiling_agent()
        if "error" in profiling_result:
            logger.error(f"Data profiling failed: {profiling_result['error']}")
            return
        
        self.profiling_result = profiling_result
        logger.info("Profiling overview generated.")

        # Step 3: Run guideline agent
        guideline = self.guideline_agent()
        if "error" in guideline:
            logger.error(f"Guideline generation failed: {guideline['error']}")
            return
        
        self.guideline = guideline
        logger.info("Guideline generated successfully.")

        # Step 4: Run Preprocessing Coder Agent
        preprocessing_code_result = self.preprocessing_coder_agent()
        if preprocessing_code_result.get("status") == "failed":
            logger.error(f"Preprocessing code generation failed: {preprocessing_code_result.get('error')}")
            return

        self.preprocessing_code = preprocessing_code_result.get("code")
        logger.info("Preprocessing code generated and validated successfully.")

        # Step 5: Run Modeling Coder Agent
        modeling_code_result = self.modeling_coder_agent()
        if modeling_code_result.get("status") == "failed":
            logger.error(f"Modeling code generation failed: {modeling_code_result.get('error')}")
            return
            
        self.modeling_code = modeling_code_result.get("code")
        logger.info("Modeling code generated successfully (not yet validated).")

        # Step 6: Run Assembler Agent to assemble, finalize, and run the code
        assembler_result = self.assembler_agent()
        if assembler_result.get("status") == "failed":
            logger.error(f"Final code assembly and execution failed: {assembler_result.get('error')}")
            return
        
        self.assembled_code = assembler_result.get("code")
        logger.info(f"Initial script generated and executed successfully.")
        
        # --- SELF-IMPROVEMENT LOOP ---
        logger.info("Starting self-improvement loop...")

        # Step 7: Run Feedback Agent
        feedback_result = self.feedback_agent()
        if feedback_result.get("status") == "failed":
            logger.error(f"Feedback generation failed: {feedback_result.get('error')}")
            # This is not a fatal error, we can still proceed with the original code
            logger.warning("Proceeding without code improvement.")
        else:
            self.feedback = feedback_result.get("feedback")
            logger.info("Feedback for improvement generated successfully.")

            # Step 8: Run Candidate Generator Agent
            candidate_result = self.candidate_generator_agent()
            if candidate_result.get("status") == "failed" or not candidate_result.get("candidates"):
                logger.error(f"Candidate generation failed: {candidate_result.get('error')}")
                logger.warning("Proceeding without code improvement.")
            else:
                self.candidates = candidate_result.get("candidates")
                logger.info(f"Generated {len(self.candidates)} valid candidates.")

                # Step 9: Run Candidate Selector Agent
                selector_result = self.candidate_selector_agent()
                if selector_result.get("status") == "failed":
                    logger.error(f"Candidate selection failed: {selector_result.get('error')}")
                    logger.warning("Proceeding without code improvement.")
                else:
                    self.selected_code = selector_result.get("selected_code")
                    logger.info("Best candidate selected. Preparing for final execution.")

                    # Step 10: Final Execution of the selected code
                    logger.info("Executing the improved final script...")
                    final_execution_result = self.execute_code(self.selected_code)
                    if final_execution_result["success"]:
                         logger.info(f"Improved script executed successfully.")
                    else:
                         logger.error(f"Execution of improved script failed. Error: {final_execution_result['stderr']}")
                         logger.warning("The improved script failed. The result from the initial script is still available.")


        logger.info("AutoML pipeline completed successfully!")

    def generate_initial_prompts(self):

        # TODO: remove the hard code for "create_venv" (add in tool registry if need installation)
        asds =1

    @property
    def user_input(self) -> str:
        assert self.time_step >= 0, "No user input because the prompt generator is not stepped yet."
        assert len(self.user_inputs) == self.time_step + 1, "user input is not updated yet"
        return self.user_inputs[self.time_step]

    @property
    def python_code(self) -> str:
        assert self.time_step >= 0, "No python code because the prompt generator is not stepped yet."
        assert len(self.python_codes) == self.time_step + 1, "python code is not updated yet"
        return self.python_codes[self.time_step]

    @property
    def python_file_path(self) -> str:
        assert self.time_step >= 0, "No python file path because the prompt generator is not stepped yet."
        assert len(self.python_file_paths) == self.time_step + 1, "python file path is not updated yet"
        return self.python_file_paths[self.time_step]

    @property
    def previous_python_code(self) -> str:
        if self.time_step >= 1:
            return self.python_codes[self.time_step - 1]
        else:
            return ""

    @property
    def bash_script(self) -> str:
        assert self.time_step >= 0, "No bash script because the prompt generator is not stepped yet."
        assert len(self.bash_scripts) == self.time_step + 1, "bash script is not updated yet"
        return self.bash_scripts[self.time_step]

    @property
    def previous_bash_script(self) -> str:
        if self.time_step >= 1:
            return self.bash_scripts[self.time_step - 1]
        else:
            return ""

    @property
    def error_message(self) -> str:
        assert self.time_step >= 0, "No error message because the prompt generator is not stepped yet."
        assert len(self.error_messages) == self.time_step + 1, "error message is not updated yet"
        return self.error_messages[self.time_step]

    @property
    def previous_error_message(self) -> str:
        if self.time_step >= 1:
            return self.error_messages[self.time_step - 1]
        else:
            return ""

    @property
    def error_prompt(self) -> str:
        assert self.time_step >= 0, "No error prompt because the prompt generator is not stepped yet."
        assert len(self.error_prompts) == self.time_step + 1, "error prompt is not updated yet"
        return self.error_prompts[self.time_step]

    @property
    def previous_error_prompt(self) -> str:
        if self.time_step >= 1:
            return self.error_prompts[self.time_step - 1]
        else:
            return ""

    @property
    def all_previous_error_prompts(self) -> str:
        if self.time_step >= 1:
            return "\n\n".join(self.error_prompts[: self.time_step])
        else:
            return ""

    @property
    def tutorial_prompt(self) -> str:
        assert self.time_step >= 0, "No tutorial prompt because the prompt generator is not stepped yet."
        assert len(self.tutorial_prompts) == self.time_step + 1, "tutorial prompt is not updated yet"
        return self.tutorial_prompts[self.time_step]

    @property
    def previous_tutorial_prompt(self) -> str:
        if self.time_step >= 1:
            return self.tutorial_prompts[self.time_step - 1]
        else:
            return ""

    @property
    def tutorial_retrieval(self) -> str:
        assert self.time_step >= 0, "No tutorial retrieval because the prompt generator is not stepped yet."
        assert len(self.tutorial_retrievals) == self.time_step + 1, "tutorial retrieval is not updated yet"
        return self.tutorial_retrievals[self.time_step]

    @property
    def previous_tutorial_retrieval(self) -> str:
        if self.time_step >= 1:
            return self.tutorial_retrievals[self.time_step - 1]
        else:
            return ""

    @property
    def iteration_folder(self) -> str:
        if self.time_step >= 0:
            iter_folder = os.path.join(self.output_folder, f"generation_iter_{self.time_step}")
        else:
            iter_folder = os.path.join(self.output_folder, "initialization")
        os.makedirs(iter_folder, exist_ok=True)
        return iter_folder

    def set_initial_user_input(self, need_user_input, initial_user_input):
        self.need_user_input = need_user_input
        self.initial_user_input = initial_user_input

    def step(self):
        """Step the prompt generator forward."""
        self.time_step += 1

        user_input = self.initial_user_input
        # Get per iter user inputs if needed
        if self.need_user_input:
            if self.time_step > 0:
                logger.brief(
                    f"[bold green]Previous iteration info is stored in:[/bold green] {os.path.join(self.output_folder, f'iteration_{self.time_step - 1}')}"
                )
            else:
                logger.brief(
                    f"[bold green]Initialization info is stored in:[/bold green] {os.path.join(self.output_folder, 'initialization')}"
                )
            if user_input is None:
                user_input = ""

        assert len(self.user_inputs) == self.time_step
        self.user_inputs.append(user_input)

        if self.time_step > 0:
            previous_error_prompt = self.error_analyzer()

            assert len(self.error_prompts) == self.time_step - 1
            self.error_prompts.append(previous_error_prompt)

        retrieved_tutorials = self.retriever()
        assert len(self.tutorial_retrievals) == self.time_step
        self.tutorial_retrievals.append(retrieved_tutorials)

        tutorial_prompt = self.reranker()
        assert len(self.tutorial_prompts) == self.time_step
        self.tutorial_prompts.append(tutorial_prompt)

    def write_code_script(self, script, output_code_file):
        with open(output_code_file, "w") as file:
            file.write(script)

    def execute_code(self, code_to_execute: str) -> dict:
        """
        Executes a string of Python code in a subprocess.

        Args:
            code_to_execute: The Python code to run.

        Returns:
            A dictionary with execution status, stdout, and stderr.
        """
        # Create a temporary file to write the code to
        # Use a unique name for each execution to avoid conflicts
        temp_script_name = f"temp_exec_{uuid.uuid4().hex[:8]}.py"
        temp_script_path = os.path.join(self.iteration_folder, temp_script_name)
        
        self.write_code_script(code_to_execute, temp_script_path)

        logger.info(f"Executing code from temporary file: {temp_script_path}")

        try:
            # Execute the script using subprocess
            # The script should be executed from a directory where it can access the data
            # Assuming the data paths in the script are relative to the input folder's parent
            working_dir = str(Path(self.input_data_folder).parent)
            
            process = subprocess.run(
                ["python", temp_script_path],
                capture_output=True,
                text=True,
                check=False,  # Do not raise exception on non-zero exit code
                cwd=working_dir
            )

            stdout = process.stdout
            stderr = process.stderr

            self.save_and_log_states(stdout, f"exec_stdout_{temp_script_name}.txt", per_iteration=True)
            self.save_and_log_states(stderr, f"exec_stderr_{temp_script_name}.txt", per_iteration=True)

            if process.returncode == 0:
                logger.info("Code executed successfully.")
                return {"success": True, "stdout": stdout, "stderr": stderr}
            else:
                logger.error(f"Code execution failed with return code {process.returncode}.")
                # Combine stdout and stderr for a complete error context
                full_error = f"STDOUT:\n{stdout}\n\nSTDERR:\n{stderr}"
                return {"success": False, "stdout": stdout, "stderr": full_error}

        except Exception as e:
            logger.error(f"An exception occurred during code execution: {e}")
            return {"success": False, "stdout": "", "stderr": str(e)}


    def update_python_code(self):
        """Update the current Python code."""
        assert len(self.python_codes) == self.time_step
        assert len(self.python_file_paths) == self.time_step

        python_code = self.python_coder()

        python_file_path = os.path.join(self.iteration_folder, "generated_code.py")

        self.write_code_script(python_code, python_file_path)

        self.python_codes.append(python_code)
        self.python_file_paths.append(python_file_path)

    def update_bash_script(self):
        """Update the current bash script."""
        assert len(self.bash_scripts) == self.time_step

        bash_script = self.bash_coder()

        bash_file_path = os.path.join(self.iteration_folder, "execution_script.sh")

        self.write_code_script(bash_script, bash_file_path)

        self.bash_scripts.append(bash_script)

    def execute_code_old(self):
        planner_decision, planner_error_summary, planner_prompt, stderr, stdout = self.executer(
            code_to_execute=self.bash_script,
            code_to_analyze=self.python_code,
            task_description=self.task_description,
            data_prompt=self.data_prompt,
        )

        self.save_and_log_states(stderr, "stderr", per_iteration=True, add_uuid=False)
        self.save_and_log_states(stdout, "stdout", per_iteration=True, add_uuid=False)

        if planner_decision == "FIX":
            logger.brief(f"[bold red]Code generation failed in iteration[/bold red] {self.time_step}!")
            # Add suggestions to the error message to guide next iteration
            error_message = f"stderr: {stderr}\n\n" if stderr else ""
            error_message += (
                f"Error summary from planner (the error can appear in stdout if it's catched): {planner_error_summary}"
            )
            self.update_error_message(error_message=error_message)
            return False
        elif planner_decision == "FINISH":
            logger.brief(
                f"[bold green]Code generation successful after[/bold green] {self.time_step + 1} [bold green]iterations[/bold green]"
            )
            self.update_error_message(error_message="")
            return True
        else:
            logger.warning(f"###INVALID Planner Output: {planner_decision}###")
            self.update_error_message(error_message="")
            return False

    def update_error_message(self, error_message: str):
        """Update the current error message."""
        assert len(self.error_messages) == self.time_step
        self.error_messages.append(error_message)

    def save_and_log_states(self, content, save_name, per_iteration=False, add_uuid=False):
        if add_uuid:
            # Split filename and extension
            name, ext = os.path.splitext(save_name)
            # Generate 4-digit UUID (using first 4 characters of hex)
            uuid_suffix = str(uuid.uuid4()).replace("-", "")[:4]
            save_name = f"{name}_{uuid_suffix}{ext}"

        if per_iteration:
            states_dir = os.path.join(self.iteration_folder, "states")
        else:
            states_dir = os.path.join(self.output_folder, "states")
        os.makedirs(states_dir, exist_ok=True)
        output_file = os.path.join(states_dir, save_name)

        logger.info(f"Saving {output_file}...")
        with open(output_file, "w") as file:
            if content is not None:
                if isinstance(content, list):
                    # Join list elements with newlines
                    file.write("\n".join(str(item) for item in content))
                else:
                    # Handle as string (original behavior)
                    file.write(content)
            else:
                file.write("<None>")

    def log_agent_start(self, message: str):
        logger.brief(message)

    def log_agent_end(self, message: str):
        logger.brief(message)

    def report_token_usage(self):
        token_usage_path = os.path.join(self.output_folder, "token_usage.json")
        usage = ChatLLMFactory.get_total_token_usage(save_path=token_usage_path)
        total = usage["total"]
        logger.brief(
            f"Total tokens — input: {total['total_input_tokens']}, "
            f"output: {total['total_output_tokens']}, "
            f"sum: {total['total_tokens']}"
        )

        logger.info(f"Full token usage detail:\n{usage}")

    def cleanup(self):
        """Clean up resources."""
        if hasattr(self, "retriever"):
            self.retriever.cleanup()

    def __del__(self):
        """Destructor to ensure cleanup."""
        self.cleanup()

--- END FILE: src/iML/core/manager.py ---

--- START FILE: src/iML/core/__init__.py ---

--- END FILE: src/iML/core/__init__.py ---

--- START FILE: src/iML/utils/rich_logging.py ---
import logging
import sys
from pathlib import Path

from rich.console import Console
from rich.logging import RichHandler

from .constants import BRIEF_LEVEL, CONSOLE_HANDLER, DETAIL_LEVEL

# ── Custom log levels ─────────────────────────────
logging.addLevelName(DETAIL_LEVEL, "DETAIL")
logging.addLevelName(BRIEF_LEVEL, "BRIEF")

def detail(self, msg, *args, **kw):
    if self.isEnabledFor(DETAIL_LEVEL):
        # Add stacklevel=2 to skip one frame up the call stack
        kw.setdefault("stacklevel", 2)
        self._log(DETAIL_LEVEL, msg, args, **kw)


def brief(self, msg, *args, **kw):
    if self.isEnabledFor(BRIEF_LEVEL):
        # Add stacklevel=2 to skip one frame up the call stack
        kw.setdefault("stacklevel", 2)
        self._log(BRIEF_LEVEL, msg, args, **kw)


logging.Logger.detail = detail  # type: ignore
logging.Logger.brief = brief  # type: ignore
# ─────────────────────────────────────────


def _configure_logging(console_level: int, output_dir: Path = None) -> None:
    """
    Globally initialize logging with separate levels for console and file

    Args:
        console_level: Logging level for terminal output
        output_dir: If provided, creates both debug and info level file loggers in this directory
    """

    # Set root logger level to DEBUG to allow file handlers to capture all logs
    root_level = logging.DEBUG

    if sys.stdout.isatty():
        console = Console(file=sys.stdout)
        console_handler = RichHandler(console=console, markup=True, rich_tracebacks=True)
        console_handler.setLevel(console_level)
        console_handler.name = CONSOLE_HANDLER
        handlers = [console_handler]
    else:
        stdout_handler = logging.StreamHandler(sys.stdout)
        stdout_handler.setLevel(console_level)
        stdout_fmt = logging.Formatter("%(levelname)s %(message)s")
        stdout_handler.setFormatter(stdout_fmt)
        handlers = [stdout_handler]

    # Add file handlers if output_dir is provided
    if output_dir is not None:
        output_dir.mkdir(parents=True, exist_ok=True)

        # Debug log file (captures everything DEBUG and above)
        debug_log_path = output_dir / "debugging_logs.txt"
        debug_handler = logging.FileHandler(str(debug_log_path), mode="w", encoding="utf-8")
        debug_handler.setLevel(logging.DEBUG)
        debug_formatter = logging.Formatter(
            "%(asctime)s %(levelname)-8s [%(name)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        debug_handler.setFormatter(debug_formatter)
        handlers.append(debug_handler)

        # Detail log file (captures DETAIL and above only)
        detail_log_path = output_dir / "detail_logs.txt"
        detail_handler = logging.FileHandler(str(detail_log_path), mode="w", encoding="utf-8")
        detail_handler.setLevel(DETAIL_LEVEL)
        detail_formatter = logging.Formatter(
            "%(asctime)s %(levelname)-8s [%(name)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        detail_handler.setFormatter(detail_formatter)
        handlers.append(detail_handler)

        # Info log file (captures INFO and above only)
        info_log_path = output_dir / "info_logs.txt"
        info_handler = logging.FileHandler(str(info_log_path), mode="w", encoding="utf-8")
        info_handler.setLevel(logging.INFO)
        info_formatter = logging.Formatter(
            "%(asctime)s %(levelname)-8s [%(name)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        info_handler.setFormatter(info_formatter)
        handlers.append(info_handler)

        # Console log file (captures same level as console output)
        console_log_path = output_dir / "logs.txt"
        console_file_handler = logging.FileHandler(str(console_log_path), mode="w", encoding="utf-8")
        console_file_handler.setLevel(console_level)
        console_formatter = logging.Formatter(
            "%(asctime)s %(levelname)-8s [%(name)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        console_file_handler.setFormatter(console_formatter)
        handlers.append(console_file_handler)

    logging.basicConfig(
        level=root_level,
        format="%(message)s",
        handlers=handlers,
        force=True,  # Ensure override
    )


def configure_logging(verbosity: int, output_dir: Path = None) -> None:
    match verbosity:
        case 0:
            level = logging.ERROR  # Only errors
        case 1:
            level = BRIEF_LEVEL  # Brief summaries
        case 2:
            level = logging.INFO  # Standard info
        case 3:
            level = DETAIL_LEVEL  # Model details
        case _:  # 4+
            level = logging.DEBUG  # Full debug info
    _configure_logging(console_level=level, output_dir=output_dir)


def show_progress_bar():
    root_logger = logging.getLogger()
    console_handler_level = None
    for handler in root_logger.handlers:
        if hasattr(handler, "name") and handler.name == CONSOLE_HANDLER:
            console_handler_level = handler.level

    if console_handler_level is None:
        return False
    return console_handler_level > DETAIL_LEVEL

--- END FILE: src/iML/utils/rich_logging.py ---

--- START FILE: src/iML/utils/utils.py ---
import logging
import shutil
import sys
import zipfile
from pathlib import Path

from .constants import WEBUI_INPUT_MARKER, WEBUI_INPUT_REQUEST

logger = logging.getLogger(__name__)


# We do not suggest using the agent with zip files.
# But MLEBench contains datasets zip files.
# https://github.com/WecoAI/aideml/blob/main/aide/utils/__init__.py
def clean_up_dataset(path: Path):
    for item in path.rglob("__MACOSX"):
        if item.is_dir():
            shutil.rmtree(item)
    for item in path.rglob(".DS_Store"):
        if item.is_file():
            item.unlink()


# We do not suggest using the agent with zip files.
# But MLEBench contains datasets zip files.
# https://github.com/WecoAI/aideml/blob/main/aide/utils/__init__.py
def extract_archives(path):
    """
    Unzips all .zip files within `path` and cleans up task dir

    Args:
        path: Path object or string path to directory containing zip files

    [TODO] handle nested zips
    """
    # Convert string path to Path object if necessary
    if isinstance(path, str):
        path = Path(path)

    for zip_f in path.rglob("*.zip"):
        f_out_dir = zip_f.with_suffix("")
        # special case: the intended output path already exists (maybe data has already been extracted by user)
        if f_out_dir.exists():
            logger.debug(f"Skipping {zip_f} as an item with the same name already exists.")
            # if it's a file, it's probably exactly the same as in the zip -> remove the zip
            # [TODO] maybe add an extra check to see if zip file content matches the colliding file
            if f_out_dir.is_file() and f_out_dir.suffix != "":
                zip_f.unlink()
                continue

        logger.debug(f"Extracting: {zip_f}")
        f_out_dir.mkdir(exist_ok=True)
        with zipfile.ZipFile(zip_f, "r") as zip_ref:
            zip_ref.extractall(f_out_dir)

        # remove any unwanted files
        clean_up_dataset(f_out_dir)
        contents = list(f_out_dir.iterdir())

        # special case: the zip contains a single dir/file with the same name as the zip
        if len(contents) == 1 and contents[0].name == f_out_dir.name:
            sub_item = contents[0]
            # if it's a dir, move its contents to the parent and remove it
            if sub_item.is_dir():
                logger.debug(f"Special handling (child is dir) enabled for: {zip_f}")
                for f in sub_item.rglob("*"):
                    shutil.move(f, f_out_dir)
                sub_item.rmdir()
            # if it's a file, rename it to the parent and remove the parent
            elif sub_item.is_file():
                logger.debug(f"Special handling (child is file) enabled for: {zip_f}")
                sub_item_tmp = sub_item.rename(f_out_dir.with_suffix(".__tmp_rename"))
                f_out_dir.rmdir()
                sub_item_tmp.rename(f_out_dir)

        zip_f.unlink()


def get_user_input_webui(prompt: str) -> str:
    """Get user input in WebUI environment"""
    # Send special marker with the prompt
    print(f"{WEBUI_INPUT_REQUEST} {prompt}", flush=True)

    # Read from stdin - Flask will send the user input here
    while True:
        line = sys.stdin.readline().strip()
        if line.startswith(WEBUI_INPUT_MARKER):
            # Extract the actual user input after the marker
            user_input = line[len(WEBUI_INPUT_MARKER) :].strip()
            logger.debug(f"Received WebUI input: {user_input}")
            return user_input

--- END FILE: src/iML/utils/utils.py ---

--- START FILE: src/iML/utils/__init__.py ---

--- END FILE: src/iML/utils/__init__.py ---

--- START FILE: src/iML/utils/file_io.py ---
import os
from pathlib import Path
from typing import List
import pandas as pd

def get_directory_structure(root_dir: str,
                        max_files_per_dir: int = 3,
                        sample_rows: int = 5) -> str:
    root_dir = Path(root_dir)
    if not root_dir.is_dir():
        raise ValueError(f"'{root_dir}' is not a valid directory.")

    lines: List[str] = []
    root_name = root_dir.name
    lines.append(f"Data directory structure {root_dir}:")

    csv_paths: List[Path] = []

    for dirpath, _, filenames in os.walk(root_dir):
        dirpath = Path(dirpath)
        # Exclude description.txt files
        filenames = [fname for fname in filenames if fname.lower() != "description.txt"]
        filenames.sort()
        rel_dir = dirpath.relative_to(root_dir)
        rel_dir_str = "" if rel_dir == Path(".") else str(rel_dir)

        # Process only first max_files_per_dir files
        for fname in filenames[:max_files_per_dir]:
            rel_file = (rel_dir / fname) if rel_dir_str else Path(fname)
            lines.append(str(rel_file))

            if fname.lower().endswith(".csv"):
                csv_paths.append(rel_file)

        if len(filenames) > max_files_per_dir:
            prefix = f"{rel_dir_str}\\" if rel_dir_str else ""
            lines.append(f"{prefix}...")

        # Process remaining files for csv collection
        for fname in filenames[max_files_per_dir:]:
            if fname.lower().endswith(".csv"):
                rel_file = (rel_dir / fname) if rel_dir_str else Path(fname)
                csv_paths.append(rel_file)

    if csv_paths:
        lines.append("\n" + "="*60)
        lines.append("SUMMARY OF CSV FILES")
        lines.append("="*60)

        for rel_path in csv_paths:
            abs_path = root_dir / rel_path
            try:
                df = pd.read_csv(abs_path, nrows=sample_rows)
            except Exception as e:
                lines.append(f"\nCould not read '{rel_path}': {e}")
                continue

            lines.append(f"\nStructure of file {rel_path}:")
            lines.append("Columns: " + ", ".join(df.columns.astype(str)))
            lines.append("Some first rows:")
            lines.append(df.to_string(index=False))

    summary_str = "\n".join(lines)
    return summary_str
--- END FILE: src/iML/utils/file_io.py ---

--- START FILE: src/iML/utils/constants.py ---
from pathlib import Path

VALID_CODING_LANGUAGES = ["python", "bash"]
DEMO_URL = "https://youtu.be/kejJ3QJPW7E"
DETAIL_LEVEL = 19
BRIEF_LEVEL = 25
CONSOLE_HANDLER = "console_handler"

API_URL = "http://localhost:5000/api"

# Special markers for WebUI communication
WEBUI_INPUT_REQUEST = "###WEBUI_INPUT_REQUEST###"
WEBUI_INPUT_MARKER = "###WEBUI_USER_INPUT###"
WEBUI_OUTPUT_DIR = "###WEBUI_OUTPUT_DIR###"

# Success message displayed after task completion
SUCCESS_MESSAGE = """🎉🎉 Task completed successfully! If you found this useful, please consider:
⭐ [Starring our repository](https://github.com/autogluon/autogluon-assistant)
⭐ [Citing our paper](https://arxiv.org/abs/2505.13941)"""

PACKAGE_ROOT = Path(__file__).parent  # /src/autogluon/assistant
DEFAULT_CONFIG_PATH = PACKAGE_ROOT / "configs" / "default.yaml"
LOGO_DAY_PATH = PACKAGE_ROOT / "webui" / "static" / "sidebar_logo_blue.png"
LOGO_NIGHT_PATH = PACKAGE_ROOT / "webui" / "static" / "sidebar_icon.png"
LOGO_PATH = PACKAGE_ROOT / "webui" / "static" / "page_icon.png"

# TODO
IGNORED_MESSAGES = [
    "Too many requests, please wait before trying again",
]

VERBOSITY_MAP = {
    "DETAIL": "3",
    "INFO": "2",
    "BRIEF": "1",
}

# Provider defaults
PROVIDER_DEFAULTS = {
    "bedrock": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
    "openai": "gpt-4o-2024-08-06",
    "anthropic": "claude-3-7-sonnet-20250219",
}

--- END FILE: src/iML/utils/constants.py ---

--- START FILE: src/iML/agents/profiling_agent.py ---
import json
import logging
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd
from tqdm import tqdm
from ydata_profiling import ProfileReport

from .base_agent import BaseAgent

# Configure logger
logger = logging.getLogger(__name__)

class ProfilingAgent(BaseAgent):
    """
    This agent performs data profiling based on analysis results
    from DescriptionAnalyzerAgent. It reads CSV files, creates reports,
    and returns a comprehensive summary.
    """

    def __init__(self, config, manager):
        super().__init__(config=config, manager=manager)
        # This agent doesn't need LLM or specific prompt configuration
        logger.info("ProfilingAgent initialized.")

    def __call__(self) -> Dict[str, Any]:
        """
        Main execution method of the agent.
        """
        self.manager.log_agent_start("ProfilingAgent: Starting Data Profiling...")

        # Get analysis results from manager
        description_analysis = self.manager.description_analysis
        if not description_analysis or "error" in description_analysis:
            logger.error("ProfilingAgent: description_analysis is missing or contains an error. Skipping.")
            return {"error": "Input description_analysis not available."}

        ds_name = description_analysis.get('name', 'unnamed_dataset')
        logger.info(f"Profiling dataset: {ds_name}")

        # Check existence of data files
        paths_list = description_analysis.get("link to the dataset", [])
        if not isinstance(paths_list, list):
            logger.warning("'link to the dataset' is not a list. Skipping profiling.")
            paths_list = []

        path_status = self._check_paths(paths_list)
        
        # Filter and profile existing CSV files
        existing_csv_paths = [p for p in path_status["exists"] if p.lower().endswith(".csv")]

        # --- CHANGE: Save profiling results to memory ---
        all_summaries = {}
        all_profiles = {}

        for p_str in tqdm(existing_csv_paths, desc="Profiling CSV files"):
            csv_path = Path(p_str)
            file_stem = csv_path.stem
            summary, profile_content = self._profile_csv(csv_path)
            
            # Only add if profiling is successful
            if summary:
                all_summaries[file_stem] = summary
            if profile_content:
                all_profiles[file_stem] = profile_content
            
        # Combine back into a single object
        profiling_result = {
            "summaries": all_summaries,
            "profiles": all_profiles
        }

        # Save aggregated results to the run's states directory
        self.manager.save_and_log_states(
            content=json.dumps(profiling_result, indent=2, ensure_ascii=False),
            save_name="profiling_result.json" # File name as requested
        )

        self.manager.log_agent_end(f"ProfilingAgent: Profiling COMPLETED.")
        
        # Return aggregated results
        return profiling_result

    def _check_paths(self, paths: List[str]) -> Dict[str, List[str]]:
        """Check if file paths exist."""
        exists, missing = [], []
        for p in paths:
            pth = Path(p)
            (exists if pth.exists() else missing).append(p)
        return {"exists": exists, "missing": missing}

    def _filter_value_counts(self, profile_json_str: str) -> str:
        """Remove heavy parts from profile JSON to reduce file size."""
        try:
            profile_dict = json.loads(profile_json_str)
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error while filtering profile: {e}")
            return profile_json_str
        
        if "variables" not in profile_dict:
            return profile_json_str
            
        for var_info in profile_dict.get("variables", {}).values():
            var_type = var_info.get("type", "")
            n_unique = var_info.get("n_unique", 0)
            should_remove = (
                var_type in ["Text", "Numeric", "Date", "DateTime", "Time", "URL", "Path"]
                or (var_type == "Categorical" and n_unique > 50)
            )
            if should_remove:
                keys_to_remove = [
                    "value_counts_without_nan", "value_counts_index_sorted", "histogram",
                    "length_histogram", "histogram_length", "block_alias_char_counts",
                    "word_counts", "category_alias_char_counts", "script_char_counts",
                    "block_alias_values", "category_alias_values", "character_counts",
                    "block_alias_counts", "script_counts", "category_alias_counts",
                    "n_block_alias", "n_scripts", "n_category",
                ]
                for key in keys_to_remove:
                    var_info.pop(key, None)
        return json.dumps(profile_dict, ensure_ascii=False, indent=2)

    def _profile_csv(self, csv_path: Path) -> tuple[Any, Any]:
        """Create profile report for a CSV file and return (summary, profile_content)."""
        try:
            df = pd.read_csv(csv_path)
            logger.info(f"Analyzing {csv_path.name} ({df.shape[0]} rows, {df.shape[1]} columns)")
            
            profile = ProfileReport(
                df,
                title=f"Profile - {csv_path.name}",
                minimal=True,
                samples={"random": 5},
                correlations={"auto": {"calculate": False}},
                missing_diagrams={"bar": False, "matrix": False},
                interactions={"targets": []},
                explorative=False,
                progress_bar=False,
                infer_dtypes=True
            )
            
            filtered_json_str = self._filter_value_counts(profile.to_json())
            profile_content = json.loads(filtered_json_str)

            summary = {
                "file": str(csv_path),
                "n_rows": int(df.shape[0]),
                "n_cols": int(df.shape[1]),
                "dtypes": df.dtypes.astype(str).to_dict(),
                "missing_pct": df.isnull().mean().round(4).to_dict(),
                "file_size_mb": round(csv_path.stat().st_size / (1024 * 1024), 2)
            }
            
            logger.info(f"In-memory profile created for {csv_path.name}")
            return summary, profile_content
        except Exception as e:
            logger.error(f"Failed to profile {csv_path.name}: {e}")
            error_summary = {"error": str(e), "file": str(csv_path)}
            return error_summary, None

--- END FILE: src/iML/agents/profiling_agent.py ---

--- START FILE: src/iML/agents/modeling_coder_agent.py ---
# src/iML/agents/modeling_coder_agent.py
import logging
from typing import Dict, Any

from .base_agent import BaseAgent
from ..prompts import ModelingCoderPrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class ModelingCoderAgent(BaseAgent):
    """
    Agent to create modeling code.
    It only generates code once and does not execute it.
    """
    def __init__(self, config: Dict, manager: Any, llm_config: Dict, **kwargs):
        super().__init__(config, manager)
        self.llm_config = llm_config
        self.llm = init_llm(
            llm_config=llm_config,
            agent_name="modeling_coder",
            multi_turn=llm_config.get("multi_turn", False),
        )
        self.prompt_handler = ModelingCoderPrompt(
            manager=manager, 
            llm_config=self.llm_config
        )

    def __call__(self) -> Dict[str, Any]:
        """
        Agent for generating code based on training requirements.
        """
        self.manager.log_agent_start("Starting modeling code generation...")

        guideline = self.manager.guideline
        description = self.manager.description_analysis
        preprocessing_code = self.manager.preprocessing_code
        
        if not preprocessing_code:
            logger.error("Preprocessing code not found. Cannot continue.")
            return {"status": "failed", "error": "Preprocessing code not available."}

        # 1. Generate modeling code
        prompt = self.prompt_handler.build(
            guideline=guideline,
            description=description,
            preprocessing_code=preprocessing_code,
        )
        response = self.llm.assistant_chat(prompt)
        modeling_code = self.prompt_handler.parse(response)

        self.manager.log_agent_end("Completed modeling code generation.")
        return {"status": "success", "code": modeling_code}
--- END FILE: src/iML/agents/modeling_coder_agent.py ---

--- START FILE: src/iML/agents/utils.py ---
from datetime import datetime

from ..llm import ChatLLMFactory


def init_llm(llm_config, agent_name, multi_turn):
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    if multi_turn:
        session_name = f"multi_turn_{agent_name}_{timestamp}"
    else:
        session_name = f"single_turn_{agent_name}_{timestamp}"
    llm = ChatLLMFactory.get_chat_model(llm_config, session_name=session_name)

    return llm

--- END FILE: src/iML/agents/utils.py ---

--- START FILE: src/iML/agents/assembler_agent.py ---
# src/iML/agents/assembler_agent.py
import logging
import os
from typing import Dict, Any

from .base_agent import BaseAgent
from ..prompts import AssemblerPrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class AssemblerAgent(BaseAgent):
    """
    Agent to assemble, finalize, execute and fix final code.
    """
    def __init__(self, config: Dict, manager: Any, llm_config: Dict, max_retries: int = 5):
        super().__init__(config, manager)
        self.llm_config = llm_config
        self.llm = init_llm(
            llm_config=llm_config,
            agent_name="assembler",
            multi_turn=llm_config.get("multi_turn", False),
        )
        self.prompt_handler = AssemblerPrompt(
            manager=manager, 
            llm_config=self.llm_config
        )
        self.max_retries = max_retries

    def __call__(self) -> Dict[str, Any]:
        """
        Assemble, execute and retry final code until successful.
        """
        self.manager.log_agent_start("Starting assembly and testing of final code...")

        preprocessing_code = self.manager.preprocessing_code
        modeling_code = self.manager.modeling_code

        if not preprocessing_code or not modeling_code:
            error = "Preprocessing or modeling code not available."
            logger.error(error)
            return {"status": "failed", "error": error}

        # Combine initial code
        combined_code = preprocessing_code + "\n\n" + modeling_code
        submission_path = os.path.join(self.manager.output_folder, "submission.csv")
        error_message = None

        for attempt in range(self.max_retries):
            logger.info(f"Assembly and execution attempt {attempt + 1}/{self.max_retries}...")

            # 1. Assemble/Fix code
            # On first attempt, error_message is None, LLM will just assemble.
            # In subsequent attempts, LLM will fix errors.
            prompt = self.prompt_handler.build(
                original_code=combined_code,
                output_path=submission_path,
                error_message=error_message
            )
            response = self.llm.assistant_chat(prompt)
            final_code = self.prompt_handler.parse(response)
            
            # 2. Execute final code
            execution_result = self.manager.execute_code(final_code)
            
            if execution_result["success"]:
                logger.info("Final code executed successfully!")
                logger.info(f"Submission file created at: {submission_path}")
                self.manager.save_and_log_states(final_code, "final_executable_code.py")
                self.manager.log_agent_end("Completed assembly and execution of code.")
                return {"status": "success", "code": final_code, "submission_path": submission_path}
            else:
                error_message = execution_result["stderr"]
                logger.warning(f"Code execution failed on attempt {attempt + 1}. Error: {error_message}")
                self.manager.save_and_log_states(
                    f"---ATTEMPT {attempt+1}---\nCODE:\n{final_code}\n\nERROR:\n{error_message}",
                    f"assembler_attempt_{attempt+1}_failed.log"
                )
                # Update combined_code so next iteration LLM will fix the latest error version
                combined_code = final_code

        logger.error(f"Unable to generate working code after {self.max_retries} attempts.")
        self.manager.log_agent_end("Code assembly failed.")
        return {"status": "failed", "error": "Exceeded maximum retry attempts to generate code."}

--- END FILE: src/iML/agents/assembler_agent.py ---

--- START FILE: src/iML/agents/candidate_generator_agent.py ---
import logging
import re
from .base_agent import BaseAgent
from ..prompts.candidate_generator_prompt import CandidateGeneratorPrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class CandidateGeneratorAgent(BaseAgent):
    """
    An agent that generates multiple candidate code versions based on feedback.
    It ensures that each generated candidate is executable.
    """
    def __init__(self, config, manager, llm_config, prompt_template=None):
        super().__init__(config=config, manager=manager)
        self.llm_config = llm_config
        self.prompt_template = prompt_template
        self.llm = init_llm(
            llm_config=self.llm_config,
            agent_name="candidate_generator",
            multi_turn=self.llm_config.multi_turn,
        )


    def __call__(self) -> dict:
        """
        Generates and validates candidate code versions sequentially.
        """
        self.manager.log_agent_start("Generating candidate codes sequentially based on feedback...")

        if not hasattr(self.manager, "feedback"):
            error_msg = "Missing feedback in manager for candidate generation."
            logger.error(error_msg)
            return {"status": "failed", "error": error_msg}

        prompt_handler = CandidateGeneratorPrompt(
            manager=self.manager,
            llm_config=self.llm_config,
        )

        valid_candidates = []
        max_candidates = 3
        max_retries_per_candidate = 3

        for i in range(max_candidates):
            logger.info(f"--- Generating Candidate {i+1}/{max_candidates} ---")
            found_valid_candidate_for_slot = False
            for attempt in range(max_retries_per_candidate):
                logger.info(f"Attempt {attempt + 1}/{max_retries_per_candidate} for Candidate {i+1}...")
                
                try:
                    prompt = prompt_handler.build(
                        candidate_number=i + 1,
                        previous_candidates=valid_candidates
                    )
                    response = self.llm.assistant_chat(prompt)
                    code = prompt_handler.parse(response)

                    if not code:
                        logger.warning("LLM returned empty code. Retrying...")
                        continue

                    logger.info(f"Validating generated code for Candidate {i+1}...")
                    execution_result = self.manager.execute_code(code)

                    if execution_result["success"]:
                        logger.info(f"Candidate {i+1} is valid and has been added.")
                        valid_candidates.append(code)
                        found_valid_candidate_for_slot = True
                        break  # Exit retry loop and move to the next candidate
                    else:
                        logger.warning(f"Candidate {i+1} (attempt {attempt+1}) failed validation. Retrying. Error: {execution_result['stderr']}")

                except Exception as e:
                    logger.error(f"An error occurred during generation for Candidate {i+1} (attempt {attempt + 1}): {e}")
            
            if not found_valid_candidate_for_slot:
                logger.error(f"Could not generate a valid Candidate {i+1} after {max_retries_per_candidate} attempts. Moving on.")

        if valid_candidates:
            self.manager.log_agent_end(f"Successfully generated {len(valid_candidates)} valid candidate codes.")
            return {"status": "success", "candidates": valid_candidates}
        else:
            error_msg = "Failed to generate any valid candidate codes after all attempts."
            self.manager.log_agent_end(error_msg)
            return {"status": "failed", "error": error_msg}

    def _parse_candidates(self, response: str) -> list[str]:
        """
        Parses the LLM response to extract individual code candidates.
        """
        # The pattern looks for ### CANDIDATE X ### and captures the code block that follows.
        # re.DOTALL allows `.` to match newlines.
        pattern = r"### CANDIDATE \d+ ###\s*```python\n(.*?)\n```"
        candidates = re.findall(pattern, response, re.DOTALL)
        return [c.strip() for c in candidates]

--- END FILE: src/iML/agents/candidate_generator_agent.py ---

--- START FILE: src/iML/agents/feedback_agent.py ---
import logging
from .base_agent import BaseAgent
from ..prompts.feedback_prompt import FeedbackPrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class FeedbackAgent(BaseAgent):
    """
    An agent that analyzes the successfully executed code and provides feedback for improvement.
    """
    def __init__(self, config, manager, llm_config, prompt_template=None):
        super().__init__(config=config, manager=manager)
        self.llm_config = llm_config
        self.prompt_template = prompt_template
        self.llm = init_llm(
            llm_config=self.llm_config,
            agent_name="feedback",
            multi_turn=self.llm_config.multi_turn,
        )


    def __call__(self) -> dict:
        """
        Analyzes the code and returns feedback.

        Returns:
            A dictionary containing the feedback or an error message.
        """
        self.manager.log_agent_start("Analyzing code for feedback...")

        # Ensure the required information is available in the manager
        if not hasattr(self.manager, "description_analysis") or \
           not hasattr(self.manager, "profiling_result") or \
           not hasattr(self.manager, "guideline") or \
           not hasattr(self.manager, "assembled_code"):
            error_msg = "Missing necessary data in manager for feedback generation."
            logger.error(error_msg)
            return {"status": "failed", "error": error_msg}

        prompt_handler = FeedbackPrompt(
            manager=self.manager,
            llm_config=self.llm_config,
        )

        try:
            prompt = prompt_handler.build()
            response = self.llm.assistant_chat(prompt)
            parsed_response = prompt_handler.parse(response)
            self.manager.log_agent_end("Feedback generated successfully.")
            return {"status": "success", "feedback": parsed_response}
        except Exception as e:
            error_msg = f"An error occurred while generating feedback: {e}"
            logger.error(error_msg)
            self.manager.log_agent_end(f"Feedback generation failed: {error_msg}", level="error")
            return {"status": "failed", "error": error_msg}

--- END FILE: src/iML/agents/feedback_agent.py ---

--- START FILE: src/iML/agents/preprocessing_coder_agent.py ---
import logging
from typing import Dict, Any

from .base_agent import BaseAgent
from ..prompts import PreprocessingCoderPrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class PreprocessingCoderAgent(BaseAgent):
    """
    Agent to create and execute preprocessing code.
    It has a retry loop to generate and validate code until it runs successfully or runs out of retries.
    """
    def __init__(self, config: Dict, manager: Any, llm_config: Dict, max_retries: int = 5):
        super().__init__(config, manager)
        self.llm_config = llm_config
        self.llm = init_llm(
            llm_config=llm_config,
            agent_name="preprocessing_coder",
            multi_turn=llm_config.get("multi_turn", False),
        )
        self.prompt_handler = PreprocessingCoderPrompt(
            manager=manager, 
            llm_config=self.llm_config
        )
        self.max_retries = max_retries

    def __call__(self) -> Dict[str, Any]:
        """
        Generate, execute and retry preprocessing code until successful or maximum retries exceeded.
        """
        self.manager.log_agent_start("Starting preprocessing code generation...")

        guideline = self.manager.guideline
        description = self.manager.description_analysis
        
        code_to_execute = None
        error_message = None
        
        for attempt in range(self.max_retries):
            logger.info(f"Code generation attempt {attempt + 1}/{self.max_retries}...")

            # 1. Generate code
            prompt = self.prompt_handler.build(
                guideline=guideline,
                description=description,
                previous_code=code_to_execute,
                error_message=error_message
            )
            response = self.llm.assistant_chat(prompt)
            code_to_execute = self.prompt_handler.parse(response)

            # 2. Execute code
            execution_result = self.manager.execute_code(code_to_execute)
            
            # 3. Check results
            if execution_result["success"]:
                logger.info("Preprocessing code executed successfully!")
                self.manager.save_and_log_states(code_to_execute, "final_preprocessing_code.py")
                self.manager.log_agent_end("Completed preprocessing code generation.")
                return {"status": "success", "code": code_to_execute}
            else:
                error_message = execution_result["stderr"]
                logger.warning(f"Code execution failed on attempt {attempt + 1}. Error: {error_message}")
                self.manager.save_and_log_states(
                    f"---ATTEMPT {attempt+1}---\nCODE:\n{code_to_execute}\n\nERROR:\n{error_message}",
                    f"preprocessing_attempt_{attempt+1}_failed.log"
                )

        logger.error(f"Unable to generate working preprocessing code after {self.max_retries} attempts.")
        self.manager.log_agent_end("Preprocessing code generation failed.")
        return {"status": "failed", "error": "Exceeded maximum retry attempts to generate code."}

--- END FILE: src/iML/agents/preprocessing_coder_agent.py ---

--- START FILE: src/iML/agents/description_analyzer_agent.py ---
# src/iML/agents/description_analyzer_agent.py

from .base_agent import BaseAgent
from ..prompts.description_analyzer_prompt import DescriptionAnalyzerPrompt
from ..utils.file_io import get_directory_structure
from .utils import init_llm

import logging
import os
from pathlib import Path

# Basic configuration
logging.basicConfig(level=logging.INFO)

# Create a logger
logger = logging.getLogger(__name__)

class DescriptionAnalyzerAgent(BaseAgent):
    """
    Agent for analyzing the project description and directory structure to understand the task.
    Agent Input: Path to the dataset
    Agent Output: A structured analysis of the ML task.
    """

    def __init__(self, config, manager, llm_config, prompt_template=None):
        super().__init__(config=config, manager=manager)

        self.description_analyzer_llm_config = llm_config
        self.description_analyzer_prompt_template = prompt_template

        # Initialize the corresponding prompt handler
        self.prompt_handler = DescriptionAnalyzerPrompt(
            llm_config=self.description_analyzer_llm_config,
            manager=self.manager,
            template=self.description_analyzer_prompt_template,
        )

        if self.description_analyzer_llm_config.multi_turn:
            self.llm = init_llm(
                llm_config=self.description_analyzer_llm_config,
                agent_name="description_analyzer",
            )

    def __call__(self):
        """
        Executes the description analysis logic.
        """
        self.manager.log_agent_start("DescriptionAnalyzerAgent: Starting Description Analysis...")

        dataset_path = self.manager.input_data_folder
        description_file_path = Path(dataset_path) / "description.txt"

        # Step 1: Read description file
        if not description_file_path.exists():
            logger.error(f"Description file not found at: {description_file_path}")
            return {"error": f"description.txt not found in {dataset_path}"}
        
        with open(description_file_path, "r", encoding="utf-8") as desc_file:
            description = desc_file.read()

        # Step 2: Get directory structure
        directory_structure = get_directory_structure(dataset_path)

        # Step 3: Build the prompt
        prompt = self.prompt_handler.build(
            description=description,
            directory_structure=directory_structure
        )

        # Step 4: Initialize LLM if not already done (for single-turn)
        if not self.description_analyzer_llm_config.multi_turn:
            self.llm = init_llm(
                llm_config=self.description_analyzer_llm_config,
                agent_name="description_analyzer",
                multi_turn=self.description_analyzer_llm_config.multi_turn,
            )

        # Step 5: Call the LLM
        response = self.llm.assistant_chat(prompt)
        self.manager.save_and_log_states(
            content=response, 
            save_name="description_analyzer_raw_response.txt"
        )

        # Step 6: Parse the response
        analysis_result = self.prompt_handler.parse(response)

        self.manager.log_agent_end("DescriptionAnalyzerAgent: Description Analysis COMPLETED.")
        return analysis_result
--- END FILE: src/iML/agents/description_analyzer_agent.py ---

--- START FILE: src/iML/agents/candidate_selector_agent.py ---
import logging
import re
from .base_agent import BaseAgent
from ..prompts.candidate_selector_prompt import CandidateSelectorPrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class CandidateSelectorAgent(BaseAgent):
    """
    An agent that uses an LLM to select the best code candidate from a list.
    """
    def __init__(self, config, manager, llm_config, prompt_template=None):
        super().__init__(config=config, manager=manager)
        self.llm_config = llm_config
        self.prompt_template = prompt_template
        self.llm = init_llm(
            llm_config=self.llm_config,
            agent_name="candidate_selector",
            multi_turn=self.llm_config.multi_turn,
        )


    def __call__(self) -> dict:
        """
        Selects the best candidate code.

        Returns:
            A dictionary containing the selected code or an error message.
        """
        self.manager.log_agent_start("Selecting the best candidate code...")

        if not hasattr(self.manager, "candidates") or not self.manager.candidates:
            error_msg = "No candidates available for selection."
            logger.error(error_msg)
            return {"status": "failed", "error": error_msg}

        prompt_handler = CandidateSelectorPrompt(
            manager=self.manager,
            llm_config=self.llm_config,
        )

        try:
            prompt = prompt_handler.build()
            response = self.llm.assistant_chat(prompt)
            parsed_response = prompt_handler.parse(response)
            # The LLM is instructed to return only the code, so we do a simple cleanup
            selected_code = self._extract_code(parsed_response)
            self.manager.save_and_log_states(selected_code, "final_selected_code.py")

            if not selected_code:
                raise ValueError("LLM did not return a valid code block.")

            self.manager.log_agent_end("Best candidate selected successfully.")
            return {"status": "success", "selected_code": selected_code}
        except Exception as e:
            error_msg = f"An error occurred while selecting the best candidate: {e}"
            logger.error(error_msg)
            self.manager.log_agent_end(error_msg, level="error")
            return {"status": "failed", "error": error_msg}

    def _extract_code(self, response: str) -> str:
        """
        Extracts the Python code from the LLM's response.
        Handles cases where the code is wrapped in markdown.
        """
        # Pattern to find a python code block
        match = re.search(r"```python\n(.*?)\n```", response, re.DOTALL)
        if match:
            return match.group(1).strip()
        # If no markdown block is found, assume the whole response is the code
        return response.strip()

--- END FILE: src/iML/agents/candidate_selector_agent.py ---

--- START FILE: src/iML/agents/executer_agent.py ---
import logging

from rich.progress import (
    Progress,
    TextColumn,
)

from ..prompts import ExecuterPrompt
from ..utils.rich_logging import show_progress_bar
from .base_agent import BaseAgent
from .utils import init_llm

logger = logging.getLogger(__name__)


def execute_code(code, language, timeout):
    """
    Execute code with real-time output streaming and timeout and show a linear timeout progress bar..
    Args:
        code (str): The code to execute (Python code or bash script)
        language (str): The language to execute ("python" or "bash")
        timeout (float): Maximum execution time in seconds before terminating the process.
    Returns:
        tuple: (success: bool, stdout: str, stderr: str)
    """
    import select
    import subprocess
    import time

    try:
        # Set up the command based on language
        if language.lower() == "python":
            cmd = ["python", "-c", code]
        elif language.lower() == "bash":
            cmd = ["bash", "-c", code]
        else:
            return False, "", f"Unsupported language: {language}. Use 'python' or 'bash'."

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
        )

        stdout_chunks, stderr_chunks = [], []

        # Set up tracking of both output streams
        streams = [process.stdout, process.stderr]

        # Track start time for timeout
        start_time = time.time()

        with Progress(
            TextColumn(f"[bold cyan]Executing {language}:"),
            TextColumn("[bold green]{task.completed:.1f}s[/bold green] [dim](time limit: {task.total:.0f}s)[/dim]"),
            refresh_per_second=2,
            transient=False,
            disable=not show_progress_bar(),
        ) as progress_context:

            task = progress_context.add_task("", total=timeout)

            while streams:
                # Calculate remaining time
                elapsed_time = time.time() - start_time
                progress_context.update(task, completed=elapsed_time)
                remaining_time = max(0, timeout - elapsed_time)

                # Check if we've exceeded timeout
                if remaining_time <= 0:
                    process.terminate()
                    time.sleep(3)  # Give it a moment to terminate gracefully
                    if process.poll() is None:  # If still running
                        process.kill()  # Force kill
                    stdout_chunks.append(f"\nProcess reached time limit after {timeout} seconds.\n")
                    logger.info(f"\nProcess reached time limit after {timeout} seconds.\n")
                    break

                # Wait for output on either stream with timeout
                # select.select returns empty lists if the timeout elapses
                readable, _, _ = select.select(streams, [], [], min(1, remaining_time))

                # If nothing was read but process is still running, continue the loop
                if not readable and process.poll() is None:
                    continue

                # If nothing was read and process exited, exit loop
                if not readable and process.poll() is not None:
                    break

                for stream in readable:
                    line = stream.readline()
                    if not line:  # EOF
                        streams.remove(stream)
                        continue

                    # Handle stdout
                    if stream == process.stdout:
                        stdout_chunks.append(line)
                        logger.detail(line.rstrip())
                    # Handle stderr
                    else:
                        stderr_chunks.append(line)
                        logger.detail(line.rstrip())

            elapsed_time = time.time() - start_time
            progress_context.update(task, completed=elapsed_time)

        # Wait for process to complete (should already be done, but just in case)
        if process.poll() is None:
            try:
                process.wait(timeout=1)
            except subprocess.TimeoutExpired:
                process.kill()
                stderr_chunks.append("Process forcibly terminated after timeout\n")

        success = process.returncode == 0
        return success, "".join(stdout_chunks), "".join(stderr_chunks)

    except Exception as e:
        return False, "", f"Error executing {language} code: {str(e)}"


class ExecuterAgent(BaseAgent):
    """
    Execute the code and give analysis.

    Agent Input:

    Agent Output:
    """

    def __init__(self, config, manager, language, timeout, executer_llm_config, executer_prompt_template):
        super().__init__(config=config, manager=manager)
        assert language in ["bash", "python"]

        self.timeout = timeout
        self.language = language
        self.executer_llm_config = executer_llm_config

        if executer_prompt_template is not None:
            self.executer_prompt_template = executer_prompt_template
        elif self.executer_llm_config.template is not None:
            self.executer_prompt_template = self.executer_llm_config.template
        else:
            self.executer_prompt_template = None

        if self.executer_llm_config.multi_turn:
            self.executer_llm = init_llm(
                llm_config=self.executer_llm_config,
                agent_name=f"{language}_executer",
                multi_turn=self.executer_llm_config.multi_turn,
            )

        self.executer_prompt = ExecuterPrompt(
            llm_config=self.executer_llm_config, manager=manager, template=self.executer_prompt_template
        )

    def __call__(self, code_to_execute, code_to_analyze=None, task_description=None, data_prompt=None):

        self.manager.log_agent_start("ExecuterAgent: executing code and collecting stdout/stderr for evaluation.")

        if code_to_analyze is None:
            code_to_analyze = code_to_execute

        success, stdout, stderr = execute_code(code=code_to_execute, language=self.language, timeout=self.timeout)

        if not self.executer_llm_config.multi_turn:
            self.executer_llm = init_llm(
                llm_config=self.executer_llm_config,
                agent_name=f"{self.language}_executer",
                multi_turn=self.executer_llm_config.multi_turn,
            )

        # Build prompt for evaluating execution results
        prompt = self.executer_prompt.build(
            stdout=stdout,
            stderr=stderr,
            python_code=code_to_analyze,
            task_description=task_description,
            data_prompt=data_prompt,
        )

        # Query the LLM
        response = self.executer_llm.assistant_chat(prompt)

        # Parse the LLM response to extract decision and error summary
        decision, error_summary = self.executer_prompt.parse(response)

        # Log the decision and error summary
        logger.brief(f"Planner decision: {decision}")
        if error_summary:
            logger.info(f"Error summary: {error_summary}")

        self.manager.log_agent_end("ExecuterAgent: execution finished; planner decision logged.")

        return decision, error_summary, prompt, stderr, stdout

--- END FILE: src/iML/agents/executer_agent.py ---

--- START FILE: src/iML/agents/__init__.py ---
from .description_analyzer_agent import DescriptionAnalyzerAgent
from .profiling_agent import ProfilingAgent
from .base_agent import BaseAgent
from ..prompts.description_analyzer_prompt import DescriptionAnalyzerPrompt
from ..utils.file_io import get_directory_structure
from .utils import init_llm
from .guideline_agent import GuidelineAgent
from .preprocessing_coder_agent import PreprocessingCoderAgent
from .modeling_coder_agent import ModelingCoderAgent
from .assembler_agent import AssemblerAgent
from .feedback_agent import FeedbackAgent
from .candidate_generator_agent import CandidateGeneratorAgent
from .candidate_selector_agent import CandidateSelectorAgent

import logging
import os
from pathlib import Path

__all__ = [
    "BaseAgent",
    "DescriptionAnalyzerAgent",
    "ProfilingAgent",
    "GuidelineAgent",
    "PreprocessingCoderAgent",
    "ModelingCoderAgent",
    "AssemblerAgent",
    "FeedbackAgent",
    "CandidateGeneratorAgent",
    "CandidateSelectorAgent",
]
--- END FILE: src/iML/agents/__init__.py ---

--- START FILE: src/iML/agents/base_agent.py ---
class BaseAgent:
    def __init__(self, config, manager):
        self.config = config
        self.manager = manager

--- END FILE: src/iML/agents/base_agent.py ---

--- START FILE: src/iML/agents/guideline_agent.py ---
# src/iML/agents/guideline_agent.py
import logging
from typing import Dict, Any

from .base_agent import BaseAgent
from ..prompts.guideline_prompt import GuidelinePrompt
from .utils import init_llm

logger = logging.getLogger(__name__)

class GuidelineAgent(BaseAgent):
    """
    This agent creates a detailed guideline to solve the problem,
    based on description information and data profiling results.
    """

    def __init__(self, config, manager, llm_config, prompt_template=None):
        super().__init__(config=config, manager=manager)
        
        self.llm_config = llm_config
        self.prompt_template = prompt_template

        self.prompt_handler = GuidelinePrompt(
            llm_config=self.llm_config,
            manager=self.manager,
            template=self.prompt_template,
        )

        # Initialize LLM
        self.llm = init_llm(
            llm_config=self.llm_config,
            agent_name="guideline_agent",
            multi_turn=self.llm_config.get('multi_turn', False)
        )

    def __call__(self) -> Dict[str, Any]:
        """
        Execute agent to create guideline.
        """
        self.manager.log_agent_start("GuidelineAgent: Starting guideline generation...")

        description_analysis = self.manager.description_analysis
        profiling_result = self.manager.profiling_result

        if not description_analysis or "error" in description_analysis:
            logger.error("GuidelineAgent: description_analysis is missing.")
            return {"error": "description_analysis not available."}
        
        if not profiling_result or "error" in profiling_result:
            logger.error("GuidelineAgent: profiling_result is missing.")
            return {"error": "profiling_result not available."}

        # Build prompt
        prompt = self.prompt_handler.build(
            description_analysis=description_analysis,
            profiling_result=profiling_result
        )

        # Call LLM
        response = self.llm.assistant_chat(prompt)
        self.manager.save_and_log_states(response, "guideline_raw_response.txt")

        # Analyze results
        guideline = self.prompt_handler.parse(response)

        self.manager.log_agent_end("GuidelineAgent: Guideline generation COMPLETED.")
        return guideline

--- END FILE: src/iML/agents/guideline_agent.py ---

--- START FILE: src/iML/prompts/assembler_prompt.py ---
# src/iML/prompts/assembler_prompt.py
import json
from typing import Dict, Any

from .base_prompt import BasePrompt

class AssemblerPrompt(BasePrompt):
    """
    Prompt handler to assemble and fix final code.
    """

    def default_template(self) -> str:
        """Default template to request LLM to rewrite and fix code."""
        return """
You are a senior ML engineer finalizing a project. You have been given a Python script that combines preprocessing and modeling.
Your task is to ensure the script is clean, robust, and correct.

## REQUIREMENTS:
1.  **Final Script**: The output must be a single, standalone, executable Python file.
2.  **Validation Score**: If validation data is available, you MUST calculate and print a relevant validation score.
3.  **Absolute Output Path**: The script MUST save `submission.csv` to the following absolute path: `{output_path}`.
4.  **Error Handling**: Maintain the `try...except` block for robust execution.
5.  **Clarity**: Ensure the final script is clean and well-structured.

## ORIGINAL CODE:
```python
{original_code}
```
{retry_context}
## INSTRUCTIONS:
Based on the context above, generate the complete and corrected Python code. The output should be ONLY the final Python code.

## FINAL, CORRECTED CODE:
"""

    def build(self, original_code: str, output_path: str, error_message: str = None) -> str:
        """Build prompt to assemble or fix code."""
        
        retry_context = ""
        if error_message:
            retry_context = f"""
## PREVIOUS ATTEMPT FAILED:
The code above failed with the following error.

### Error Message:
```
{error_message}
```

### FIX INSTRUCTIONS:
1.  Analyze the error message and the original code carefully.
2.  Fix the specific issue that caused the error.
3.  Generate a new, complete, and corrected version of the Python code that resolves the issue and meets all requirements.
"""
        
        prompt = self.template.format(
            original_code=original_code,
            output_path=output_path,
            retry_context=retry_context
        )
        
        self.manager.save_and_log_states(prompt, "assembler_prompt.txt", per_iteration=True)
        return prompt

    def parse(self, response: str) -> str:
        """Extract Python code from LLM response."""
        if "```python" in response:
            code = response.split("```python")[1].split("```")[0].strip()
        elif "```" in response:
            code = response.split("```")[1].split("```")[0].strip()
        else:
            code = response
        
        self.manager.save_and_log_states(code, "final_assembled_code.py", per_iteration=True)
        return code

--- END FILE: src/iML/prompts/assembler_prompt.py ---

--- START FILE: src/iML/prompts/base_prompt.py ---
import logging
from abc import ABC, abstractmethod
from typing import Dict

logger = logging.getLogger(__name__)


class BasePrompt(ABC):
    """Abstract base class for prompt handling"""

    def __init__(self, manager, llm_config, template=None, **kwargs):
        """
        Initialize prompt handler with configuration and optional template.

        Args:
            manager: The manager instance
            llm_config: Configuration for the language model
            template: Optional custom template. Can be:
                     - None: use default template
                     - A string path ending in .txt: load template from file
                     - A string: use as template directly
        """
        self.manager = manager
        self.llm_config = llm_config
        self.set_template(template)

    def _load_template(self, template_str_or_path):
        if isinstance(template_str_or_path, str) and template_str_or_path.endswith(".txt"):
            try:
                logger.info(f"Loading template from file {template_str_or_path}")
                with open(template_str_or_path, "r") as f:
                    self.template = f.read()
            except Exception as e:
                logger.warning(f"Failed to load template from file {template_str_or_path}: {e}")
                self.template = self.default_template()
        else:
            self.template = template_str_or_path

    def set_template(self, template):
        """
        Set a new template.

        Args:
            template: Can be a file path ending in .txt or a template string
        """
        if template is not None:
            self._load_template(template)
        elif self.llm_config.template is not None:
            self._load_template(self.llm_config.template)
        else:
            self.template = self.default_template()

    def _truncate_output_end(self, output: str, max_length: int) -> str:
        """Helper method to truncate output from the end if it exceeds max length"""
        if len(output) > max_length:
            truncated_text = f"\n[...TRUNCATED ({len(output) - max_length} characters)...]\n"
            return output[:max_length] + truncated_text
        return output

    def _truncate_output_mid(self, output: str, max_length: int) -> str:
        """Helper method to truncate output from the middle if it exceeds max length"""
        if len(output) > max_length:
            half_size = max_length // 2
            start_part = output[:half_size]
            end_part = output[-half_size:]
            truncated_text = f"\n[...TRUNCATED ({len(output) - max_length} characters)...]\n"
            return start_part + truncated_text + end_part
        return output

    @abstractmethod
    def build(self) -> str:
        """Build the prompt string"""
        pass

    @abstractmethod
    def parse(self, response: Dict) -> any:
        """Parse the LLM response"""
        pass

    @abstractmethod
    def default_template(self) -> str:
        """Default prompt template"""
        pass

--- END FILE: src/iML/prompts/base_prompt.py ---

--- START FILE: src/iML/prompts/description_analyzer_prompt.py ---
# src/iML/prompts/description_analyzer_prompt.py
import json
import os
from .base_prompt import BasePrompt
from typing import Dict, Any 

class DescriptionAnalyzerPrompt(BasePrompt):

    def default_template(self) -> str:
        return """
You are an expert AI assistant specializing in analyzing Kaggle competition descriptions. Your task is to read the provided text and extract key information into a specific JSON structure.
The output MUST be a valid JSON object and nothing else. Do not include any explanatory text before or after the JSON.
Extract the following information:
- "name": Dataset name
- "input_data": A description of the primary input data for the model.
- "output_data": A description of the expected output format from the model.
- "task": A summary of the main objective or task of the competition.
- "data file description": A dictionary where the keys are filenames (e.g., "train.csv") and the values are their descriptions.
- "link to the dataset": A list containing the filenames and folders of the core data files (like train, test, sample submission). Do NOT invent or guess full paths.
## EXAMPLE:
### INPUT TEXT:

\"\"\"
Welcome to the 'Paddy Disease Classification' challenge! The goal is to classify diseases in rice paddy images. The input data consists of images of rice plants (JPG files) from the `train_images` folder. Your model should output a class label for one of ten possible diseases. The dataset includes `train.csv` which maps image IDs to their labels, `test_images` for prediction, and `sample_submission.csv` for the required format.
\"\"\"

### OUTPUT JSON:

{{
    "name": "paddy_disease_classification",
    "input_data": "The input data consists of images of rice plants (JPG files).",
    "output_data": "The model should output a class label corresponding to one of ten possible diseases.",
    "task": "The main goal is to build a model that can classify diseases in rice paddy images.",
    "data file description": {{
        "train.csv": "Maps image IDs to their respective disease labels.",
        "train_images": "A folder containing the training images as JPG files.",
        "test_images": "A folder containing the test images for which predictions are required.",
        "sample_submission.csv": "An example file showing the required submission format."
    }},
    "link to the dataset": ["train.csv", "train_images", "test_images", "sample_submission.csv"]
}}
## END OF EXAMPLE. NOW, PROCESS THE FOLLOWING TEXT:
### INPUT TEXT:
\"\"\"
{description}
{directory_structure}
\"\"\"
### OUTPUT JSON:
"""

    # Fixed build method to be correct
    def build(self, description: str, directory_structure: str) -> str:
        """
        Build complete prompt from template and input values.
        """

        prompt = self.template.format(
            description=description,
            directory_structure=directory_structure
        )
        self.manager.save_and_log_states(
            content=prompt, 
            save_name="description_analyzer_prompt.txt"
        )
        return prompt

    # Fixed parse method to be correct
    def parse(self, response: str) -> Dict[str, Any]:
        """
        Parse LLM response to extract JSON object.
        """
        try:
            # Clean and parse JSON string from response
            clean_response = response.strip().replace("```json", "").replace("```", "")
            parsed_response = json.loads(clean_response)
        except json.JSONDecodeError as e:
            self.manager.logger.error(f"Failed to parse JSON from LLM response: {e}")
            parsed_response = {"error": "Invalid JSON response from LLM", "raw_response": response}

        # Process additional full path if available
        if "link to the dataset" in parsed_response and isinstance(parsed_response["link to the dataset"], list):
            dataset_path = self.manager.input_data_folder
            file_names = parsed_response["link to the dataset"]
            full_paths = [os.path.join(dataset_path, fname).replace("\\", "/") for fname in file_names]
            parsed_response["link to the dataset"] = full_paths

        self.manager.save_and_log_states(
            content=json.dumps(parsed_response, indent=2), 
            save_name="description_analyzer_response.json"
        )
        return parsed_response
--- END FILE: src/iML/prompts/description_analyzer_prompt.py ---

--- START FILE: src/iML/prompts/candidate_selector_prompt.py ---
from .base_prompt import BasePrompt

class CandidateSelectorPrompt(BasePrompt):
    """
    A prompt to guide the LLM in selecting the best candidate code from a list of options.
    """

    def default_template(self) -> str:
        """Default prompt template"""
        return """
You are an expert Python programmer and data scientist. Your task is to analyze several candidate scripts and select the one that is most likely to achieve the best performance for the given problem.

**Context:**
- **Problem Description:** {description_analysis}
- **Data Profiling Overview:** {profiling_result}
- **High-Level Plan (Guideline):** {guideline}
- **Original Code's Feedback:** {feedback}

**Candidate Scripts:**
{candidate_scripts}

**Instructions:**
1.  Review each candidate script carefully.
2.  Compare them against each other and the original feedback.
3.  Evaluate them based on potential for high performance, robustness, and adherence to the improvement suggestions.
4.  Your final output must be only the full, complete code of the single best candidate.
5.  **Do not add any explanation, commentary, or formatting.** Your output should be only the raw Python code for the chosen script, ready to be executed.

**Example of a valid output:**
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# ... rest of the chosen candidate's code
```
"""

    def __init__(self, manager, llm_config, **kwargs):
        super().__init__(manager, llm_config, **kwargs)

    def build(self) -> str:
        candidate_scripts = ""
        for i, code in enumerate(self.manager.candidates):
            candidate_scripts += f"### CANDIDATE {i+1} ###\n```python\n{code}\n```\n\n"

        return self.template.format(
            description_analysis=self.manager.description_analysis,
            profiling_result=self.manager.profiling_result,
            guideline=self.manager.guideline,
            feedback=self.manager.feedback,
            candidate_scripts=candidate_scripts.strip(),
        )

    def parse(self, response: str) -> any:
        """Parse the LLM response"""
        return response

--- END FILE: src/iML/prompts/candidate_selector_prompt.py ---

--- START FILE: src/iML/prompts/candidate_generator_prompt.py ---
from .base_prompt import BasePrompt
import re

class CandidateGeneratorPrompt(BasePrompt):
    """
    A prompt to guide the LLM in generating multiple candidate code versions based on feedback.
    """

    def default_template(self) -> str:
        """Default prompt template"""
        return """
You are an expert Python programmer and data scientist. Your task is to generate ONE improved version of the original Python script based on the provided feedback.

**Context:**
- **Problem Description:** {description_analysis}
- **Data Profiling Overview:** {profiling_result}
- **High-Level Plan (Guideline):** {guideline}
- **Original Code:**
  ```python
  {assembled_code}
  ```
- **Feedback for Improvement:**
  {feedback}
- **Previously Generated Valid Candidates:**
  {previous_candidates}

**Instructions:**
1.  Carefully read the feedback and understand the suggested improvements.
2.  Review the previously generated candidates to ensure your new suggestion is **distinct** and explores a **different improvement strategy**.
3.  Generate **one new candidate script** (Candidate #{candidate_number}). This script should incorporate one or more of the suggestions from the feedback.
4.  **Crucially, the candidate must be a complete and runnable Python script.** Do not use placeholders.
5.  **IMPORTANT**: Do not use command-line arguments (e.g., `sys.argv`). The script must run without any external arguments.
6.  **CRITICAL**: The original code contains absolute paths to data files. You **MUST** preserve these exact paths in your generated code. Do not change them.
7.  Format your output as a single, raw Python code block, enclosed in ```python ... ```.

**Example Output Format:**
```python
# Complete Python code for the new candidate...
import pandas as pd
# ... rest of the script
```
"""

    def __init__(self, manager, llm_config, **kwargs):
        super().__init__(manager, llm_config, **kwargs)

    def build(self, candidate_number: int, previous_candidates: list[str]) -> str:
        
        previous_candidates_str = "No valid candidates have been generated yet."
        if previous_candidates:
            previous_candidates_str = ""
            for i, code in enumerate(previous_candidates):
                previous_candidates_str += f"### PREVIOUS CANDIDATE {i+1} ###\n```python\n{code}\n```\n\n"

        return self.template.format(
            description_analysis=self.manager.description_analysis,
            profiling_result=self.manager.profiling_result,
            guideline=self.manager.guideline,
            assembled_code=self.manager.assembled_code,
            feedback=self.manager.feedback,
            candidate_number=candidate_number,
            previous_candidates=previous_candidates_str.strip(),
        )

    def parse(self, response: str) -> any:
        """Parse the LLM response to extract the code block."""
        match = re.search(r"```python\n(.*?)\n```", response, re.DOTALL)
        if match:
            return match.group(1).strip()
        # Fallback if the LLM doesn't use markdown
        return response.strip()

--- END FILE: src/iML/prompts/candidate_generator_prompt.py ---

--- START FILE: src/iML/prompts/modeling_coder_prompt.py ---
# src/iML/prompts/modeling_coder_prompt.py
import json
from typing import Dict, Any

from .base_prompt import BasePrompt

class ModelingCoderPrompt(BasePrompt):
    """
    Prompt handler to generate Python code for modeling.
    """

    def default_template(self) -> str:
        """Default template to request LLM to generate modeling code."""
        return """
You are an expert ML engineer. Your task is to generate Python code for modeling, which will be combined with the provided preprocessing code.

## CONTEXT
- **Dataset Name**: {dataset_name}
- **Task Description**: {task_desc}
- **File Paths**: {file_paths}
- **Ground Truth Paths**: {ground_truth_paths}

## MODELING GUIDELINES:
{modeling_guideline}

## PREPROCESSING CODE (Do NOT include this in your response):
The following preprocessing code, including a function `preprocess_data(file_paths: dict)`, will be available in the execution environment. You must call it to get the data.
```python
{preprocessing_code}
```

## REQUIREMENTS:
1.  **Generate COMPLETE Python code for the modeling part ONLY.** Do NOT repeat the preprocessing code.
2.  Your code should start with necessary imports for modeling (e.g., `import pandas as pd`, `from sklearn.ensemble import RandomForestClassifier`).
3.  Define a function `train_and_predict(X_train, y_train, X_test)`.
4.  The main execution block (`if __name__ == "__main__":`) must:
    a. Call the existing `preprocess_data()` function to get the datasets.
    b. Call your `train_and_predict()` function.
    c. Save the predictions to a `submission.csv` file. The format should typically be two columns: an identifier column and the prediction column.
5.  **Critical Error Handling**: The main execution block MUST be wrapped in a `try...except` block. If ANY exception occurs, the script MUST print the error to stderr and **exit with a non-zero status code** (`sys.exit(1)`).
6.  Follow the modeling guidelines for algorithm choice.
7.  Do not use extensive hyperparameter tuning unless specified. Keep the code efficient.
8.  Limit comments in the code.
9.  The submission file must have the same structure (number of columns) as the sample submission file provided in the dataset, but may have different ID. You have to use the test data to generate predictions and your right submission file. In some cases, you must browse the test image folder to get the IDs and data.
10. Your final COMPLETE Python code should have only ONE main function. If there are duplicate main function, remove the duplicates and keep only one main function.

## CODE STRUCTURE EXAMPLE:
```python
# Your modeling code starts here
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import sys
import os

{preprocessing_code}

def train_and_predict(X_train, y_train, X_test):
    # Your modeling, training, and prediction logic here
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    return predictions

if __name__ == "__main__":
    try:
        # These file paths will be available in the execution environment
        file_paths = {file_paths_main}
        
        # 1. Preprocess data using the provided function
        # The number of returned elements must match the preprocess_data function
        X_train, X_test, y_train, y_test, test_ids = preprocess_data(file_paths)

        # 2. Train model and get predictions
        predictions = train_and_predict(X_train, y_train, X_test)

        # 3. Create submission file
        submission_df = pd.DataFrame({{'test_id_column_name': test_ids, 'prediction_column_name': predictions}})
        submission_df.to_csv("submission.csv", index=False)

        print("Modeling script executed successfully and submission.csv created!")

    except Exception as e:
        print(f"An error occurred during modeling: {{e}}", file=sys.stderr)
        sys.exit(1)
```
"""

    def build(self, guideline: Dict, description: Dict, preprocessing_code: str, previous_code: str = None, error_message: str = None) -> str:
        """Build prompt to generate modeling code."""
        
        modeling_guideline = guideline.get('modeling', {})

        prompt = self.template.format(
            dataset_name=description.get('name', 'N/A'),
            task_desc=description.get('task', 'N/A'),
            file_paths=description.get('link to the dataset', []),
            file_paths_main=description.get('link to the dataset', []),
            ground_truth_paths=description.get('link to the ground truth', []),
            modeling_guideline=json.dumps(modeling_guideline, indent=2),
            preprocessing_code=preprocessing_code
        )

        if previous_code and error_message:
            retry_context = f"""
## PREVIOUS ATTEMPT FAILED:
The previously generated code failed with an error.

### Previous Code:
```python
{previous_code}
```

### Error Message:
```
{error_message}
```

## FIX INSTRUCTIONS:
1. Analyze the error message and the previous code carefully.
2. Fix the specific issue that caused the error.
3. Ensure your code correctly uses the data returned by the `preprocess_data` function.
4. Generate a new, complete, and corrected version of the Python code that resolves the issue.
5. Adhere to all original requirements.

Generate the corrected Python code:
"""
            prompt += retry_context
        
        self.manager.save_and_log_states(prompt, "modeling_coder_prompt.txt", per_iteration=True)
        return prompt

    def parse(self, response: str) -> str:
        """Extract Python code from LLM response."""
        if "```python" in response:
            code = response.split("```python")[1].split("```")[0].strip()
        elif "```" in response:
            code = response.split("```")[1].split("```")[0].strip()
        else:
            code = response
        
        self.manager.save_and_log_states(code, "modeling_code_response.py", per_iteration=True)
        return code

--- END FILE: src/iML/prompts/modeling_coder_prompt.py ---

--- START FILE: src/iML/prompts/preprocessing_coder_prompt.py ---
# src/iML/prompts/preprocessing_coder_prompt.py
import json
from typing import Dict, Any

from .base_prompt import BasePrompt

class PreprocessingCoderPrompt(BasePrompt):
    """
    Prompt handler to generate Python code for data preprocessing.
    """

    def default_template(self) -> str:
        """Default template to request LLM to generate code."""
        return """
You are a professional Machine Learning Engineer.
Generate complete and executable Python preprocessing code for the dataset below.

## DATASET INFO:
- Name: {dataset_name}
- Task: {task_desc}
- Input: {input_desc}
- Output: {output_desc}
- Data files: {data_file_desc}
- File paths: {file_paths}
- Ground truth paths: {ground_truth_paths}

## PREPROCESSING GUIDELINES:
{preprocessing_guideline}

## TARGET INFO:
{target_info}

## REQUIREMENTS:
1. Generate COMPLETE, EXECUTABLE Python code.
2. Include all necessary imports (pandas, scikit-learn, numpy, etc.).
3. Handle file loading from the provided paths.
4. Follow the preprocessing guidelines exactly.
5. Create a function `preprocess_data()` that takes a dictionary of file paths and returns a tuple of preprocessed data (e.g., X_train, X_test, y_train, y_test).
6. Include basic error handling and data validation within the function.
7. Limit comments in the code.
8. Preprocess both the train and test data consistently.
9. The main execution block (`if __name__ == "__main__":`) should test the function with the actual file paths.
10. **Critical Error Handling**: The main execution block MUST be wrapped in a `try...except` block. If ANY exception occurs, the script MUST print the error and then **exit with a non-zero status code** using `sys.exit(1)`.
11. DO NOT USE NLTK
12. Sample submission file given is for template reference only. You have to use the test data to generate predictions and your right submission file. In some cases, you must browse the test image folder to get the IDs and data.

## CODE STRUCTURE:
```python
# import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import sys
import os

def preprocess_data(file_paths: dict):
    \"\"\"
    Preprocess data according to guidelines.
    Returns: tuple (e.g., X_train, X_test, y_train, y_test)
    \"\"\"
    # Your preprocessing code here
    
    # Placeholder return
    X_train, X_test, y_train, y_test = (None, None, None, None)
    
    return X_train, X_test, y_train, y_test

# Test the function
if __name__ == "__main__":
    try:
        # This assumes the script is run from a directory where it can access the paths
        file_paths = {file_paths_main}
        preprocess_data(file_paths)
        print("Preprocessing script executed successfully!")
    except Exception as e:
        print(f"An error occurred during preprocessing test: {{e}}", file=sys.stderr)
        sys.exit(1)
````
"""

    def build(self, guideline: Dict, description: Dict, previous_code: str = None, error_message: str = None) -> str:
        """Build prompt to generate preprocessing code."""
        
        preprocessing_guideline = guideline.get('preprocessing', {})
        target_info = guideline.get("target_identification", {})

        prompt = self.template.format(
            dataset_name=description.get('name', 'N/A'),
            task_desc=description.get('task', 'N/A'),
            input_desc=description.get('input_data', ''),
            output_desc=description.get('output_data', ''),
            data_file_desc=json.dumps(description.get('data file description', {})),
            file_paths=description.get('link to the dataset', []),
            file_paths_main=description.get('link to the dataset', []),
            ground_truth_paths=description.get('link to the ground truth', []),
            preprocessing_guideline=json.dumps(preprocessing_guideline, indent=2),
            target_info=json.dumps(target_info, indent=2)
        )

        if previous_code and error_message:
            retry_context = f"""
## PREVIOUS ATTEMPT FAILED:
The previously generated code failed with an error.

### Previous Code:
```python
{previous_code}
```

### Error Message:
```
{error_message}
```

## FIX INSTRUCTIONS:
1. Analyze the error message and the previous code carefully.
2. Generate a new, complete, and corrected version of the Python code that resolves the issue.
3. Ensure the corrected code adheres to all the original requirements.

Generate the corrected Python code:
"""
            prompt += retry_context
        
        self.manager.save_and_log_states(prompt, "preprocessing_coder_prompt.txt", per_iteration=True)
        return prompt

    def parse(self, response: str) -> str:
        """Extract Python code from LLM response."""
        if "```python" in response:
            code = response.split("```python")[1].split("```")[0].strip()
        elif "```" in response:
            code = response.split("```")[1].split("```")[0].strip()
        else:
            code = response
        
        self.manager.save_and_log_states(code, "preprocessing_code_response.py", per_iteration=True)
        return code

--- END FILE: src/iML/prompts/preprocessing_coder_prompt.py ---

--- START FILE: src/iML/prompts/__init__.py ---
from .description_analyzer_prompt import DescriptionAnalyzerPrompt
from .preprocessing_coder_prompt import PreprocessingCoderPrompt
from .executer_prompt import ExecuterPrompt
from .modeling_coder_prompt import ModelingCoderPrompt
from .assembler_prompt import AssemblerPrompt
from .feedback_prompt import FeedbackPrompt
from .guideline_prompt import GuidelinePrompt
from .candidate_generator_prompt import CandidateGeneratorPrompt
from .candidate_selector_prompt import CandidateSelectorPrompt
--- END FILE: src/iML/prompts/__init__.py ---

--- START FILE: src/iML/prompts/feedback_prompt.py ---
from .base_prompt import BasePrompt

class FeedbackPrompt(BasePrompt):
    """
    A prompt to guide the LLM in analyzing the generated code and providing feedback for improvement.
    """

    def default_template(self) -> str:
        """Default prompt template"""
        return """
You are an expert Python programmer and data scientist. Your task is to analyze the provided Python script, which has been successfully executed, and provide constructive feedback on how to improve its performance and robustness.

**Context:**
- **Problem Description:** {description_analysis}
- **Data Profiling Overview:** {profiling_result}
- **High-Level Plan (Guideline):** {guideline}

**Code to Analyze:**
```python
{assembled_code}
```

**Instructions:**
Please analyze the code and provide feedback in the following format:
1.  **Strengths:** What are the good aspects of this code? (e.g., clarity, correctness, good use of libraries).
2.  **Weaknesses:** What are the potential issues or areas for improvement? (e.g., performance bottlenecks, inefficient algorithms, lack of error handling, hardcoded values that could be generalized).
3.  **Suggestions for Improvement:** Provide specific, actionable suggestions to address the weaknesses. Focus on changes that could lead to better model performance (e.g., different feature engineering techniques, alternative models, hyperparameter tuning strategies).

Your feedback should be clear, concise, and directly aimed at improving the final model's predictive accuracy or efficiency.
"""

    def __init__(self, manager, llm_config, **kwargs):
        super().__init__(manager, llm_config, **kwargs)

    def build(self) -> str:
        return self.template.format(
            description_analysis=self.manager.description_analysis,
            profiling_result=self.manager.profiling_result,
            guideline=self.manager.guideline,
            assembled_code=self.manager.assembled_code,
        )

    def parse(self, response: str) -> any:
        """Parse the LLM response"""
        return response

--- END FILE: src/iML/prompts/feedback_prompt.py ---

--- START FILE: src/iML/prompts/guideline_prompt.py ---
# src/iML/prompts/guideline_prompt.py
import json
from typing import Dict, Any

from .base_prompt import BasePrompt

def _create_variables_summary(variables: dict) -> dict:
    """Create a concise summary for variables in the profile."""
    summary = {}
    for var_name, var_details in variables.items():
        summary[var_name] = {
            "type": var_details.get("type"),
            "n_unique": var_details.get("n_unique"),
            "p_missing": var_details.get("p_missing"),
            "mean": var_details.get("mean"),
            "std": var_details.get("std"),
            "min": var_details.get("min"),
            "max": var_details.get("max"),
        }
    return summary

class GuidelinePrompt(BasePrompt):
    """
    Prompt handler to create guidelines for AutoML pipeline.
    """

    def default_template(self) -> str:
        """Default template to request LLM to create guidelines."""
        return """You are an expert Machine Learning architect. Your task is to analyze the provided dataset information and create a specific, actionable, and justified guideline for an AutoML pipeline.
## Dataset Information:
- Dataset: {dataset_name}
- Task: {task_desc}
- Size: {n_rows:,} rows, {n_cols} columns
- Key Quality Alerts: {alerts}

## Variables Analysis Summary:
```json
{variables_summary_str}
```

## Guideline Generation Principles & Examples
Your response must be guided by the following principles. Refer to these examples to understand the required level of detail.

BE SPECIFIC AND ACTIONABLE: Your recommendations must be concrete actions.
- Bad (Generic): "Handle missing values"
- Good (Specific): "Impute 'Age' with the median"

JUSTIFY YOUR CHOICES INTERNALLY: Even if the final JSON does not include every reasoning detail, your internal decision process must be sound, based on the data properties.

IT IS ACCEPTABLE TO OMIT: If a step is not necessary, provide an empty list or null for that key in the JSON output.

High-Quality Examples:

Example 1: Feature Engineering for a DateTime column
For a DateTime column like 'transaction_date', a good feature_engineering list would be ["Extract 'month' from 'transaction_date'", "Extract 'day_of_week' from 'transaction_date'"].

Example 2: Handling High Cardinality Categorical Data
For a categorical column 'product_id' with over 100 unique values, a good recommendation is ["Apply frequency encoding to 'product_id'"].

Example 3: Handling Missing Numerical Data
For a numeric column 'income' with 25% missing values and a skewed distribution, a good recommendation is ["Impute 'income' with its median"].

Before generating the final JSON, consider:
1. Identify the target variable and task type (classification, regression, etc.).
2. Review each variable's type, statistics, and potential issues.
3. Decide on specific actions for data preprocessing and modeling based on the data's properties.
4. If using pretrained models, choose the most appropriate ones.
5. Compile these specific actions into the required JSON format.

Output Format: Your response must be in the JSON format below:
Provide your response in JSON format. An empty list or null is acceptable for recommendations if not applicable.

IMPORTANT: Ensure the generated JSON is perfectly valid.
- All strings must be enclosed in double quotes.
- All backslashes inside strings must be properly escaped.
- There should be no unescaped newline characters within a string value.
- Do not include comments within the JSON output.

{{
    "target_identification": {{
        "target_variable": "identified_target_column_name",
        "reasoning": "explanation for target selection",
        "task_type": "classification/regression/etc"
    }},
    "preprocessing": {{
        "data_cleaning": ["specific step 1", "specific step 2"],
        "feature_engineering": ["specific technique 1", "specific technique 2"],
        "missing_values": ["strategy 1", "strategy 2"],
        "feature_selection": ["method 1", "method 2"],
        "data_splitting": {{"train": 0.8, "val": 0.2, "strategy": "stratified"}}
    }},
    "modeling": {{
        "recommended_algorithms": ["algorithm 1", "algorithm 2"],
        "model_selection": ["model_name1", "model_name2"],
        "cross_validation": {{"method": "stratified_kfold", "folds": 5, "scoring": "appropriate_metric"}}
    }},
    "evaluation": {{
        "metrics": ["metric 1", "metric 2"],
        "validation_strategy": ["approach 1", "approach 2"],
        "performance_benchmarking": ["baseline 1", "baseline 2"],
        "result_interpretation": ["interpretation 1", "interpretation 2"]
    }}
}}"""

    def build(self, description_analysis: Dict[str, Any], profiling_result: Dict[str, Any]) -> str:
        """Build prompt from analysis and profiling results."""
        task_info = description_analysis
        
        # Find key of train file in profiling results
        train_key = None
        for key in profiling_result.get('summaries', {}).keys():
            if 'test' not in key.lower() and 'submission' not in key.lower():
                train_key = key
                break
        
        if not train_key:
             # If not found, take the first key as default
            train_key = next(iter(profiling_result.get('summaries', {})), None)

        train_summary = profiling_result.get('summaries', {}).get(train_key, {})
        train_profile = profiling_result.get('profiles', {}).get(train_key, {})

        n_rows = train_summary.get('n_rows', 0)
        n_cols = train_summary.get('n_cols', 0)
        alerts = train_profile.get('alerts', [])
        variables = train_profile.get('variables', {})
        variables_summary_str = json.dumps(_create_variables_summary(variables), indent=2, ensure_ascii=False)
        dataset_name = task_info.get('name', 'N/A')
        task_desc = task_info.get('task', 'N/A')

        prompt = self.template.format(
            dataset_name=dataset_name,
            task_desc=task_desc,
            n_rows=n_rows,
            n_cols=n_cols,
            alerts=alerts[:3] if alerts else 'None',
            variables_summary_str=variables_summary_str
        )
        
        self.manager.save_and_log_states(prompt, "guideline_prompt.txt")
        return prompt

    def parse(self, response: str) -> Dict[str, Any]:
        """Parse JSON response from LLM."""
        try:
            parsed_response = json.loads(response.strip().replace("```json", "").replace("```", ""))
        except json.JSONDecodeError as e:
            self.manager.logger.error(f"Failed to parse JSON from LLM response for guideline: {e}")
            parsed_response = {"error": "Invalid JSON response from LLM", "raw_response": response}
        
        self.manager.save_and_log_states(
            json.dumps(parsed_response, indent=4, ensure_ascii=False), 
            "guideline_response.json"
        )
        return parsed_response

--- END FILE: src/iML/prompts/guideline_prompt.py ---

--- START FILE: src/iML/prompts/executer_prompt.py ---
import logging
from typing import Dict, Optional, Tuple

from .base_prompt import BasePrompt

logger = logging.getLogger(__name__)


class ExecuterPrompt(BasePrompt):
    """Handles prompts for code execution evaluation"""

    def default_template(self) -> str:
        """Default template for code execution evaluation"""
        return """You are an expert code evaluator. Analyze the execution results of the following Python code and determine if the execution was successful or if issues need to be fixed.

### Task Descriptions
{task_description}

### Data Structure
{data_prompt}

### Python Code
{python_code}

## Execution Results
### Standard Output (stdout)

{stdout}

### Standard Error (stderr)

{stderr}

Evaluate the execution results and decide on one of the following actions:
1. FINISH - If the execution was completely successful and met all requirements.
2. FIX - If there were errors, issues, or performance problems that need to be addressed.
Provide your decision in the following format:
DECISION: [FINISH or FIX]
ERROR_SUMMARY: [Brief summary of errors if any, or "None" if no errors]
The error summary should be brief but informative enough for another agent to understand what needs to be fixed.
Even if the code executed without throwing errors, it might still have issues with logic or not meet all requirements."""

    def build(self, stdout: str, stderr: str, python_code: str, task_description: str, data_prompt: str) -> str:
        """Build a prompt for the LLM to evaluate execution logs."""
        self.manager.save_and_log_states(content=stdout, save_name="stdout.txt", per_iteration=True, add_uuid=True)
        self.manager.save_and_log_states(content=stderr, save_name="stderr.txt", per_iteration=True, add_uuid=True)

        # Truncate outputs if they exceed max length
        stdout = self._truncate_output_mid(stdout, self.llm_config.max_stdout_length)
        stderr = self._truncate_output_mid(stderr, self.llm_config.max_stderr_length)

        self.manager.save_and_log_states(
            content=stdout, save_name="stdout(truncated).txt", per_iteration=True, add_uuid=True
        )
        self.manager.save_and_log_states(
            content=stderr, save_name="stderr(truncated).txt", per_iteration=True, add_uuid=True
        )

        # Format the prompt using the template
        prompt = self.template.format(
            task_description=task_description,
            data_prompt=data_prompt,
            python_code=python_code,
            stdout=stdout or "No standard output",
            stderr=stderr or "No standard error",
        )

        self.manager.save_and_log_states(
            content=prompt, save_name="executer_prompt.txt", per_iteration=True, add_uuid=True
        )

        return prompt

    def parse(self, response: Dict) -> Tuple[str, Optional[str]]:
        """Parse the LLM's response to extract decision and error summary."""

        # Extract content from LLM response
        if isinstance(response, dict) and "content" in response:
            content = response["content"]
        elif isinstance(response, str):
            content = response
        else:
            logger.warning("Unexpected response format from LLM")
            return "FIX", "Parser error"

        # Parse the decision
        decision = "FIX"  # Default to FIX if parsing fails
        if "DECISION:" in content:
            decision_line = [line for line in content.split("\n") if "DECISION:" in line]
            if decision_line:
                decision_text = decision_line[0].split("DECISION:")[1].strip()
                if "FINISH" in decision_text.upper():
                    decision = "FINISH"
                elif "FIX" in decision_text.upper():
                    decision = "FIX"

        # Parse the error summary
        error_summary = None
        if "ERROR_SUMMARY:" in content:
            error_summary_parts = content.split("ERROR_SUMMARY:")[1].strip()
            error_summary = error_summary_parts.split("\n\n")[0].strip()
            if error_summary.lower() == "none" or not error_summary:
                error_summary = None

        self.manager.save_and_log_states(
            content=response, save_name="executer_response.txt", per_iteration=True, add_uuid=True
        )
        self.manager.save_and_log_states(content=decision, save_name="decision.txt", per_iteration=True, add_uuid=True)
        self.manager.save_and_log_states(
            content=error_summary, save_name="error_summary.txt", per_iteration=True, add_uuid=True
        )

        return decision, error_summary

--- END FILE: src/iML/prompts/executer_prompt.py ---

