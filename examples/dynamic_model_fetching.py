#!/usr/bin/env python3
"""
Example: Dynamic Model Fetching Comparison
Shows how both OpenAI and Gemini can fetch models dynamically
"""

import os
import sys
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

def example_openai_dynamic_models():
    """Example of OpenAI dynamic model fetching"""
    print("ğŸ”µ OpenAI Dynamic Model Fetching")
    print("-" * 40)
    
    if "OPENAI_API_KEY" not in os.environ:
        print("âŒ OPENAI_API_KEY not set - skipping OpenAI example")
        return
    
    try:
        from iML.llm.openai_chat import get_openai_models
        
        # Simple approach - direct API call
        models = get_openai_models()
        
        if models:
            print(f"âœ… Found {len(models)} OpenAI models")
            print("ğŸ“‹ Sample models:")
            for model in models[:10]:  # Show first 10
                print(f"   â€¢ {model}")
            if len(models) > 10:
                print(f"   ... and {len(models) - 10} more")
        else:
            print("âŒ No models returned")
            
    except Exception as e:
        print(f"âŒ Error: {e}")

def example_gemini_dynamic_models():
    """Example of Gemini dynamic model fetching"""
    print("\nğŸŸ¡ Gemini Dynamic Model Fetching")
    print("-" * 40)
    
    if "GOOGLE_API_KEY" not in os.environ:
        print("âŒ GOOGLE_API_KEY not set - skipping Gemini example")
        return
    
    try:
        from iML.llm.gemini_chat import (
            get_gemini_models, 
            refresh_gemini_models,
            test_gemini_api_connection
        )
        
        # Test connection first
        print("ğŸ” Testing API connection...")
        if not test_gemini_api_connection():
            print("âŒ Cannot connect to Gemini API")
            return
        
        # Enhanced approach with caching and fallback
        print("ğŸ“¥ Fetching models (with caching)...")
        models = get_gemini_models()
        
        print(f"âœ… Found {len(models)} Gemini models")
        print("ğŸ“‹ All models:")
        for model in models:
            print(f"   â€¢ {model}")
        
        # Demonstrate cache refresh
        print("\nğŸ”„ Refreshing cache...")
        refreshed_models = refresh_gemini_models()
        print(f"âœ… Refreshed: {len(refreshed_models)} models")
        
    except Exception as e:
        print(f"âŒ Error: {e}")

def example_gemini_fallback():
    """Example of Gemini fallback behavior"""
    print("\nğŸŸ  Gemini Fallback Behavior")
    print("-" * 40)
    
    # Temporarily hide API key to test fallback
    original_key = os.environ.get("GOOGLE_API_KEY")
    if original_key:
        del os.environ["GOOGLE_API_KEY"]
    
    try:
        from iML.llm.gemini_chat import get_gemini_models
        
        print("ğŸ” Testing without API key...")
        models = get_gemini_models()
        
        print(f"âœ… Fallback successful: {len(models)} default models")
        print("ğŸ“‹ Default models:")
        for model in models:
            print(f"   â€¢ {model}")
            
    except Exception as e:
        print(f"âŒ Fallback failed: {e}")
    finally:
        # Restore API key
        if original_key:
            os.environ["GOOGLE_API_KEY"] = original_key

def compare_implementations():
    """Compare the two implementations"""
    print("\nğŸ“Š Implementation Comparison")
    print("=" * 50)
    
    print("\nğŸ”µ OpenAI Approach:")
    print("```python")
    print("def get_openai_models() -> List[str]:")
    print("    try:")
    print("        client = OpenAI()")
    print("        models = client.models.list()")
    print("        return [model.id for model in models]")
    print("    except Exception as e:")
    print("        logger.error(f'Error: {e}')")
    print("        return []  # Returns empty on failure")
    print("```")
    
    print("\nğŸŸ¡ Gemini Approach:")
    print("```python")
    print("@lru_cache(maxsize=1)")
    print("def get_gemini_models() -> List[str]:")
    print("    try:")
    print("        if 'GOOGLE_API_KEY' not in os.environ:")
    print("            return _get_default_gemini_models()")
    print("        ")
    print("        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])")
    print("        models = genai.list_models()")
    print("        # Filter and clean model names")
    print("        return sorted(valid_models)")
    print("    except Exception:")
    print("        return _get_default_gemini_models()  # Always usable")
    print("```")
    
    print("\nğŸ¯ Key Differences:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Feature         â”‚ OpenAI       â”‚ Gemini       â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ Caching         â”‚ No           â”‚ Yes (@lru)   â”‚")
    print("â”‚ Fallback        â”‚ Empty list   â”‚ Default list â”‚")
    print("â”‚ Filtering       â”‚ All models   â”‚ Compatible   â”‚")
    print("â”‚ Error Handling  â”‚ Basic        â”‚ Robust       â”‚")
    print("â”‚ Reliability     â”‚ May fail     â”‚ Always works â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

def main():
    """Run all examples"""
    print("ğŸš€ Dynamic Model Fetching Examples")
    print("=" * 50)
    
    # Run examples
    example_openai_dynamic_models()
    example_gemini_dynamic_models()
    example_gemini_fallback()
    
    # Show comparison
    compare_implementations()
    
    print("\nğŸ‰ Summary:")
    print("â€¢ Both approaches support dynamic model fetching")
    print("â€¢ Gemini approach is more robust with caching and fallback")
    print("â€¢ No more hardcoded model lists needed!")
    print("â€¢ API keys still required for live fetching")

if __name__ == "__main__":
    main()
